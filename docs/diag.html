<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Diagnostics | Statistical Foundations</title>
  <meta name="description" content="Chapter 5 Diagnostics | Statistical Foundations" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Diagnostics | Statistical Foundations" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="IAA-Faculty/statistical_foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Diagnostics | Statistical Foundations" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="model-selection.html"/>
<link rel="next" href="model-building-and-scoring-for-prediction.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a style="font-weight:bold, text-align:center" href="https://github.com/IAA-Faculty/statistical_foundations/">Statistical Foundations</a>
<img src="./img/iaaicon.png" alt="IAA"  class="center"</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-stat.html"><a href="intro-stat.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a>
<ul>
<li class="chapter" data-level="1.0.1" data-path="intro-stat.html"><a href="intro-stat.html#python-code"><i class="fa fa-check"></i><b>1.0.1</b> Python Code:</a></li>
<li class="chapter" data-level="1.1" data-path="intro-stat.html"><a href="intro-stat.html#eda"><i class="fa fa-check"></i><b>1.1</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro-stat.html"><a href="intro-stat.html#vartypes"><i class="fa fa-check"></i><b>1.1.1</b> Types of Variables</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-stat.html"><a href="intro-stat.html#nominal-variables"><i class="fa fa-check"></i><b>1.1.2</b> Nominal Variables</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-stat.html"><a href="intro-stat.html#python-code-1"><i class="fa fa-check"></i><b>1.1.3</b> Python Code</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-stat.html"><a href="intro-stat.html#interval-variables"><i class="fa fa-check"></i><b>1.1.4</b> Interval Variables</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-stat.html"><a href="intro-stat.html#ordinal-variables"><i class="fa fa-check"></i><b>1.1.5</b> Ordinal Variables</a></li>
<li class="chapter" data-level="1.1.6" data-path="intro-stat.html"><a href="intro-stat.html#distributions"><i class="fa fa-check"></i><b>1.1.6</b> Distributions</a></li>
<li class="chapter" data-level="1.1.7" data-path="intro-stat.html"><a href="intro-stat.html#python-code-2"><i class="fa fa-check"></i><b>1.1.7</b> Python code:</a></li>
<li class="chapter" data-level="1.1.8" data-path="intro-stat.html"><a href="intro-stat.html#location"><i class="fa fa-check"></i><b>1.1.8</b> Location</a></li>
<li class="chapter" data-level="1.1.9" data-path="intro-stat.html"><a href="intro-stat.html#spread"><i class="fa fa-check"></i><b>1.1.9</b> Spread</a></li>
<li class="chapter" data-level="1.1.10" data-path="intro-stat.html"><a href="intro-stat.html#shape"><i class="fa fa-check"></i><b>1.1.10</b> Shape</a></li>
<li class="chapter" data-level="1.1.11" data-path="intro-stat.html"><a href="intro-stat.html#python-code-3"><i class="fa fa-check"></i><b>1.1.11</b> Python code:</a></li>
<li class="chapter" data-level="1.1.12" data-path="intro-stat.html"><a href="intro-stat.html#normal"><i class="fa fa-check"></i><b>1.1.12</b> The Normal Distribution</a></li>
<li class="chapter" data-level="1.1.13" data-path="intro-stat.html"><a href="intro-stat.html#skew"><i class="fa fa-check"></i><b>1.1.13</b> Skewness</a></li>
<li class="chapter" data-level="1.1.14" data-path="intro-stat.html"><a href="intro-stat.html#kurt"><i class="fa fa-check"></i><b>1.1.14</b> Kurtosis</a></li>
<li class="chapter" data-level="1.1.15" data-path="intro-stat.html"><a href="intro-stat.html#graphdist"><i class="fa fa-check"></i><b>1.1.15</b> Graphical Displays of Distributions</a></li>
<li class="chapter" data-level="1.1.16" data-path="intro-stat.html"><a href="intro-stat.html#histograms"><i class="fa fa-check"></i><b>1.1.16</b> Histograms</a></li>
<li class="chapter" data-level="1.1.17" data-path="intro-stat.html"><a href="intro-stat.html#python-code-4"><i class="fa fa-check"></i><b>1.1.17</b> Python Code:</a></li>
<li class="chapter" data-level="1.1.18" data-path="intro-stat.html"><a href="intro-stat.html#normal-probability-plots-qq-plots"><i class="fa fa-check"></i><b>1.1.18</b> Normal probability plots (QQ Plots)</a></li>
<li class="chapter" data-level="1.1.19" data-path="intro-stat.html"><a href="intro-stat.html#python-code-5"><i class="fa fa-check"></i><b>1.1.19</b> Python code:</a></li>
<li class="chapter" data-level="1.1.20" data-path="intro-stat.html"><a href="intro-stat.html#box-plots"><i class="fa fa-check"></i><b>1.1.20</b> Box Plots</a></li>
<li class="chapter" data-level="1.1.21" data-path="intro-stat.html"><a href="intro-stat.html#python-code-6"><i class="fa fa-check"></i><b>1.1.21</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro-stat.html"><a href="intro-stat.html#pointest"><i class="fa fa-check"></i><b>1.2</b> Point Estimates</a></li>
<li class="chapter" data-level="1.3" data-path="intro-stat.html"><a href="intro-stat.html#ci"><i class="fa fa-check"></i><b>1.3</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-stat.html"><a href="intro-stat.html#python-code-7"><i class="fa fa-check"></i><b>1.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-stat.html"><a href="intro-stat.html#hypotest"><i class="fa fa-check"></i><b>1.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro-stat.html"><a href="intro-stat.html#onesample"><i class="fa fa-check"></i><b>1.4.1</b> One-Sample T-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro-stat.html"><a href="intro-stat.html#python-code-8"><i class="fa fa-check"></i><b>1.4.2</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-stat.html"><a href="intro-stat.html#two-sample-t-tests"><i class="fa fa-check"></i><b>1.5</b> Two-Sample t-tests</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-stat.html"><a href="intro-stat.html#testnorm"><i class="fa fa-check"></i><b>1.5.1</b> Testing Normality of Groups</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-stat.html"><a href="intro-stat.html#ftest"><i class="fa fa-check"></i><b>1.5.2</b> Testing Equality of Variances</a></li>
<li class="chapter" data-level="1.5.3" data-path="intro-stat.html"><a href="intro-stat.html#tsttest"><i class="fa fa-check"></i><b>1.5.3</b> Testing Equality of Means</a></li>
<li class="chapter" data-level="1.5.4" data-path="intro-stat.html"><a href="intro-stat.html#python-code-9"><i class="fa fa-check"></i><b>1.5.4</b> Python Code</a></li>
<li class="chapter" data-level="1.5.5" data-path="intro-stat.html"><a href="intro-stat.html#wilcoxon"><i class="fa fa-check"></i><b>1.5.5</b> Mann-Whitney-Wilcoxon Test</a></li>
<li class="chapter" data-level="1.5.6" data-path="intro-stat.html"><a href="intro-stat.html#python-code-10"><i class="fa fa-check"></i><b>1.5.6</b> Python Code</a></li>
<li class="chapter" data-level="1.5.7" data-path="intro-stat.html"><a href="intro-stat.html#bootstrap"><i class="fa fa-check"></i><b>1.5.7</b> Bootstrap</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="slr.html"><a href="slr.html"><i class="fa fa-check"></i><b>2</b> Introduction to ANOVA and Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="slr.html"><a href="slr.html#evp"><i class="fa fa-check"></i><b>2.1</b> Predictive vs. Explanatory</a></li>
<li class="chapter" data-level="2.2" data-path="slr.html"><a href="slr.html#trainvalidtest"><i class="fa fa-check"></i><b>2.2</b> Honest Assessment</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="slr.html"><a href="slr.html#python-code-11"><i class="fa fa-check"></i><b>2.2.1</b> Python Code:</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="slr.html"><a href="slr.html#bivariate-eda"><i class="fa fa-check"></i><b>2.3</b> Bivariate EDA</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="slr.html"><a href="slr.html#continuous-continuous-associations"><i class="fa fa-check"></i><b>2.3.1</b> Continuous-Continuous Associations</a></li>
<li class="chapter" data-level="2.3.2" data-path="slr.html"><a href="slr.html#continuous-categorical-associations"><i class="fa fa-check"></i><b>2.3.2</b> Continuous-Categorical Associations</a></li>
<li class="chapter" data-level="2.3.3" data-path="slr.html"><a href="slr.html#python-code-12"><i class="fa fa-check"></i><b>2.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="slr.html"><a href="slr.html#oneway"><i class="fa fa-check"></i><b>2.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="slr.html"><a href="slr.html#python-code-13"><i class="fa fa-check"></i><b>2.4.1</b> Python Code</a></li>
<li class="chapter" data-level="2.4.2" data-path="slr.html"><a href="slr.html#kruskal"><i class="fa fa-check"></i><b>2.4.2</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="2.4.3" data-path="slr.html"><a href="slr.html#python-code-14"><i class="fa fa-check"></i><b>2.4.3</b> Python Code:</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="slr.html"><a href="slr.html#posthoc"><i class="fa fa-check"></i><b>2.5</b> ANOVA Post-hoc Testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="slr.html"><a href="slr.html#tukey"><i class="fa fa-check"></i><b>2.5.1</b> Tukey-Kramer</a></li>
<li class="chapter" data-level="2.5.2" data-path="slr.html"><a href="slr.html#dunnett"><i class="fa fa-check"></i><b>2.5.2</b> Dunnett’s Test</a></li>
<li class="chapter" data-level="2.5.3" data-path="slr.html"><a href="slr.html#python-code-15"><i class="fa fa-check"></i><b>2.5.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="slr.html"><a href="slr.html#cor"><i class="fa fa-check"></i><b>2.6</b> Pearson Correlation</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="slr.html"><a href="slr.html#testcor"><i class="fa fa-check"></i><b>2.6.1</b> Statistical Test</a></li>
<li class="chapter" data-level="2.6.2" data-path="slr.html"><a href="slr.html#effect-of-anomalous-observations"><i class="fa fa-check"></i><b>2.6.2</b> Effect of Anomalous Observations</a></li>
<li class="chapter" data-level="2.6.3" data-path="slr.html"><a href="slr.html#the-correlation-matrix"><i class="fa fa-check"></i><b>2.6.3</b> The Correlation Matrix</a></li>
<li class="chapter" data-level="2.6.4" data-path="slr.html"><a href="slr.html#python-code-16"><i class="fa fa-check"></i><b>2.6.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="slr.html"><a href="slr.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="slr.html"><a href="slr.html#slrassumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions of Linear Regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="slr.html"><a href="slr.html#testing-for-association"><i class="fa fa-check"></i><b>2.7.2</b> Testing for Association</a></li>
<li class="chapter" data-level="2.7.3" data-path="slr.html"><a href="slr.html#python-code-17"><i class="fa fa-check"></i><b>2.7.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Complex ANOVA and Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#two-way-anova"><i class="fa fa-check"></i><b>3.1</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mlr.html"><a href="mlr.html#exploration"><i class="fa fa-check"></i><b>3.1.1</b> Exploration</a></li>
<li class="chapter" data-level="3.1.2" data-path="mlr.html"><a href="mlr.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
<li class="chapter" data-level="3.1.3" data-path="mlr.html"><a href="mlr.html#post-hoc-testing"><i class="fa fa-check"></i><b>3.1.3</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.1.4" data-path="mlr.html"><a href="mlr.html#python-code-18"><i class="fa fa-check"></i><b>3.1.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#two-way-anova-with-interactions"><i class="fa fa-check"></i><b>3.2</b> Two-Way ANOVA with Interactions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#post-hoc-testing-1"><i class="fa fa-check"></i><b>3.2.1</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.2.2" data-path="mlr.html"><a href="mlr.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="mlr.html"><a href="mlr.html#python-code-19"><i class="fa fa-check"></i><b>3.2.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#randomized-block-design"><i class="fa fa-check"></i><b>3.3</b> Randomized Block Design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mlr.html"><a href="mlr.html#garlic-bulb-weight-example"><i class="fa fa-check"></i><b>3.3.1</b> Garlic Bulb Weight Example</a></li>
<li class="chapter" data-level="3.3.2" data-path="mlr.html"><a href="mlr.html#assumptions-1"><i class="fa fa-check"></i><b>3.3.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.3.3" data-path="mlr.html"><a href="mlr.html#python-code-20"><i class="fa fa-check"></i><b>3.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model Structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="mlr.html"><a href="mlr.html#global-local-inference"><i class="fa fa-check"></i><b>3.4.2</b> Global &amp; Local Inference</a></li>
<li class="chapter" data-level="3.4.3" data-path="mlr.html"><a href="mlr.html#assumptions-2"><i class="fa fa-check"></i><b>3.4.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.4.4" data-path="mlr.html"><a href="mlr.html#multiple-coefficients-of-determination"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Coefficients of Determination</a></li>
<li class="chapter" data-level="3.4.5" data-path="mlr.html"><a href="mlr.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>3.4.5</b> Categorical Predictor Variables</a></li>
<li class="chapter" data-level="3.4.6" data-path="mlr.html"><a href="mlr.html#python-code-21"><i class="fa fa-check"></i><b>3.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>4</b> Model Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-selection.html"><a href="model-selection.html#selection-criteria"><i class="fa fa-check"></i><b>4.1</b> Selection Criteria</a></li>
<li class="chapter" data-level="4.2" data-path="model-selection.html"><a href="model-selection.html#stepwise-selection"><i class="fa fa-check"></i><b>4.2</b> Stepwise Selection</a>
<ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward"><i class="fa fa-check"></i>Backward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#stepwise"><i class="fa fa-check"></i>Stepwise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-selection.html"><a href="model-selection.html#significance-levels"><i class="fa fa-check"></i><b>4.3</b> Significance Levels</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-selection.html"><a href="model-selection.html#python-code-22"><i class="fa fa-check"></i><b>4.3.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>5</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="diag.html"><a href="diag.html#python-code-23"><i class="fa fa-check"></i><b>5.0.1</b> Python Code:</a></li>
<li class="chapter" data-level="5.1" data-path="diag.html"><a href="diag.html#examining-residuals"><i class="fa fa-check"></i><b>5.1</b> Examining Residuals</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="diag.html"><a href="diag.html#python-code-24"><i class="fa fa-check"></i><b>5.1.1</b> Python Code:</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="diag.html"><a href="diag.html#misspecified-model"><i class="fa fa-check"></i><b>5.2</b> Misspecified Model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="diag.html"><a href="diag.html#python-code-25"><i class="fa fa-check"></i><b>5.2.1</b> Python Code:</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="diag.html"><a href="diag.html#constant-variance"><i class="fa fa-check"></i><b>5.3</b> Constant Variance</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="diag.html"><a href="diag.html#python-code-26"><i class="fa fa-check"></i><b>5.3.1</b> Python Code:</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="diag.html"><a href="diag.html#normality"><i class="fa fa-check"></i><b>5.4</b> Normality</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="diag.html"><a href="diag.html#python-code-27"><i class="fa fa-check"></i><b>5.4.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="diag.html"><a href="diag.html#correlated-errors"><i class="fa fa-check"></i><b>5.5</b> Correlated Errors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="diag.html"><a href="diag.html#python-code-28"><i class="fa fa-check"></i><b>5.5.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="diag.html"><a href="diag.html#influential-observations-and-outliers"><i class="fa fa-check"></i><b>5.6</b> Influential Observations and Outliers</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="diag.html"><a href="diag.html#python-code-29"><i class="fa fa-check"></i><b>5.6.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="diag.html"><a href="diag.html#multicollinearity"><i class="fa fa-check"></i><b>5.7</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="diag.html"><a href="diag.html#python-code-30"><i class="fa fa-check"></i><b>5.7.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html"><i class="fa fa-check"></i><b>6</b> Model Building and Scoring for Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#regularized-regression"><i class="fa fa-check"></i><b>6.1</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#penalties-in-models"><i class="fa fa-check"></i><b>6.1.1</b> Penalties in Models</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#lasso"><i class="fa fa-check"></i><b>6.1.3</b> LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="6.1.5" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-31"><i class="fa fa-check"></i><b>6.1.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#optimizing-penalties"><i class="fa fa-check"></i><b>6.2</b> Optimizing Penalties</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cross-validation"><i class="fa fa-check"></i><b>6.2.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cv-in-regularized-regression"><i class="fa fa-check"></i><b>6.2.2</b> CV in Regularized Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-comparisons"><i class="fa fa-check"></i><b>6.3</b> Model Comparisons</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-metrics"><i class="fa fa-check"></i><b>6.3.1</b> Model Metrics</a></li>
<li class="chapter" data-level="6.3.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#test-dataset-comparison"><i class="fa fa-check"></i><b>6.3.2</b> Test Dataset Comparison</a></li>
<li class="chapter" data-level="6.3.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-32"><i class="fa fa-check"></i><b>6.3.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#describing-categorical-data"><i class="fa fa-check"></i><b>7.1</b> Describing Categorical Data</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-33"><i class="fa fa-check"></i><b>7.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#tests-of-association"><i class="fa fa-check"></i><b>7.2</b> Tests of Association</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-34"><i class="fa fa-check"></i><b>7.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#measures-of-association"><i class="fa fa-check"></i><b>7.3</b> Measures of Association</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-35"><i class="fa fa-check"></i><b>7.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Introduction to Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#linear-probability-model"><i class="fa fa-check"></i><b>7.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.4.2</b> Binary Logistic Regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#adding-categorical-variables"><i class="fa fa-check"></i><b>7.4.3</b> Adding Categorical Variables</a></li>
<li class="chapter" data-level="7.4.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#model-assessment"><i class="fa fa-check"></i><b>7.4.4</b> Model Assessment</a></li>
<li class="chapter" data-level="7.4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#variable-selection-and-regularized-regression"><i class="fa fa-check"></i><b>7.4.5</b> Variable Selection and Regularized Regression</a></li>
<li class="chapter" data-level="7.4.6" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-36"><i class="fa fa-check"></i><b>7.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="diag" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Diagnostics<a href="diag.html#diag" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this Chapter, we will take a look at each of the assumptions in a linear model. We will discuss what tools you will use to assess these assumptions, how to diagnose if the assumptions are met, some common problems often encountered when these assumptions do not hold and finally some remedies to fix these common issues. You will notice that a variety of data sets are used throughout this Chapter. This is to help visualize and test for various assumptions and how to identify when these assumptions are not met.</p>
<p>This Chapter aims to answer the following questions:</p>
<ul>
<li>How to use residuals from a multiple linear regression model to assess the assumptions.
<ul>
<li>Does the mean of the response have a linear pattern in the explanatory variables?</li>
<li>Do the residuals have a constant variance?</li>
<li>Are the residuals normally distributed?</li>
<li>Are the residuals independent?</li>
<li>How to identify potential outliers and influential points.</li>
<li>How to identify potential multicolliearity.</li>
</ul></li>
</ul>
<p>In multiple linear regression, the assumptions are as follows:</p>
<ol style="list-style-type: decimal">
<li>The mean of the Y’s is accurately modeled by a linear function of the X’s.<br />
</li>
<li>The random error term, <span class="math inline">\(\varepsilon\)</span>, is assumed to have a normal distribution with a mean of zero.<br />
</li>
<li>The random error term, <span class="math inline">\(\varepsilon\)</span>, is assumed to have a constant variance, <span class="math inline">\(\sigma^{2}\)</span>.<br />
</li>
<li>The errors are independent.<br />
</li>
<li>No perfect collinearity.<br />
</li>
</ol>
<p>Before exploring the assumptions of a linear model, it is always good to visually take a look at your data (if it is not too large). The <code>pairs</code> command in R allows you to look at all scatterplots between the variables in a data set.</p>
<p>To illustrate this, we will use the Salaries data set in the package carData in R that has 397 observations of salaries for professors. The explanatory variables include:</p>
<ul>
<li><em>rank</em>: a factor with levels AssocProf, AsstProf, Prof</li>
<li><em>discipline</em>: a factor with levels A (“theoretical” departments) or B (“applied” departments)</li>
<li><em>yrs.since.phd</em>: years since PhD</li>
<li><em>yrs.service</em>: years of service</li>
<li><em>sex</em>: a factor with levels Female and Male</li>
<li><em>salary</em>: nine-month salary, in dollars</li>
</ul>
<p>Using this data set, let’s take a look at the relationship between each pair of variables in Figure <a href="diag.html#fig:pairsex">5.1</a>.</p>
<div id="r-code-22" class="section level4 hasAnchor" number="5.0.0.1">
<h4><span class="header-section-number">5.0.0.1</span> R code:<a href="diag.html#r-code-22" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="diag.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb1-2"><a href="diag.html#cb1-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="diag.html#cb1-3" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb1-4"><a href="diag.html#cb1-4" tabindex="-1"></a><span class="fu">library</span>(grid)</span>
<span id="cb1-5"><a href="diag.html#cb1-5" tabindex="-1"></a><span class="fu">library</span>(lattice)</span>
<span id="cb1-6"><a href="diag.html#cb1-6" tabindex="-1"></a><span class="fu">library</span>(nortest)</span>
<span id="cb1-7"><a href="diag.html#cb1-7" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb1-8"><a href="diag.html#cb1-8" tabindex="-1"></a><span class="fu">library</span>(TSA)</span>
<span id="cb1-9"><a href="diag.html#cb1-9" tabindex="-1"></a><span class="fu">library</span>(lmtest)</span>
<span id="cb1-10"><a href="diag.html#cb1-10" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb1-11"><a href="diag.html#cb1-11" tabindex="-1"></a><span class="fu">pairs</span>(Salaries)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pairsex"></span>
<img src="bookdownproj_files/figure-html/pairsex-1.png" alt="Matrix of Scatter Plots for the Salaries Data" width="672" />
<p class="caption">
Figure 5.1: Matrix of Scatter Plots for the Salaries Data
</p>
</div>
<p>If there are not too many variables, this plot is a nice way to see all the relationships in the data set. The variables are listed along the diagonal and each off diagonal plot is a scatterplot of the variables represented by that row and column. For example, the first row of plots have rank on the y-axis. The first column of plots have rank along the x-axis.</p>
</div>
<div id="python-code-23" class="section level3 hasAnchor" number="5.0.1">
<h3><span class="header-section-number">5.0.1</span> Python Code:<a href="diag.html#python-code-23" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="diag.html#cb2-1" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb2-2"><a href="diag.html#cb2-2" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb2-3"><a href="diag.html#cb2-3" tabindex="-1"></a><span class="im">import</span> statsmodels.stats <span class="im">as</span> ss</span>
<span id="cb2-4"><a href="diag.html#cb2-4" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-5"><a href="diag.html#cb2-5" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb2-6"><a href="diag.html#cb2-6" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb2-7"><a href="diag.html#cb2-7" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-8"><a href="diag.html#cb2-8" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb2-9"><a href="diag.html#cb2-9" tabindex="-1"></a></span>
<span id="cb2-10"><a href="diag.html#cb2-10" tabindex="-1"></a>Salaries <span class="op">=</span> r.Salaries</span>
<span id="cb2-11"><a href="diag.html#cb2-11" tabindex="-1"></a>train<span class="op">=</span>r.train</span>
<span id="cb2-12"><a href="diag.html#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="diag.html#cb2-13" tabindex="-1"></a>Salaries <span class="op">=</span> Salaries.rename(columns<span class="op">=</span>{<span class="st">&quot;yrs.since.phd&quot;</span>: <span class="st">&quot;yrs_since_phd&quot;</span>, <span class="st">&quot;yrs.service&quot;</span>: <span class="st">&quot;yrs_service&quot;</span>})</span>
<span id="cb2-14"><a href="diag.html#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="diag.html#cb2-15" tabindex="-1"></a>ax <span class="op">=</span> sns.pairplot(data <span class="op">=</span> Salaries[[<span class="st">&#39;rank&#39;</span>, <span class="st">&#39;discipline&#39;</span>, <span class="st">&#39;yrs_since_phd&#39;</span>, <span class="st">&#39;yrs_service&#39;</span>, <span class="st">&#39;salary&#39;</span>]])</span>
<span id="cb2-16"><a href="diag.html#cb2-16" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-4-1.png" width="720" /></p>
</div>
<div id="examining-residuals" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Examining Residuals<a href="diag.html#examining-residuals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As you can see from the above list of assumptions that most of them involve the error term which is estimated by the residuals. We will be using the residuals for many of these diagnostics. One of the most useful plot is referred as the residual plot. This plot will have the residuals along the y-axis and either the predicted values or individual x-values along the x-axis. The following figure is an example using the residuals from a linear model predicting salary using all of the explanatory variables.</p>
<div id="r-code-23" class="section level4 hasAnchor" number="5.1.0.1">
<h4><span class="header-section-number">5.1.0.1</span> R code:<a href="diag.html#r-code-23" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="diag.html#cb3-1" tabindex="-1"></a>lm.model<span class="ot">=</span><span class="fu">lm</span>(salary<span class="sc">~</span>.,<span class="at">data=</span>Salaries)</span>
<span id="cb3-2"><a href="diag.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="diag.html#cb3-3" tabindex="-1"></a><span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span><span class="fu">fitted</span>(lm.model),<span class="at">y=</span><span class="fu">resid</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Predicted Values&quot;</span>,<span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:salresid"></span>
<img src="bookdownproj_files/figure-html/salresid-3.png" alt="Residuals vs. Predicted Values for Salary Model" width="672" />
<p class="caption">
Figure 5.2: Residuals vs. Predicted Values for Salary Model
</p>
</div>
</div>
<div id="python-code-24" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Python Code:<a href="diag.html#python-code-24" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="diag.html#cb4-1" tabindex="-1"></a>model_mlr <span class="op">=</span> smf.ols(<span class="st">&quot;salary ~ C(rank) + C(discipline) + yrs_since_phd + yrs_service + C(sex)&quot;</span>, data <span class="op">=</span> Salaries).fit()</span>
<span id="cb4-2"><a href="diag.html#cb4-2" tabindex="-1"></a>model_mlr.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>salary</td>      <th>  R-squared:         </th> <td>   0.455</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.446</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   54.20</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>1.79e-48</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:03</td>     <th>  Log-Likelihood:    </th> <td> -4538.9</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   397</td>      <th>  AIC:               </th> <td>   9092.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   390</td>      <th>  BIC:               </th> <td>   9120.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     6</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
            <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>            <td> 6.596e+04</td> <td> 4588.601</td> <td>   14.374</td> <td> 0.000</td> <td> 5.69e+04</td> <td>  7.5e+04</td>
</tr>
<tr>
  <th>C(rank)[T.AssocProf]</th> <td> 1.291e+04</td> <td> 4145.278</td> <td>    3.114</td> <td> 0.002</td> <td> 4757.700</td> <td> 2.11e+04</td>
</tr>
<tr>
  <th>C(rank)[T.Prof]</th>      <td> 4.507e+04</td> <td> 4237.523</td> <td>   10.635</td> <td> 0.000</td> <td> 3.67e+04</td> <td> 5.34e+04</td>
</tr>
<tr>
  <th>C(discipline)[T.B]</th>   <td> 1.442e+04</td> <td> 2342.875</td> <td>    6.154</td> <td> 0.000</td> <td> 9811.380</td> <td>  1.9e+04</td>
</tr>
<tr>
  <th>C(sex)[T.Male]</th>       <td> 4783.4928</td> <td> 3858.668</td> <td>    1.240</td> <td> 0.216</td> <td>-2802.901</td> <td> 1.24e+04</td>
</tr>
<tr>
  <th>yrs_since_phd</th>        <td>  535.0583</td> <td>  240.994</td> <td>    2.220</td> <td> 0.027</td> <td>   61.248</td> <td> 1008.869</td>
</tr>
<tr>
  <th>yrs_service</th>          <td> -489.5157</td> <td>  211.938</td> <td>   -2.310</td> <td> 0.021</td> <td> -906.199</td> <td>  -72.833</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>46.385</td> <th>  Durbin-Watson:     </th> <td>   1.919</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  82.047</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.699</td> <th>  Prob(JB):          </th> <td>1.53e-18</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.733</td> <th>  Cond. No.          </th> <td>    179.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="diag.html#cb5-1" tabindex="-1"></a>Salaries[<span class="st">&#39;pred_mlr&#39;</span>] <span class="op">=</span> model_mlr.predict()</span>
<span id="cb5-2"><a href="diag.html#cb5-2" tabindex="-1"></a>Salaries[<span class="st">&#39;resid_mlr&#39;</span>] <span class="op">=</span> model_mlr.resid</span>
<span id="cb5-3"><a href="diag.html#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="diag.html#cb5-4" tabindex="-1"></a>Salaries[[<span class="st">&#39;salary&#39;</span>, <span class="st">&#39;pred_mlr&#39;</span>, <span class="st">&#39;resid_mlr&#39;</span>]].head(n <span class="op">=</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##    salary       pred_mlr     resid_mlr
## 0  139750  131577.173919   8172.826081
## 1  173200  133091.263631  40108.736369
## 2   79750   85828.036746  -6078.036746
## 3  115000  135208.859230 -20208.859230
## 4  141500  131554.536390   9945.463610
## 5   97000   98337.194064  -1337.194064
## 6  175000  135015.236444  39984.763556
## 7  147765  132271.764939  15493.235061
## 8  119250  131668.259052 -12418.259052
## 9  129000  126258.622800   2741.377200</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="diag.html#cb7-1" tabindex="-1"></a>p<span class="op">=</span>(ggplot(Salaries, aes(x<span class="op">=</span><span class="st">&#39;pred_mlr&#39;</span>,y<span class="op">=</span><span class="st">&#39;resid_mlr&#39;</span>)) <span class="op">+</span> geom_point() <span class="op">+</span></span>
<span id="cb7-2"><a href="diag.html#cb7-2" tabindex="-1"></a>          geom_hline(yintercept <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> </span>
<span id="cb7-3"><a href="diag.html#cb7-3" tabindex="-1"></a>          labs(x<span class="op">=</span><span class="st">&quot;Fitted Values&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb7-4"><a href="diag.html#cb7-4" tabindex="-1"></a>         )</span>
<span id="cb7-5"><a href="diag.html#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a href="diag.html#cb7-6" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-7-1.png" width="614" /></p>
</div>
</div>
<div id="misspecified-model" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Misspecified Model<a href="diag.html#misspecified-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the assumptions assumes that the expected value of the response is accurately modeled by a linear function of the explanatory variables. If this is true, then we would expect our residual plots to be random scatter (in other words, all of the “signal” was correctly captured in the model and there is just noise left over).</p>
<div class="figure"><span style="display:block;" id="fig:correctspec"></span>
<img src="bookdownproj_files/figure-html/correctspec-3.png" alt="Ideal residual plot showing residual values randomly distributed with equal variance" width="672" />
<p class="caption">
Figure 5.3: Ideal residual plot showing residual values randomly distributed with equal variance
</p>
</div>
<p>Looking at the plot in Figure <a href="diag.html#fig:correctspec">5.3</a>, we see that there is no pattern. If you did see some type of pattern in this residual plot, it would indicate that you are missing something and need to do some more modeling. For example, a quadratic shape or curvilinear pattern to the residuals would indicate that one of our input variables has a nonlinear relationship to the response and transformations should be made to that input accordingly. For example, a residual scatter plot like Figure <a href="diag.html#fig:quadpattern">5.4</a> would prompt us to consider a quadratic term.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadpattern"></span>
<img src="bookdownproj_files/figure-html/quadpattern-1.png" alt="Residual plot indicating that a quadratic term is required" width="672" />
<p class="caption">
Figure 5.4: Residual plot indicating that a quadratic term is required
</p>
</div>
<p>If your model has more than one x, it is easier to see if an individual input variable has a quadratic relationship with the response when looking at plots like Figure <a href="diag.html#fig:quadpattern">5.4</a> where the input variable is on the x-axis.</p>
<p><strong>Example</strong></p>
<p>Let’s take a look at an example of where a quadratic linear regression is needed. This example is studying the effect of a chemical additive on paper strength. The response variable is the amount of force required to break the paper (strength) and the explanatory variable is the amount of chemical additive (amount).</p>
<div id="r-code-24" class="section level4 hasAnchor" number="5.2.0.1">
<h4><span class="header-section-number">5.2.0.1</span> R Code:<a href="diag.html#r-code-24" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="diag.html#cb8-1" tabindex="-1"></a>chemical <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb8-2"><a href="diag.html#cb8-2" tabindex="-1"></a> <span class="at">amount=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>),</span>
<span id="cb8-3"><a href="diag.html#cb8-3" tabindex="-1"></a> <span class="at">strength=</span><span class="fu">c</span>(<span class="fl">2.4</span>,<span class="fl">2.6</span>,<span class="fl">2.7</span>,<span class="fl">2.5</span>,<span class="fl">2.6</span>,<span class="fl">2.6</span>,<span class="fl">2.7</span>,<span class="fl">2.8</span>,<span class="fl">2.8</span>,<span class="fl">2.8</span>,<span class="fl">3.0</span>,<span class="fl">3.0</span>,<span class="fl">3.0</span>,<span class="fl">2.9</span>,<span class="fl">2.9</span>,<span class="fl">3.0</span>,<span class="fl">3.1</span>,<span class="fl">2.9</span>,<span class="fl">2.9</span>,<span class="fl">3.0</span>,<span class="fl">2.9</span>,<span class="fl">2.8</span>)</span>
<span id="cb8-4"><a href="diag.html#cb8-4" tabindex="-1"></a>)</span>
<span id="cb8-5"><a href="diag.html#cb8-5" tabindex="-1"></a> lm.quad<span class="ot">=</span><span class="fu">lm</span>(strength<span class="sc">~</span>amount, <span class="at">data=</span>chemical)</span>
<span id="cb8-6"><a href="diag.html#cb8-6" tabindex="-1"></a> <span class="fu">summary</span>(lm.quad)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = strength ~ amount, data = chemical)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.199780 -0.091850  0.004185  0.101707  0.206167 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.50176    0.06920  36.154  &lt; 2e-16 ***
## amount       0.09802    0.01998   4.907 8.52e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1283 on 20 degrees of freedom
## Multiple R-squared:  0.5462, Adjusted R-squared:  0.5236 
## F-statistic: 24.08 on 1 and 20 DF,  p-value: 8.518e-05</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="diag.html#cb10-1" tabindex="-1"></a> <span class="fu">ggplot</span>(lm.quad,<span class="fu">aes</span>(<span class="at">x=</span>amount,<span class="at">y=</span><span class="fu">resid</span>(lm.quad)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>,<span class="at">size=</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">labs</span>( <span class="at">x=</span><span class="st">&quot;Amount&quot;</span>, <span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadexample1"></span>
<img src="bookdownproj_files/figure-html/quadexample1-1.png" alt="Residual Plot Showing a Quadratic Relationship" width="672" />
<p class="caption">
Figure 5.5: Residual Plot Showing a Quadratic Relationship
</p>
</div>
<p>The above fitted model is
<span class="math display">\[\hat{Y}_{i} = 2.5 + 0.1x_{i}.\]</span>
However, after looking at the residual plot and noticing the quadratic shape, we realize that we need a higher order term for amount.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="diag.html#cb11-1" tabindex="-1"></a>chemical<span class="sc">$</span>amount.c<span class="ot">&lt;-</span><span class="fu">scale</span>(chemical<span class="sc">$</span>amount,<span class="at">scale=</span>F)</span>
<span id="cb11-2"><a href="diag.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="diag.html#cb11-3" tabindex="-1"></a>lm.quad<span class="ot">=</span><span class="fu">lm</span>(strength<span class="sc">~</span>amount.c <span class="sc">+</span> <span class="fu">I</span>(amount.c<span class="sc">^</span><span class="dv">2</span>),<span class="at">data=</span>chemical)</span>
<span id="cb11-4"><a href="diag.html#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="diag.html#cb11-5" tabindex="-1"></a><span class="fu">summary</span>(lm.quad)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = strength ~ amount.c + I(amount.c^2), data = chemical)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.22276 -0.06562 -0.02763  0.07602  0.19466 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    2.88358    0.03783  76.228  &lt; 2e-16 ***
## amount.c       0.09202    0.01807   5.091 6.49e-05 ***
## I(amount.c^2) -0.03728    0.01535  -2.428   0.0253 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.115 on 19 degrees of freedom
## Multiple R-squared:  0.6537, Adjusted R-squared:  0.6173 
## F-statistic: 17.93 on 2 and 19 DF,  p-value: 4.212e-05</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="diag.html#cb13-1" tabindex="-1"></a><span class="fu">ggplot</span>(lm.quad,<span class="fu">aes</span>(<span class="at">x=</span>amount.c,<span class="at">y=</span><span class="fu">resid</span>(lm.quad)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>,<span class="at">size=</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;Residual plot&quot;</span>, <span class="at">x=</span><span class="st">&quot;Amount&quot;</span>, <span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadexample2"></span>
<img src="bookdownproj_files/figure-html/quadexample2-1.png" alt="New Residual Plot after Fitting a Quadratic Term" width="672" />
<p class="caption">
Figure 5.6: New Residual Plot after Fitting a Quadratic Term
</p>
</div>
<p>The second order polynomial model is
<span class="math display">\[\hat{Y}_{i} = 2.88 + 0.09(x_{i}-\bar{x})-0.04(x_{i}-\bar{x})^{2}\]</span>
and the residuals from this model are shown in Figure <a href="diag.html#fig:quadexample2">5.6</a>. If you think there might still be some pattern in Figure <a href="diag.html#fig:quadexample2">5.6</a>, you could try a third degree polynomial:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="diag.html#cb14-1" tabindex="-1"></a>lm<span class="fl">.3</span><span class="ot">=</span><span class="fu">lm</span>(strength<span class="sc">~</span>amount.c<span class="sc">+</span><span class="fu">I</span>(amount.c<span class="sc">^</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">I</span>(amount.c<span class="sc">^</span><span class="dv">3</span>),<span class="at">data=</span>chemical)</span>
<span id="cb14-2"><a href="diag.html#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="diag.html#cb14-3" tabindex="-1"></a><span class="fu">summary</span>(lm<span class="fl">.3</span>)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = strength ~ amount.c + I(amount.c^2) + I(amount.c^3), 
##     data = chemical)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.15941 -0.06360  0.00272  0.08579  0.14142 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    2.89841    0.03495  82.934  &lt; 2e-16 ***
## amount.c       0.18335    0.04372   4.194 0.000546 ***
## I(amount.c^2) -0.04979    0.01500  -3.320 0.003805 ** 
## I(amount.c^3) -0.02862    0.01270  -2.254 0.036927 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1044 on 18 degrees of freedom
## Multiple R-squared:  0.7299, Adjusted R-squared:  0.6849 
## F-statistic: 16.22 on 3 and 18 DF,  p-value: 2.344e-05</code></pre>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="diag.html#cb16-1" tabindex="-1"></a><span class="fu">ggplot</span>(lm<span class="fl">.3</span>,<span class="fu">aes</span>(<span class="at">x=</span>amount.c,<span class="at">y=</span><span class="fu">resid</span>(lm<span class="fl">.3</span>)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>,<span class="at">size=</span><span class="dv">2</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Amount&quot;</span>, <span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadexample3"></span>
<img src="bookdownproj_files/figure-html/quadexample3-1.png" alt="New Residual Plot after Fitting a Cubic Term" width="672" />
<p class="caption">
Figure 5.7: New Residual Plot after Fitting a Cubic Term
</p>
</div>
<p>The new regression equation is now
<span class="math display">\[\hat{Y}_{i} = 2.9 + 0.18(x_{i}-\bar{x}) - 0.05(x_{i}-\bar{x})^{2}-0.03(x_{i}-\bar{x})^{3}.\]</span></p>
<p>In wrapping up the misspecified model, if a linear model does not look appropriate (there is a pattern in the residual plot), then you can try the following remedies:</p>
<ol style="list-style-type: decimal">
<li>Fit a polynomial or more complex regression model.<br />
</li>
<li>Transform the dependent and/or independent variables to obtain linearity.<br />
</li>
<li>Fit a nonlinear regression model, if appropriate (will need to decide the shape of a nonlinear model).<br />
</li>
<li>Fit a nonparametric regression model (for example splines or a LOESS regression).<br />
</li>
</ol>
</div>
<div id="python-code-25" class="section level3 hasAnchor" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Python Code:<a href="diag.html#python-code-25" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="diag.html#cb17-1" tabindex="-1"></a>chemical <span class="op">=</span> pd.DataFrame({<span class="st">&#39;amount&#39;</span>: [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>],</span>
<span id="cb17-2"><a href="diag.html#cb17-2" tabindex="-1"></a>                   <span class="st">&#39;strength&#39;</span>: [<span class="fl">2.4</span>,<span class="fl">2.6</span>,<span class="fl">2.7</span>,<span class="fl">2.5</span>,<span class="fl">2.6</span>,<span class="fl">2.6</span>,<span class="fl">2.7</span>,<span class="fl">2.8</span>,<span class="fl">2.8</span>,<span class="fl">2.8</span>,<span class="fl">3.0</span>,<span class="fl">3.0</span>,<span class="fl">3.0</span>,<span class="fl">2.9</span>,<span class="fl">2.9</span>,<span class="fl">3.0</span>,<span class="fl">3.1</span>,<span class="fl">2.9</span>,<span class="fl">2.9</span>,<span class="fl">3.0</span>,<span class="fl">2.9</span>,<span class="fl">2.8</span>]})</span>
<span id="cb17-3"><a href="diag.html#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="diag.html#cb17-4" tabindex="-1"></a>model_quad <span class="op">=</span> smf.ols(<span class="st">&quot;strength ~ amount&quot;</span>, data <span class="op">=</span> chemical).fit()</span>
<span id="cb17-5"><a href="diag.html#cb17-5" tabindex="-1"></a>model_quad.summary()                   </span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>strength</td>     <th>  R-squared:         </th> <td>   0.546</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.524</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   24.08</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>8.52e-05</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:04</td>     <th>  Log-Likelihood:    </th> <td>  15.001</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>  -26.00</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    20</td>      <th>  BIC:               </th> <td>  -23.82</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    2.5018</td> <td>    0.069</td> <td>   36.154</td> <td> 0.000</td> <td>    2.357</td> <td>    2.646</td>
</tr>
<tr>
  <th>amount</th>    <td>    0.0980</td> <td>    0.020</td> <td>    4.907</td> <td> 0.000</td> <td>    0.056</td> <td>    0.140</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.729</td> <th>  Durbin-Watson:     </th> <td>   1.144</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.694</td> <th>  Jarque-Bera (JB):  </th> <td>   0.681</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.082</td> <th>  Prob(JB):          </th> <td>   0.712</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.154</td> <th>  Cond. No.          </th> <td>    9.38</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="diag.html#cb18-1" tabindex="-1"></a>chemical[<span class="st">&#39;pred_mlr&#39;</span>] <span class="op">=</span> model_quad.predict()</span>
<span id="cb18-2"><a href="diag.html#cb18-2" tabindex="-1"></a>chemical[<span class="st">&#39;resid_mlr&#39;</span>] <span class="op">=</span> model_quad.resid</span>
<span id="cb18-3"><a href="diag.html#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="diag.html#cb18-4" tabindex="-1"></a></span>
<span id="cb18-5"><a href="diag.html#cb18-5" tabindex="-1"></a>p<span class="op">=</span>(ggplot(chemical, aes(x<span class="op">=</span><span class="st">&quot;amount&quot;</span>,y<span class="op">=</span><span class="st">&quot;resid_mlr&quot;</span>)) <span class="op">+</span>geom_point(color<span class="op">=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb18-6"><a href="diag.html#cb18-6" tabindex="-1"></a>          geom_hline(yintercept <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> </span>
<span id="cb18-7"><a href="diag.html#cb18-7" tabindex="-1"></a>          labs(x<span class="op">=</span><span class="st">&quot;Amount&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb18-8"><a href="diag.html#cb18-8" tabindex="-1"></a>         )</span>
<span id="cb18-9"><a href="diag.html#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a href="diag.html#cb18-10" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-9-1.png" width="614" /></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="diag.html#cb19-1" tabindex="-1"></a>chemical[<span class="st">&#39;amount_c&#39;</span>] <span class="op">=</span> chemical[<span class="st">&#39;amount&#39;</span>] <span class="op">-</span> np.mean(chemical[<span class="st">&#39;amount&#39;</span>])</span>
<span id="cb19-2"><a href="diag.html#cb19-2" tabindex="-1"></a>model_quad2 <span class="op">=</span> smf.ols(<span class="st">&quot;strength ~ amount_c + I(amount_c**2)&quot;</span>, data <span class="op">=</span> chemical).fit()</span>
<span id="cb19-3"><a href="diag.html#cb19-3" tabindex="-1"></a>model_quad2.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>strength</td>     <th>  R-squared:         </th> <td>   0.654</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.617</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   17.93</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>4.21e-05</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:05</td>     <th>  Log-Likelihood:    </th> <td>  17.974</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>  -29.95</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    19</td>      <th>  BIC:               </th> <td>  -26.68</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>    2.8836</td> <td>    0.038</td> <td>   76.228</td> <td> 0.000</td> <td>    2.804</td> <td>    2.963</td>
</tr>
<tr>
  <th>amount_c</th>         <td>    0.0920</td> <td>    0.018</td> <td>    5.091</td> <td> 0.000</td> <td>    0.054</td> <td>    0.130</td>
</tr>
<tr>
  <th>I(amount_c ** 2)</th> <td>   -0.0373</td> <td>    0.015</td> <td>   -2.428</td> <td> 0.025</td> <td>   -0.069</td> <td>   -0.005</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.468</td> <th>  Durbin-Watson:     </th> <td>   1.658</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.791</td> <th>  Jarque-Bera (JB):  </th> <td>   0.558</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.013</td> <th>  Prob(JB):          </th> <td>   0.757</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.221</td> <th>  Cond. No.          </th> <td>    4.21</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="diag.html#cb20-1" tabindex="-1"></a>chemical[<span class="st">&#39;pred_mlr2&#39;</span>] <span class="op">=</span> model_quad2.predict()</span>
<span id="cb20-2"><a href="diag.html#cb20-2" tabindex="-1"></a>chemical[<span class="st">&#39;resid_mlr2&#39;</span>] <span class="op">=</span> model_quad2.resid</span>
<span id="cb20-3"><a href="diag.html#cb20-3" tabindex="-1"></a></span>
<span id="cb20-4"><a href="diag.html#cb20-4" tabindex="-1"></a>p<span class="op">=</span>(ggplot(chemical, aes(x<span class="op">=</span><span class="st">&quot;amount_c&quot;</span>,y<span class="op">=</span><span class="st">&quot;resid_mlr2&quot;</span>)) <span class="op">+</span>geom_point(color<span class="op">=</span><span class="st">&quot;orange&quot;</span>) <span class="op">+</span></span>
<span id="cb20-5"><a href="diag.html#cb20-5" tabindex="-1"></a>          geom_hline(yintercept <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> </span>
<span id="cb20-6"><a href="diag.html#cb20-6" tabindex="-1"></a>          labs(x<span class="op">=</span><span class="st">&quot;Amount&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb20-7"><a href="diag.html#cb20-7" tabindex="-1"></a>         )</span>
<span id="cb20-8"><a href="diag.html#cb20-8" tabindex="-1"></a></span>
<span id="cb20-9"><a href="diag.html#cb20-9" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-10-3.png" width="614" /></p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="diag.html#cb21-1" tabindex="-1"></a>model_quad3 <span class="op">=</span> smf.ols(<span class="st">&quot;strength ~ amount_c + I(amount_c**2) + I(amount_c**3)&quot;</span>, data <span class="op">=</span> chemical).fit()</span>
<span id="cb21-2"><a href="diag.html#cb21-2" tabindex="-1"></a>model_quad3.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>        <td>strength</td>     <th>  R-squared:         </th> <td>   0.730</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.685</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   16.22</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>2.34e-05</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:05</td>     <th>  Log-Likelihood:    </th> <td>  20.708</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    22</td>      <th>  AIC:               </th> <td>  -33.42</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    18</td>      <th>  BIC:               </th> <td>  -29.05</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>        <td>    2.8984</td> <td>    0.035</td> <td>   82.934</td> <td> 0.000</td> <td>    2.825</td> <td>    2.972</td>
</tr>
<tr>
  <th>amount_c</th>         <td>    0.1834</td> <td>    0.044</td> <td>    4.194</td> <td> 0.001</td> <td>    0.091</td> <td>    0.275</td>
</tr>
<tr>
  <th>I(amount_c ** 2)</th> <td>   -0.0498</td> <td>    0.015</td> <td>   -3.320</td> <td> 0.004</td> <td>   -0.081</td> <td>   -0.018</td>
</tr>
<tr>
  <th>I(amount_c ** 3)</th> <td>   -0.0286</td> <td>    0.013</td> <td>   -2.254</td> <td> 0.037</td> <td>   -0.055</td> <td>   -0.002</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 2.244</td> <th>  Durbin-Watson:     </th> <td>   1.733</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.326</td> <th>  Jarque-Bera (JB):  </th> <td>   1.118</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.064</td> <th>  Prob(JB):          </th> <td>   0.572</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 1.903</td> <th>  Cond. No.          </th> <td>    10.6</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="diag.html#cb22-1" tabindex="-1"></a>chemical[<span class="st">&#39;pred_mlr3&#39;</span>] <span class="op">=</span> model_quad3.predict()</span>
<span id="cb22-2"><a href="diag.html#cb22-2" tabindex="-1"></a>chemical[<span class="st">&#39;resid_mlr3&#39;</span>] <span class="op">=</span> model_quad3.resid</span>
<span id="cb22-3"><a href="diag.html#cb22-3" tabindex="-1"></a></span>
<span id="cb22-4"><a href="diag.html#cb22-4" tabindex="-1"></a>p<span class="op">=</span>(ggplot(chemical, aes(x<span class="op">=</span><span class="st">&quot;amount_c&quot;</span>,y<span class="op">=</span><span class="st">&quot;resid_mlr3&quot;</span>)) <span class="op">+</span>geom_point(color<span class="op">=</span><span class="st">&quot;orange&quot;</span>) <span class="op">+</span></span>
<span id="cb22-5"><a href="diag.html#cb22-5" tabindex="-1"></a>          geom_hline(yintercept <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> </span>
<span id="cb22-6"><a href="diag.html#cb22-6" tabindex="-1"></a>          labs(x<span class="op">=</span><span class="st">&quot;Amount&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb22-7"><a href="diag.html#cb22-7" tabindex="-1"></a>         )</span>
<span id="cb22-8"><a href="diag.html#cb22-8" tabindex="-1"></a></span>
<span id="cb22-9"><a href="diag.html#cb22-9" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-11-5.png" width="614" /></p>
</div>
</div>
<div id="constant-variance" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Constant Variance<a href="diag.html#constant-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another assumption for linear regression is that the variance is constant about the line. Looking at Figure <a href="diag.html#fig:constvar">5.8</a>, you see that the variation about the line is constant across the line (notice the bands that have been drawn around the line).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:constvar"></span>
<img src="bookdownproj_files/figure-html/constvar-7.png" alt="Residual Plot Showing Non-Constant Variance" width="672" />
<p class="caption">
Figure 5.8: Residual Plot Showing Non-Constant Variance
</p>
</div>
<p>However, an example of where this is not true is shown in Figure <a href="diag.html#fig:nonconstvar">5.9</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nonconstvar"></span>
<img src="bookdownproj_files/figure-html/nonconstvar-1.png" alt="Residual Plot Showing Non-Constant Variance" width="672" />
<p class="caption">
Figure 5.9: Residual Plot Showing Non-Constant Variance
</p>
</div>
<p>Notice how the variability increases as the predicted values increase (the bands get wider). This is referred to as heteroskedasticity in the variance, which violates the constant variance assumption (homoskedasticity). The homoskedastic assumption is about the regression line, so it is best to look at the plot of residuals versus predicted values (not individual x-values). There are a few tests for this assumption, but they are limited in what they are able to test. Best way to evaluate this assumption is by visualizing the residual plot and make a judgement call. If the variance appears to be heteroskadastic, any inferences under the traditional assumptions will be incorrect. In other words, hypothesis tests and confidence intervals based on the t, F, and <span class="math inline">\(\chi^{2}\)</span> distributions will not be valid.</p>
<p><strong>Example</strong></p>
<p>The following fictious salary data set is from the online textbook <a href="https://daviddalpiaz.github.io/appliedstats/index.html"><em>Applied Statistics with R</em></a>. The explanatory variable is number of years employment and the response variable is annual salary.</p>
<div id="r-code-25" class="section level4 hasAnchor" number="5.3.0.1">
<h4><span class="header-section-number">5.3.0.1</span> R code:<a href="diag.html#r-code-25" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="diag.html#cb23-1" tabindex="-1"></a>years<span class="ot">=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">11</span>,<span class="dv">11</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">12</span>,<span class="dv">12</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">13</span>,<span class="dv">13</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">18</span>,<span class="dv">18</span>,<span class="dv">18</span>,<span class="dv">18</span>,<span class="dv">19</span>,<span class="dv">19</span>,<span class="dv">19</span>,<span class="dv">19</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">23</span>,<span class="dv">23</span>,<span class="dv">23</span>,<span class="dv">23</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">25</span>,<span class="dv">25</span>,<span class="dv">25</span>)</span>
<span id="cb23-2"><a href="diag.html#cb23-2" tabindex="-1"></a></span>
<span id="cb23-3"><a href="diag.html#cb23-3" tabindex="-1"></a>salary<span class="ot">=</span><span class="fu">c</span>(<span class="dv">41504</span>,<span class="dv">32619</span>,<span class="dv">44322</span>,<span class="dv">40038</span>,<span class="dv">46147</span>,<span class="dv">38447</span>,<span class="dv">38163</span>,<span class="dv">42104</span>,<span class="dv">25597</span>,<span class="dv">39599</span>,<span class="dv">55698</span>,<span class="dv">47220</span>,<span class="dv">65929</span>,<span class="dv">55794</span>,<span class="dv">45959</span>,<span class="dv">52460</span>,<span class="dv">60308</span>,<span class="dv">61458</span>,<span class="dv">56951</span>,<span class="dv">56174</span>,<span class="dv">59363</span>,<span class="dv">57642</span>,<span class="dv">69792</span>,<span class="dv">59321</span>,<span class="dv">66379</span>,<span class="dv">64282</span>,<span class="dv">48901</span>,<span class="dv">100711</span>,<span class="dv">59324</span>,<span class="dv">54752</span>,<span class="dv">73619</span>,<span class="dv">65382</span>,<span class="dv">58823</span>,<span class="dv">65717</span>,<span class="dv">92816</span>,<span class="dv">72550</span>,<span class="dv">71365</span>,<span class="dv">88888</span>,<span class="dv">62969</span>,<span class="dv">45298</span>,<span class="dv">111292</span>,<span class="dv">91491</span>,<span class="dv">106345</span>,<span class="dv">99009</span>,<span class="dv">73981</span>,<span class="dv">72547</span>,<span class="dv">74991</span>,<span class="dv">139249</span>,<span class="dv">119948</span>,<span class="dv">128962</span>,<span class="dv">98112</span>,<span class="dv">97159</span>,<span class="dv">125246</span>,<span class="dv">89694</span>,<span class="dv">73333</span>,<span class="dv">108710</span>,<span class="dv">97567</span>,<span class="dv">90359</span>,<span class="dv">119806</span>,<span class="dv">101343</span>,<span class="dv">147406</span>,<span class="dv">153020</span>,<span class="dv">143200</span>,<span class="dv">97327</span>,<span class="dv">184807</span>,<span class="dv">146263</span>,<span class="dv">127925</span>,<span class="dv">159785</span>,<span class="dv">174822</span>,<span class="dv">177610</span>,<span class="dv">210984</span>,<span class="dv">160044</span>,<span class="dv">137044</span>,<span class="dv">182996</span>,<span class="dv">184183</span>,<span class="dv">168666</span>,<span class="dv">121350</span>,<span class="dv">193627</span>,<span class="dv">142611</span>,<span class="dv">170131</span>,<span class="dv">134140</span>,<span class="dv">129446</span>,<span class="dv">201469</span>,<span class="dv">202104</span>,<span class="dv">220556</span>,<span class="dv">166419</span>,<span class="dv">149044</span>,<span class="dv">247017</span>,<span class="dv">247730</span>,<span class="dv">252917</span>,<span class="dv">235517</span>,<span class="dv">241276</span>,<span class="dv">197229</span>,<span class="dv">175879</span>,<span class="dv">253682</span>,<span class="dv">262578</span>,<span class="dv">207715</span>,<span class="dv">221179</span>,<span class="dv">212028</span>,<span class="dv">312549</span>)</span>
<span id="cb23-4"><a href="diag.html#cb23-4" tabindex="-1"></a></span>
<span id="cb23-5"><a href="diag.html#cb23-5" tabindex="-1"></a>lm.var<span class="ot">=</span><span class="fu">lm</span>(salary<span class="sc">~</span>years)</span>
<span id="cb23-6"><a href="diag.html#cb23-6" tabindex="-1"></a></span>
<span id="cb23-7"><a href="diag.html#cb23-7" tabindex="-1"></a><span class="fu">ggplot</span>(lm.var,<span class="fu">aes</span>(<span class="at">x=</span><span class="fu">fitted</span>(lm.var),<span class="at">y=</span><span class="fu">resid</span>(lm.var)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;blue&quot;</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;Residual Plot&quot;</span>, <span class="at">x=</span><span class="st">&quot;Predicted Values&quot;</span>,<span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varexample"></span>
<img src="bookdownproj_files/figure-html/varexample-1.png" alt="Residuals With Heteroskedasticity" width="672" />
<p class="caption">
Figure 5.10: Residuals With Heteroskedasticity
</p>
</div>
<p>As you can see from the residual plot in Figure <a href="diag.html#fig:varexample">5.10</a>, the graph exhibits a fan-shape and the variance is increasing as the predicted values get larger. We can apply a logarithmic transform to try to stabilize the variance, the result of which is shown in Figure <a href="diag.html#fig:correctexam">5.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:correctexam"></span>
<img src="bookdownproj_files/figure-html/correctexam-1.png" alt="Residual Plot Showing Stabilized Variance after Log Transform" width="672" />
<p class="caption">
Figure 5.11: Residual Plot Showing Stabilized Variance after Log Transform
</p>
</div>
<p>The log transform stabilized the variance and produces a better residual plot.</p>
<p>Another way to handle heteroscedacity is to perform a Weighted Least Squares. The hardest part of WLS is how to identify the weights. For example:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="diag.html#cb24-1" tabindex="-1"></a>x<span class="ot">&lt;-</span><span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">10</span>,<span class="at">each=</span><span class="dv">5</span>))</span>
<span id="cb24-2"><a href="diag.html#cb24-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">26785</span>)</span>
<span id="cb24-3"><a href="diag.html#cb24-3" tabindex="-1"></a>error<span class="ot">&lt;-</span><span class="fu">rnorm</span>(<span class="fu">length</span>(x),<span class="dv">0</span>,x)</span>
<span id="cb24-4"><a href="diag.html#cb24-4" tabindex="-1"></a>y<span class="ot">&lt;-</span><span class="dv">50</span> <span class="sc">+</span> <span class="fl">1.2</span><span class="sc">*</span>x <span class="sc">+</span> error</span>
<span id="cb24-5"><a href="diag.html#cb24-5" tabindex="-1"></a>model1<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb24-6"><a href="diag.html#cb24-6" tabindex="-1"></a><span class="fu">ggplot</span>(model1, <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> .resid)) <span class="sc">+</span></span>
<span id="cb24-7"><a href="diag.html#cb24-7" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="diag.html#cb25-1" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -20.3750  -3.4632  -0.3275   5.4592  14.8181 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  50.2878     2.8033  17.939   &lt;2e-16 ***
## x             0.9811     0.4292   2.286   0.0272 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.433 on 43 degrees of freedom
## Multiple R-squared:  0.1084, Adjusted R-squared:  0.08763 
## F-statistic: 5.226 on 1 and 43 DF,  p-value: 0.02724</code></pre>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="diag.html#cb27-1" tabindex="-1"></a><span class="co"># weights using standard deviations (not usually known)</span></span>
<span id="cb27-2"><a href="diag.html#cb27-2" tabindex="-1"></a>weights1<span class="ot">&lt;-</span><span class="dv">1</span><span class="sc">/</span>x<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb27-3"><a href="diag.html#cb27-3" tabindex="-1"></a>model2<span class="ot">&lt;-</span><span class="fu">lm</span>(y<span class="sc">~</span>x,<span class="at">weights =</span> weights1)</span>
<span id="cb27-4"><a href="diag.html#cb27-4" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, weights = weights1)
## 
## Weighted Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.57601 -0.79232 -0.06276  1.03159  1.83044 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  49.9844     1.4734  33.926  &lt; 2e-16 ***
## x             1.0408     0.3641   2.858  0.00654 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.216 on 43 degrees of freedom
## Multiple R-squared:  0.1597, Adjusted R-squared:  0.1401 
## F-statistic:  8.17 on 1 and 43 DF,  p-value: 0.006542</code></pre>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="diag.html#cb29-1" tabindex="-1"></a><span class="fu">ggplot</span>(model2, <span class="fu">aes</span>(<span class="at">x =</span> .fitted, <span class="at">y =</span> .resid)) <span class="sc">+</span></span>
<span id="cb29-2"><a href="diag.html#cb29-2" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-12-2.png" width="672" /></p>
<p>Notice that the estimates do NOT change from OLS to WLS!! Just the standard errors. Another technique is to use robust standard errors. This can be done within the estimatr package:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="diag.html#cb30-1" tabindex="-1"></a><span class="fu">library</span>(estimatr)</span>
<span id="cb30-2"><a href="diag.html#cb30-2" tabindex="-1"></a>model3<span class="ot">&lt;-</span><span class="fu">lm_robust</span>(y<span class="sc">~</span>x,<span class="at">se_type =</span> <span class="st">&quot;HC1&quot;</span>)</span>
<span id="cb30-3"><a href="diag.html#cb30-3" tabindex="-1"></a><span class="fu">summary</span>(model3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm_robust(formula = y ~ x, se_type = &quot;HC1&quot;)
## 
## Standard error type:  HC1 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper DF
## (Intercept)  50.2878     1.8577  27.070 1.216e-28  46.5413   54.034 43
## x             0.9811     0.3802   2.581 1.336e-02   0.2144    1.748 43
## 
## Multiple R-squared:  0.1084 ,    Adjusted R-squared:  0.08763 
## F-statistic: 6.659 on 1 and 43 DF,  p-value: 0.01336</code></pre>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="diag.html#cb32-1" tabindex="-1"></a>model4<span class="ot">&lt;-</span><span class="fu">lm_robust</span>(y<span class="sc">~</span>x)</span>
<span id="cb32-2"><a href="diag.html#cb32-2" tabindex="-1"></a><span class="fu">summary</span>(model4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm_robust(formula = y ~ x)
## 
## Standard error type:  HC2 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper DF
## (Intercept)  50.2878     1.8672  26.932 1.496e-28  46.5222   54.053 43
## x             0.9811     0.3825   2.565 1.390e-02   0.2096    1.753 43
## 
## Multiple R-squared:  0.1084 ,    Adjusted R-squared:  0.08763 
## F-statistic: 6.578 on 1 and 43 DF,  p-value: 0.0139</code></pre>
<p>In conclusion, if the assumption of Homoskadicity of variance is violated, you can try:</p>
<ol style="list-style-type: decimal">
<li>Transform data.</li>
<li>Use Weighted Least Squares (WLS) or iteratively reweighted least squares (IRLS).</li>
<li>Use robust standard errors.</li>
<li>Use a different distribution (for example if the response is count data, use Poisson distribution).</li>
</ol>
</div>
<div id="python-code-26" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Python Code:<a href="diag.html#python-code-26" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb34"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="diag.html#cb34-1" tabindex="-1"></a>experience <span class="op">=</span> pd.DataFrame({<span class="st">&#39;salary&#39;</span>: [<span class="dv">41504</span>,<span class="dv">32619</span>,<span class="dv">44322</span>,<span class="dv">40038</span>,<span class="dv">46147</span>,<span class="dv">38447</span>,<span class="dv">38163</span>,<span class="dv">42104</span>,<span class="dv">25597</span>,<span class="dv">39599</span>,<span class="dv">55698</span>,<span class="dv">47220</span>,<span class="dv">65929</span>,<span class="dv">55794</span>,<span class="dv">45959</span>,<span class="dv">52460</span>,<span class="dv">60308</span>,<span class="dv">61458</span>,<span class="dv">56951</span>,<span class="dv">56174</span>,<span class="dv">59363</span>,<span class="dv">57642</span>,<span class="dv">69792</span>,<span class="dv">59321</span>,<span class="dv">66379</span>,<span class="dv">64282</span>,<span class="dv">48901</span>,<span class="dv">100711</span>,<span class="dv">59324</span>,<span class="dv">54752</span>,<span class="dv">73619</span>,<span class="dv">65382</span>,<span class="dv">58823</span>,<span class="dv">65717</span>,<span class="dv">92816</span>,<span class="dv">72550</span>,<span class="dv">71365</span>,<span class="dv">88888</span>,<span class="dv">62969</span>,<span class="dv">45298</span>,<span class="dv">111292</span>,<span class="dv">91491</span>,<span class="dv">106345</span>,<span class="dv">99009</span>,<span class="dv">73981</span>,<span class="dv">72547</span>,<span class="dv">74991</span>,<span class="dv">139249</span>,<span class="dv">119948</span>,<span class="dv">128962</span>,<span class="dv">98112</span>,<span class="dv">97159</span>,<span class="dv">125246</span>,<span class="dv">89694</span>,<span class="dv">73333</span>,<span class="dv">108710</span>,<span class="dv">97567</span>,<span class="dv">90359</span>,<span class="dv">119806</span>,<span class="dv">101343</span>,<span class="dv">147406</span>,<span class="dv">153020</span>,<span class="dv">143200</span>,<span class="dv">97327</span>,<span class="dv">184807</span>,<span class="dv">146263</span>,<span class="dv">127925</span>,<span class="dv">159785</span>,<span class="dv">174822</span>,<span class="dv">177610</span>,<span class="dv">210984</span>,<span class="dv">160044</span>,<span class="dv">137044</span>,<span class="dv">182996</span>,<span class="dv">184183</span>,<span class="dv">168666</span>,<span class="dv">121350</span>,<span class="dv">193627</span>,<span class="dv">142611</span>,<span class="dv">170131</span>,<span class="dv">134140</span>,<span class="dv">129446</span>,<span class="dv">201469</span>,<span class="dv">202104</span>,<span class="dv">220556</span>,<span class="dv">166419</span>,<span class="dv">149044</span>,<span class="dv">247017</span>,<span class="dv">247730</span>,<span class="dv">252917</span>,<span class="dv">235517</span>,<span class="dv">241276</span>,<span class="dv">197229</span>,<span class="dv">175879</span>,<span class="dv">253682</span>,<span class="dv">262578</span>,<span class="dv">207715</span>,<span class="dv">221179</span>,<span class="dv">212028</span>,<span class="dv">312549</span>],</span>
<span id="cb34-2"><a href="diag.html#cb34-2" tabindex="-1"></a>                   <span class="st">&#39;years&#39;</span>: [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">7</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">9</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">10</span>,<span class="dv">11</span>,<span class="dv">11</span>,<span class="dv">11</span>,<span class="dv">11</span>,<span class="dv">12</span>,<span class="dv">12</span>,<span class="dv">12</span>,<span class="dv">12</span>,<span class="dv">13</span>,<span class="dv">13</span>,<span class="dv">13</span>,<span class="dv">13</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">14</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">17</span>,<span class="dv">18</span>,<span class="dv">18</span>,<span class="dv">18</span>,<span class="dv">18</span>,<span class="dv">19</span>,<span class="dv">19</span>,<span class="dv">19</span>,<span class="dv">19</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">21</span>,<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">22</span>,<span class="dv">23</span>,<span class="dv">23</span>,<span class="dv">23</span>,<span class="dv">23</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">24</span>,<span class="dv">25</span>,<span class="dv">25</span>,<span class="dv">25</span>]})</span>
<span id="cb34-3"><a href="diag.html#cb34-3" tabindex="-1"></a></span>
<span id="cb34-4"><a href="diag.html#cb34-4" tabindex="-1"></a>model_var <span class="op">=</span> smf.ols(<span class="st">&quot;salary ~ years&quot;</span>, data <span class="op">=</span> experience).fit()</span>
<span id="cb34-5"><a href="diag.html#cb34-5" tabindex="-1"></a>model_var.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>salary</td>      <th>  R-squared:         </th> <td>   0.834</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.832</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   492.8</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>5.16e-40</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:07</td>     <th>  Log-Likelihood:    </th> <td> -1162.5</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   2329.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>   2334.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td> 5302.0807</td> <td> 5749.963</td> <td>    0.922</td> <td> 0.359</td> <td>-6108.534</td> <td> 1.67e+04</td>
</tr>
<tr>
  <th>years</th>     <td> 8636.6123</td> <td>  389.044</td> <td>   22.200</td> <td> 0.000</td> <td> 7864.567</td> <td> 9408.658</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.672</td> <th>  Durbin-Watson:     </th> <td>   1.331</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.433</td> <th>  Jarque-Bera (JB):  </th> <td>   1.129</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.212</td> <th>  Prob(JB):          </th> <td>   0.569</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.303</td> <th>  Cond. No.          </th> <td>    31.2</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="diag.html#cb35-1" tabindex="-1"></a>experience[<span class="st">&#39;pred_mlr&#39;</span>] <span class="op">=</span> model_var.predict()</span>
<span id="cb35-2"><a href="diag.html#cb35-2" tabindex="-1"></a>experience[<span class="st">&#39;resid_mlr&#39;</span>] <span class="op">=</span> model_var.resid</span>
<span id="cb35-3"><a href="diag.html#cb35-3" tabindex="-1"></a></span>
<span id="cb35-4"><a href="diag.html#cb35-4" tabindex="-1"></a>p<span class="op">=</span>(ggplot(experience, aes(x<span class="op">=</span><span class="st">&quot;pred_mlr&quot;</span>,y<span class="op">=</span><span class="st">&quot;resid_mlr&quot;</span>)) <span class="op">+</span>geom_point(color<span class="op">=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb35-5"><a href="diag.html#cb35-5" tabindex="-1"></a>          geom_hline(yintercept <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> </span>
<span id="cb35-6"><a href="diag.html#cb35-6" tabindex="-1"></a>          labs(x<span class="op">=</span><span class="st">&quot;Predicted Values&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb35-7"><a href="diag.html#cb35-7" tabindex="-1"></a>         )</span>
<span id="cb35-8"><a href="diag.html#cb35-8" tabindex="-1"></a></span>
<span id="cb35-9"><a href="diag.html#cb35-9" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-15-1.png" width="614" /></p>
<p>To perform spearman’s rank test:</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="diag.html#cb36-1" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> spearmanr</span>
<span id="cb36-2"><a href="diag.html#cb36-2" tabindex="-1"></a></span>
<span id="cb36-3"><a href="diag.html#cb36-3" tabindex="-1"></a>experience[<span class="st">&#39;abs_resid&#39;</span>]<span class="op">=</span>experience[<span class="st">&#39;resid_mlr&#39;</span>].<span class="bu">abs</span>()</span>
<span id="cb36-4"><a href="diag.html#cb36-4" tabindex="-1"></a></span>
<span id="cb36-5"><a href="diag.html#cb36-5" tabindex="-1"></a>correlation, p_value <span class="op">=</span> spearmanr(experience[<span class="st">&#39;pred_mlr&#39;</span>], experience[<span class="st">&#39;abs_resid&#39;</span>])</span>
<span id="cb36-6"><a href="diag.html#cb36-6" tabindex="-1"></a></span>
<span id="cb36-7"><a href="diag.html#cb36-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Spearman correlation: </span><span class="sc">{</span>correlation<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## Spearman correlation: 0.30767299844102397</code></pre>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="diag.html#cb38-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;P-value: </span><span class="sc">{</span>p_value<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## P-value: 0.001846005094365443</code></pre>
<p>Now let’s do the log transformation:</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="diag.html#cb40-1" tabindex="-1"></a>experience[<span class="st">&#39;log_salary&#39;</span>] <span class="op">=</span> np.log(experience[<span class="st">&#39;salary&#39;</span>])</span>
<span id="cb40-2"><a href="diag.html#cb40-2" tabindex="-1"></a></span>
<span id="cb40-3"><a href="diag.html#cb40-3" tabindex="-1"></a>model_var2 <span class="op">=</span> smf.ols(<span class="st">&quot;log_salary ~ years&quot;</span>, data <span class="op">=</span> experience).fit()</span>
<span id="cb40-4"><a href="diag.html#cb40-4" tabindex="-1"></a>model_var2.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>log_salary</td>    <th>  R-squared:         </th> <td>   0.891</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.890</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   805.2</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>4.63e-49</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:08</td>     <th>  Log-Likelihood:    </th> <td>  22.359</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>  -40.72</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    98</td>      <th>  BIC:               </th> <td>  -35.51</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   10.4838</td> <td>    0.041</td> <td>  255.179</td> <td> 0.000</td> <td>   10.402</td> <td>   10.565</td>
</tr>
<tr>
  <th>years</th>     <td>    0.0789</td> <td>    0.003</td> <td>   28.376</td> <td> 0.000</td> <td>    0.073</td> <td>    0.084</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 3.204</td> <th>  Durbin-Watson:     </th> <td>   1.818</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.202</td> <th>  Jarque-Bera (JB):  </th> <td>   2.855</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.413</td> <th>  Prob(JB):          </th> <td>   0.240</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.049</td> <th>  Cond. No.          </th> <td>    31.2</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="diag.html#cb41-1" tabindex="-1"></a>experience[<span class="st">&#39;pred_mlr2&#39;</span>] <span class="op">=</span> model_var2.predict()</span>
<span id="cb41-2"><a href="diag.html#cb41-2" tabindex="-1"></a>experience[<span class="st">&#39;resid_mlr2&#39;</span>] <span class="op">=</span> model_var2.resid</span>
<span id="cb41-3"><a href="diag.html#cb41-3" tabindex="-1"></a></span>
<span id="cb41-4"><a href="diag.html#cb41-4" tabindex="-1"></a>p <span class="op">=</span> (ggplot(experience, aes(x<span class="op">=</span><span class="st">&#39;pred_mlr2&#39;</span>,y<span class="op">=</span><span class="st">&#39;resid_mlr2&#39;</span>)) <span class="op">+</span> </span>
<span id="cb41-5"><a href="diag.html#cb41-5" tabindex="-1"></a>geom_point(color<span class="op">=</span><span class="st">&quot;orange&quot;</span>) <span class="op">+</span></span>
<span id="cb41-6"><a href="diag.html#cb41-6" tabindex="-1"></a>labs(x<span class="op">=</span><span class="st">&quot;Predicted Values&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>, title<span class="op">=</span><span class="st">&quot;Residual Plot&quot;</span>)</span>
<span id="cb41-7"><a href="diag.html#cb41-7" tabindex="-1"></a>)</span>
<span id="cb41-8"><a href="diag.html#cb41-8" tabindex="-1"></a></span>
<span id="cb41-9"><a href="diag.html#cb41-9" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-17-3.png" width="614" /></p>
<p>We can also do the weighted LS regression in Python:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="diag.html#cb42-1" tabindex="-1"></a>x <span class="op">=</span> np.repeat(np.arange(<span class="dv">2</span>, <span class="dv">11</span>), <span class="dv">5</span>)</span>
<span id="cb42-2"><a href="diag.html#cb42-2" tabindex="-1"></a></span>
<span id="cb42-3"><a href="diag.html#cb42-3" tabindex="-1"></a><span class="co"># Set seed and generate heteroscedastic error</span></span>
<span id="cb42-4"><a href="diag.html#cb42-4" tabindex="-1"></a>np.random.seed(<span class="dv">26785</span>)</span>
<span id="cb42-5"><a href="diag.html#cb42-5" tabindex="-1"></a>error <span class="op">=</span> np.random.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span>x)</span>
<span id="cb42-6"><a href="diag.html#cb42-6" tabindex="-1"></a></span>
<span id="cb42-7"><a href="diag.html#cb42-7" tabindex="-1"></a><span class="co"># Generate y</span></span>
<span id="cb42-8"><a href="diag.html#cb42-8" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">50</span> <span class="op">+</span> <span class="fl">1.2</span> <span class="op">*</span> x <span class="op">+</span> error</span>
<span id="cb42-9"><a href="diag.html#cb42-9" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">&#39;x&#39;</span>: x, <span class="st">&#39;y&#39;</span>: y})</span>
<span id="cb42-10"><a href="diag.html#cb42-10" tabindex="-1"></a></span>
<span id="cb42-11"><a href="diag.html#cb42-11" tabindex="-1"></a>model1 <span class="op">=</span> smf.ols(<span class="st">&quot;y~x&quot;</span>,data<span class="op">=</span>df).fit()</span>
<span id="cb42-12"><a href="diag.html#cb42-12" tabindex="-1"></a>model1.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.210</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.192</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   11.43</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th>  <td>0.00155</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:08</td>     <th>  Log-Likelihood:    </th> <td> -144.58</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    45</td>      <th>  AIC:               </th> <td>   293.2</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    43</td>      <th>  BIC:               </th> <td>   296.8</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   49.9070</td> <td>    2.320</td> <td>   21.514</td> <td> 0.000</td> <td>   45.229</td> <td>   54.585</td>
</tr>
<tr>
  <th>x</th>         <td>    1.2007</td> <td>    0.355</td> <td>    3.381</td> <td> 0.002</td> <td>    0.485</td> <td>    1.917</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 3.109</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.211</td> <th>  Jarque-Bera (JB):  </th> <td>   2.458</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.112</td> <th>  Prob(JB):          </th> <td>   0.293</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 4.123</td> <th>  Cond. No.          </th> <td>    16.9</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="diag.html#cb43-1" tabindex="-1"></a>df[<span class="st">&#39;pred&#39;</span>] <span class="op">=</span> model1.predict()</span>
<span id="cb43-2"><a href="diag.html#cb43-2" tabindex="-1"></a>df[<span class="st">&#39;resid&#39;</span>] <span class="op">=</span> model1.resid</span>
<span id="cb43-3"><a href="diag.html#cb43-3" tabindex="-1"></a></span>
<span id="cb43-4"><a href="diag.html#cb43-4" tabindex="-1"></a>p<span class="op">=</span>(ggplot(df, aes(x<span class="op">=</span><span class="st">&quot;pred&quot;</span>,y<span class="op">=</span><span class="st">&quot;resid&quot;</span>)) <span class="op">+</span>geom_point(color<span class="op">=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span></span>
<span id="cb43-5"><a href="diag.html#cb43-5" tabindex="-1"></a>          geom_hline(yintercept <span class="op">=</span> <span class="dv">0</span>) <span class="op">+</span> </span>
<span id="cb43-6"><a href="diag.html#cb43-6" tabindex="-1"></a>          labs(x<span class="op">=</span><span class="st">&quot;Predicted&quot;</span>, y<span class="op">=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb43-7"><a href="diag.html#cb43-7" tabindex="-1"></a>         )</span>
<span id="cb43-8"><a href="diag.html#cb43-8" tabindex="-1"></a></span>
<span id="cb43-9"><a href="diag.html#cb43-9" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-18-5.png" width="614" /></p>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="diag.html#cb44-1" tabindex="-1"></a>weights1 <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> x<span class="op">**</span><span class="dv">2</span></span>
<span id="cb44-2"><a href="diag.html#cb44-2" tabindex="-1"></a>x <span class="op">=</span> sm.add_constant(x)</span>
<span id="cb44-3"><a href="diag.html#cb44-3" tabindex="-1"></a>model2 <span class="op">=</span> sm.WLS(y, x, weights<span class="op">=</span>weights1)</span>
<span id="cb44-4"><a href="diag.html#cb44-4" tabindex="-1"></a>results2 <span class="op">=</span> model2.fit()</span>
<span id="cb44-5"><a href="diag.html#cb44-5" tabindex="-1"></a></span>
<span id="cb44-6"><a href="diag.html#cb44-6" tabindex="-1"></a><span class="co"># Print regression results</span></span>
<span id="cb44-7"><a href="diag.html#cb44-7" tabindex="-1"></a><span class="bu">print</span>(results2.summary())</span></code></pre></div>
<pre><code>##                             WLS Regression Results                            
## ==============================================================================
## Dep. Variable:                      y   R-squared:                       0.316
## Model:                            WLS   Adj. R-squared:                  0.301
## Method:                 Least Squares   F-statistic:                     19.90
## Date:                Tue, 15 Jul 2025   Prob (F-statistic):           5.78e-05
## Time:                        11:34:08   Log-Likelihood:                -135.29
## No. Observations:                  45   AIC:                             274.6
## Df Residuals:                      43   BIC:                             278.2
## Df Model:                           1                                         
## Covariance Type:            nonrobust                                         
## ==============================================================================
##                  coef    std err          t      P&gt;|t|      [0.025      0.975]
## ------------------------------------------------------------------------------
## const         49.6438      1.132     43.871      0.000      47.362      51.926
## x1             1.2478      0.280      4.461      0.000       0.684       1.812
## ==============================================================================
## Omnibus:                        0.027   Durbin-Watson:                   2.223
## Prob(Omnibus):                  0.987   Jarque-Bera (JB):                0.209
## Skew:                           0.011   Prob(JB):                        0.901
## Kurtosis:                       2.667   Cond. No.                         8.50
## ==============================================================================
## 
## Notes:
## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="diag.html#cb46-1" tabindex="-1"></a>model3 <span class="op">=</span> sm.OLS(y,x)</span>
<span id="cb46-2"><a href="diag.html#cb46-2" tabindex="-1"></a>results <span class="op">=</span> model3.fit(cov_type<span class="op">=</span><span class="st">&#39;HC1&#39;</span>)</span>
<span id="cb46-3"><a href="diag.html#cb46-3" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span></code></pre></div>
<pre><code>##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                      y   R-squared:                       0.210
## Model:                            OLS   Adj. R-squared:                  0.192
## Method:                 Least Squares   F-statistic:                     10.96
## Date:                Tue, 15 Jul 2025   Prob (F-statistic):            0.00189
## Time:                        11:34:08   Log-Likelihood:                -144.58
## No. Observations:                  45   AIC:                             293.2
## Df Residuals:                      43   BIC:                             296.8
## Df Model:                           1                                         
## Covariance Type:                  HC1                                         
## ==============================================================================
##                  coef    std err          z      P&gt;|z|      [0.025      0.975]
## ------------------------------------------------------------------------------
## const         49.9070      1.662     30.021      0.000      46.649      53.165
## x1             1.2007      0.363      3.311      0.001       0.490       1.912
## ==============================================================================
## Omnibus:                        3.109   Durbin-Watson:                   2.014
## Prob(Omnibus):                  0.211   Jarque-Bera (JB):                2.458
## Skew:                          -0.112   Prob(JB):                        0.293
## Kurtosis:                       4.123   Cond. No.                         16.9
## ==============================================================================
## 
## Notes:
## [1] Standard Errors are heteroscedasticity robust (HC1)</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="diag.html#cb48-1" tabindex="-1"></a>model4 <span class="op">=</span> sm.OLS(y,x)</span>
<span id="cb48-2"><a href="diag.html#cb48-2" tabindex="-1"></a>results <span class="op">=</span> model4.fit(cov_type<span class="op">=</span><span class="st">&#39;HC2&#39;</span>)</span>
<span id="cb48-3"><a href="diag.html#cb48-3" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span></code></pre></div>
<pre><code>##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                      y   R-squared:                       0.210
## Model:                            OLS   Adj. R-squared:                  0.192
## Method:                 Least Squares   F-statistic:                     10.74
## Date:                Tue, 15 Jul 2025   Prob (F-statistic):            0.00208
## Time:                        11:34:08   Log-Likelihood:                -144.58
## No. Observations:                  45   AIC:                             293.2
## Df Residuals:                      43   BIC:                             296.8
## Df Model:                           1                                         
## Covariance Type:                  HC2                                         
## ==============================================================================
##                  coef    std err          z      P&gt;|z|      [0.025      0.975]
## ------------------------------------------------------------------------------
## const         49.9070      1.679     29.726      0.000      46.616      53.198
## x1             1.2007      0.366      3.277      0.001       0.483       1.919
## ==============================================================================
## Omnibus:                        3.109   Durbin-Watson:                   2.014
## Prob(Omnibus):                  0.211   Jarque-Bera (JB):                2.458
## Skew:                          -0.112   Prob(JB):                        0.293
## Kurtosis:                       4.123   Cond. No.                         16.9
## ==============================================================================
## 
## Notes:
## [1] Standard Errors are heteroscedasticity robust (HC2)</code></pre>
</div>
</div>
<div id="normality" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Normality<a href="diag.html#normality" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another assumption is that the residuals are normally distributed. We can test this assumption through use of visual aids or formal hypothesis tests. visual aids include a histogram of the residuals or Q-Q plot of residuals. A few tests of normality of residuals, include (but not limited to) Shapiro-Wilk, Anderson-Darling, Kolmogorov-Smirnov tests. In addition to the base R package, you should also install and library the nortest package.</p>
<p><strong>Visualization</strong></p>
<p>To visually inspect whether or not the residuals are normally distributed, we can either graph the historgram of residuals or create a Q-Q plot of the residuals. In the histogram, we are looking for a bell-shaped curve and in the Q-Q plot, we are looking for a straight line.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:normalvis"></span>
<img src="bookdownproj_files/figure-html/normalvis-7.png" alt="Histogram and Q-Q Plot of Residuals" width="672" />
<p class="caption">
Figure 5.12: Histogram and Q-Q Plot of Residuals
</p>
</div>
<p><strong>Formal Hypothesis tests</strong></p>
<p>You can also run a formal hypothesis test. The hypotheses are<br />
<span class="math inline">\(H_{0}:\)</span> Residuals are normally distributed<br />
<span class="math inline">\(H_{A}:\)</span> Residuals are not normally distributed.</p>
<p>There are MANY different tests for normality. Only two will be covered here.</p>
<ol style="list-style-type: decimal">
<li>Anderson-Darling is based on the empirical cumulative distribution function of the data and gives more weight to the tails.<br />
</li>
<li>Shapiro-Wilk test uses the idea of correlation between the sample data and normal scores. The Shapiro-Wilk is better for smaller data sets.</li>
</ol>
<p>With large samples, you will most likely end up rejecting these hypothesis tests (using corrected significance levels is recommended). Keep in mind that you can also get different result from different tests. It is up to you to make the decision on whether or not you are comfortable in the assumption for normality holding in different situations.</p>
<div id="r-code-26" class="section level4 hasAnchor" number="5.4.0.1">
<h4><span class="header-section-number">5.4.0.1</span> R Code:<a href="diag.html#r-code-26" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="diag.html#cb50-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">55402</span>)</span>
<span id="cb50-2"><a href="diag.html#cb50-2" tabindex="-1"></a>x<span class="ot">=</span><span class="fu">rnorm</span>(<span class="dv">1000</span>,<span class="dv">0</span>,<span class="dv">4</span>)</span>
<span id="cb50-3"><a href="diag.html#cb50-3" tabindex="-1"></a><span class="fu">ad.test</span>(x)</span></code></pre></div>
<pre><code>## 
##  Anderson-Darling normality test
## 
## data:  x
## A = 0.30275, p-value = 0.5742</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="diag.html#cb52-1" tabindex="-1"></a><span class="fu">shapiro.test</span>(x)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  x
## W = 0.99889, p-value = 0.814</code></pre>
<p>Using a simulated data set (so the “TRUE” distribution is actually normal indicating that we should “Fail to reject” the null hypothesis). The p-values from both of these test illustrate that they would indicate the distribution is not significantly different from normal (which it should).</p>
<p><strong>Box-cox Transformation</strong></p>
<p>If the residuals are not normally distributed, one solution is to transform them to normality. However, the exact transformation might be difficult to identify. George Box and Sir David Cox developed an algorithm back in the 1960’s to assist in identifying “power” transformations to make a variable normally distributed (think…“what power should I raise this variable to?”). Their algorithm tries out different values of <span class="math inline">\(\lambda\)</span> or powers of the response variable in the following way:<br />
</p>
<p><span class="math display">\[y = \begin{array}
{rr}
\frac{y^{\lambda}-1}{\lambda} &amp; if \lambda \ne 0  \\
log(y) &amp; if \lambda = 0
\end{array}\]</span></p>
<p><strong>Example</strong></p>
<p>Using the residuls from the originally salary data, we can take a look at the box-cox transformation.</p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="diag.html#cb54-1" tabindex="-1"></a>lm.var<span class="ot">=</span><span class="fu">lm</span>(salary<span class="sc">~</span>years)</span>
<span id="cb54-2"><a href="diag.html#cb54-2" tabindex="-1"></a></span>
<span id="cb54-3"><a href="diag.html#cb54-3" tabindex="-1"></a><span class="fu">boxcox</span>(lm.var)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/boxcox-1.png" width="672" />
The output from the graph clearly incates that <span class="math inline">\(\lambda\)</span> should be 0, which indicates a log transform.</p>
<p>In conclusion, to deal with data that is NOT normally distributed, we can either</p>
<ol style="list-style-type: decimal">
<li>Use a robust regression (quantile regression or nonparametric)</li>
<li>Transform either response or predictor variables or both to obtain normality in the residuals.</li>
</ol>
</div>
<div id="python-code-27" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Python Code<a href="diag.html#python-code-27" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="diag.html#cb55-1" tabindex="-1"></a>p<span class="op">=</span>(ggplot(experience,aes(x<span class="op">=</span><span class="st">&quot;resid_mlr&quot;</span>)) <span class="op">+</span> geom_histogram(fill<span class="op">=</span><span class="st">&#39;blue&#39;</span>, color<span class="op">=</span><span class="st">&#39;black&#39;</span>) <span class="op">+</span></span>
<span id="cb55-2"><a href="diag.html#cb55-2" tabindex="-1"></a>    labs(title<span class="op">=</span><span class="st">&#39;Histogram&#39;</span>, x<span class="op">=</span><span class="st">&#39;Value&#39;</span>, y<span class="op">=</span><span class="st">&#39;Count&#39;</span>) <span class="op">+</span></span>
<span id="cb55-3"><a href="diag.html#cb55-3" tabindex="-1"></a>    theme_minimal()</span>
<span id="cb55-4"><a href="diag.html#cb55-4" tabindex="-1"></a>)</span>
<span id="cb55-5"><a href="diag.html#cb55-5" tabindex="-1"></a></span>
<span id="cb55-6"><a href="diag.html#cb55-6" tabindex="-1"></a>p.show()</span></code></pre></div>
<pre><code>## C:\PROGRA~3\ANACON~1\Lib\site-packages\plotnine\stats\stat_bin.py:109: PlotnineWarning: &#39;stat_bin()&#39; using &#39;bins = 11&#39;. Pick better value with &#39;binwidth&#39;.</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-19-1.png" width="614" /></p>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="diag.html#cb57-1" tabindex="-1"></a>p <span class="op">=</span> (</span>
<span id="cb57-2"><a href="diag.html#cb57-2" tabindex="-1"></a>    ggplot(experience, aes(sample<span class="op">=</span><span class="st">&#39;resid_mlr&#39;</span>)) <span class="op">+</span></span>
<span id="cb57-3"><a href="diag.html#cb57-3" tabindex="-1"></a>    stat_qq() <span class="op">+</span></span>
<span id="cb57-4"><a href="diag.html#cb57-4" tabindex="-1"></a>    stat_qq_line()</span>
<span id="cb57-5"><a href="diag.html#cb57-5" tabindex="-1"></a>)</span>
<span id="cb57-6"><a href="diag.html#cb57-6" tabindex="-1"></a></span>
<span id="cb57-7"><a href="diag.html#cb57-7" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-19-2.png" width="614" /></p>
<div class="sourceCode" id="cb58"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb58-1"><a href="diag.html#cb58-1" tabindex="-1"></a>sp.stats.shapiro(model_var.resid)</span></code></pre></div>
<pre><code>## ShapiroResult(statistic=0.9825804700820449, pvalue=0.21006919799469664)</code></pre>
<p>We can also perform the Anderson-Darling test in Python:</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="diag.html#cb60-1" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.diagnostic <span class="im">import</span> normal_ad</span>
<span id="cb60-2"><a href="diag.html#cb60-2" tabindex="-1"></a></span>
<span id="cb60-3"><a href="diag.html#cb60-3" tabindex="-1"></a>stat, pval <span class="op">=</span> normal_ad(model_var.resid)</span>
<span id="cb60-4"><a href="diag.html#cb60-4" tabindex="-1"></a></span>
<span id="cb60-5"><a href="diag.html#cb60-5" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Anderson-Darling statistic: </span><span class="sc">{</span>stat<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## Anderson-Darling statistic: 0.4271</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="diag.html#cb62-1" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;P-value: </span><span class="sc">{</span>pval<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## P-value: 0.3074</code></pre>
<p>Box-cox:</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="diag.html#cb64-1" tabindex="-1"></a>bc <span class="op">=</span> sp.stats.boxcox(experience[<span class="st">&#39;salary&#39;</span>], alpha <span class="op">=</span> <span class="fl">0.05</span>)</span>
<span id="cb64-2"><a href="diag.html#cb64-2" tabindex="-1"></a></span>
<span id="cb64-3"><a href="diag.html#cb64-3" tabindex="-1"></a>bc[<span class="dv">2</span>] <span class="co"># CI for lambda value</span></span></code></pre></div>
<pre><code>## (-0.291973062367277, 0.41815199140209897)</code></pre>
</div>
</div>
<div id="correlated-errors" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Correlated Errors<a href="diag.html#correlated-errors" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Another important assumption is that the observations are independent. There are a couple of ways in which we assess this assumtpion:</p>
<ol style="list-style-type: decimal">
<li>Experimental design or collection of data</li>
<li>Explore residual plots</li>
</ol>
<p>Depending on how the data was collected or the experimental design, there could be potential correlation in the observations. For example, if the data collected information on multiple family members (include husband and wife and kids). Or if multiple observations were collected on the same subject. There are various models that exists that can account for this dependence (longitudinal analysis, hierarchical models, etc). However, this type of analysis will be covered later throughout the year. There is no diagnostic measures to indicate this correlation structure. Your best way of knowing it is there is to know the experimental design or collection of data. 
We will focus our attention on a form of dependence that is evident from the residual plots, which is autocorrelation. This happens when ordered observations are dependent on prior observations. One of the most common instances of this correlation structure is through data that is collected over time, also referred to as time series. You will learn more about time series data in the Fall semester, but for now, let’s explore how to recognize if it is an issue in the analysis. To diagnose autocorrelation in data, you can</p>
<ol style="list-style-type: decimal">
<li>Visually inspect the plot of residuals by time (see a pattern over time…usually cyclical)</li>
<li>Conduct a Durbin-Watson test</li>
</ol>
<p>In visually inspecting a plot of residuals across time, you would expect to see a type of cyclical pattern if autocorrelation exists as it does in Figure <a href="diag.html#fig:autoplot">5.13</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:autoplot"></span>
<img src="bookdownproj_files/figure-html/autoplot-5.png" alt="Residuals vs Time: Autocorrelation Present" width="672" />
<p class="caption">
Figure 5.13: Residuals vs Time: Autocorrelation Present
</p>
</div>
<p>In using the Durbin-Watson test, the hypotheses are:<br />
<span class="math inline">\(H_{0}:\)</span> No residual correlation<br />
<span class="math inline">\(H_{A}:\)</span> Residual Correlation<br />
</p>
<p>The Durbin-Watson test statistic is calculated by</p>
<p><span class="math display">\[ d=\frac{\sum_{t=2}^T(e_{t}-e_{t-1})^2}{\sum_{t=1}^T e_{t}^2}\]</span>
where <span class="math inline">\(e\)</span> represents the error terms, or residuals. The statistic <em>d</em> ranges from 0 to 4, with a value of 2 indicating no residual correlation. Values of <em>d</em> smaller than 2 may indicate a positive autocorrelation and a value of <em>d</em> larger than 2 may indicate a negative autocorrelation. However, the question becomes when is it “significantly different” than 2? Unless there is good reason to assume a negative autocorrelation, testing for positive autocorrelation would be the preferred test. The Durbin-Watson test is in the lmtest package of R. If you want to test for positive autocorrelation, you will need to specify the alternative of “greater” (even though the test statistic is testing for a value LESS than 2!!).<br />
</p>
<p><strong>Example</strong>
The following data set illustrates this test using the Google data set in the TSA package (returns of Google stock from 08/20/04 - 09/13/06).</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="diag.html#cb66-1" tabindex="-1"></a><span class="fu">data</span>(google)</span>
<span id="cb66-2"><a href="diag.html#cb66-2" tabindex="-1"></a>x<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(google))</span>
<span id="cb66-3"><a href="diag.html#cb66-3" tabindex="-1"></a>lm.model<span class="ot">=</span><span class="fu">lm</span>(google<span class="sc">~</span>x)</span>
<span id="cb66-4"><a href="diag.html#cb66-4" tabindex="-1"></a><span class="fu">dwtest</span>(lm.model,<span class="at">alternative=</span><span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  lm.model
## DW = 1.842, p-value = 0.0321
## alternative hypothesis: true autocorrelation is greater than 0</code></pre>
<p>In running this test, you either need to have the data sorted by the correct date (it assumes the observations are correctly ordered by time). If they are not, there is an order.by option in which you can use. For this example, the p-value is 0.0321, which if we use an <span class="math inline">\(\alpha\)</span> of 0.05, we would reject the null hypothesis and conclude there appears to be significant positive autocorrelation.</p>
<p>If your data has significant autocorrelation, then the error terms will not be correct. We will discuss in the Fall semester how to model time series data appropriately.</p>
<div id="python-code-28" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> Python Code<a href="diag.html#python-code-28" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="diag.html#cb68-1" tabindex="-1"></a>google <span class="op">=</span> r.google</span>
<span id="cb68-2"><a href="diag.html#cb68-2" tabindex="-1"></a></span>
<span id="cb68-3"><a href="diag.html#cb68-3" tabindex="-1"></a>returns <span class="op">=</span> pd.DataFrame({<span class="st">&#39;returns&#39;</span>: google, <span class="st">&#39;time&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(google)<span class="op">+</span><span class="dv">1</span>)})</span></code></pre></div>
<div class="sourceCode" id="cb69"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb69-1"><a href="diag.html#cb69-1" tabindex="-1"></a>model_dw <span class="op">=</span> smf.ols(<span class="st">&quot;returns ~ time&quot;</span>, data <span class="op">=</span> returns).fit()</span>
<span id="cb69-2"><a href="diag.html#cb69-2" tabindex="-1"></a>model_dw.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>returns</td>     <th>  R-squared:         </th> <td>   0.007</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.005</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.733</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th>  <td>0.0539</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:10</td>     <th>  Log-Likelihood:    </th> <td>  1209.3</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   521</td>      <th>  AIC:               </th> <td>  -2415.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   519</td>      <th>  BIC:               </th> <td>  -2406.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>    0.0062</td> <td>    0.002</td> <td>    2.960</td> <td> 0.003</td> <td>    0.002</td> <td>    0.010</td>
</tr>
<tr>
  <th>time</th>      <td> -1.34e-05</td> <td> 6.93e-06</td> <td>   -1.932</td> <td> 0.054</td> <td> -2.7e-05</td> <td> 2.24e-07</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>76.878</td> <th>  Durbin-Watson:     </th> <td>   1.842</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 398.126</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.510</td> <th>  Prob(JB):          </th> <td>3.53e-87</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 7.159</td> <th>  Cond. No.          </th> <td>    603.</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<p>Durbin-Watson test statistic is reported by default in summary of linear regression, but there is no way of getting a p-value for this metric in Python. However, you can use a more general test called the Breusch-Godfrey test (too general…not very powerful).</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="diag.html#cb70-1" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.diagnostic <span class="im">import</span> acorr_breusch_godfrey</span>
<span id="cb70-2"><a href="diag.html#cb70-2" tabindex="-1"></a></span>
<span id="cb70-3"><a href="diag.html#cb70-3" tabindex="-1"></a><span class="co"># Get test result</span></span>
<span id="cb70-4"><a href="diag.html#cb70-4" tabindex="-1"></a>bg_test <span class="op">=</span> acorr_breusch_godfrey(model_dw, nlags<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb70-5"><a href="diag.html#cb70-5" tabindex="-1"></a></span>
<span id="cb70-6"><a href="diag.html#cb70-6" tabindex="-1"></a><span class="co"># Output: test statistic, p-value, f-statistic, f p-value</span></span>
<span id="cb70-7"><a href="diag.html#cb70-7" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Breusch-Godfrey p-value: </span><span class="sc">{</span>bg_test[<span class="dv">1</span>]<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<pre><code>## Breusch-Godfrey p-value: 0.1186</code></pre>
</div>
</div>
<div id="influential-observations-and-outliers" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Influential Observations and Outliers<a href="diag.html#influential-observations-and-outliers" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Influential points and outliers play a large role in the estimation of the model and its prediction. Influential points are usually those points on the edges of the x-values and can greatly impact the slopes in the regression equation. Outliers tend to be those values that do not follow the trend of the data and are generally found by large deviations in the y direction. For multiple linear regression, outliers are found by using residuals. These can be standardize residuals or studentized residuals. Influential points can be discovered by Cook’s D, dffits, dfbetas or Hat values. These points are important to identify and recognize their influence on the regression, however, it does NOT mean that these points should be removed. Removal of data should be taken very seriously. We would obviously want to omit any observations made in error, due to typos or reporting inaccuracies, but we do not remove data points just because they are outside the scope of our expectations. We have many tools at our disposal for modeling with such observations, and we should always develop a deep understanding of the application-specific risks and rewards associated with data removal.<br />
</p>
<p>In the this section we will cover the following statistics to help us detect outliers and influential observations respectively.</p>
<p><em>Outliers</em></p>
<ul>
<li>rstandard</li>
<li>rstudent</li>
</ul>
<p><em>Influential observations</em></p>
<ul>
<li>dffits</li>
<li>dfbetas</li>
<li>cooks.distance</li>
<li>hatvalues</li>
</ul>
<p><em>Outliers</em></p>
<p>In using residuals to detect outliers, we first need to “standardize” them, or divide by their standard errors. In this sense, we can think of these “standardized” residuals as an approximate z-score. Therefore, we look for residuals greater in magnitude than 3 as potential outliers. R calculates two different types of standardized residuals: 1. standardized residuals (rstandard) and 2. studentized residuals (rstudent). Standardized residuals are definied as
<span class="math display">\[ r_{i}=\frac{e_{i}}{\hat\sigma\sqrt{1-H_{ii}}},\]</span>
where <span class="math inline">\(\hat\sigma\)</span> is the square root of the MSE and <span class="math inline">\(H_{ii}\)</span> is from the diagonal of the hat matrix (hat matrix will be discussed in influential observations). Studentized residuals are definied as
<span class="math display">\[ t_{i}=\frac{e_{i}}{\hat\sigma_{(i)}\sqrt{1-H_{ii}}},\]</span>
where <span class="math inline">\(\hat\sigma_{(i)}\)</span> is the square root of the MSE calculated when observation <em>i</em> is deleted and <span class="math inline">\(H_{ii}\)</span> is from the diagonal of the hat matrix. The studentized residuals follow a t-distribution with <span class="math inline">\(n-k-2\)</span>, where <span class="math inline">\(k\)</span> is the number of explanatory variables(notice that this has one less degree of freedom than the usual error in regression which is due to the one deleted observation).</p>
<p><em>Influential observations</em></p>
<p>Influential observations are observations that can dramatically change the model’s estimates. It is important to identify and understand these where these observations are. There are a number of different measures to aid in identifying influential observations, which will be discussed below.</p>
<p>Cook’s distance, also referred to as Cook’s D, measures the difference in the regression estimates when the <span class="math inline">\(i^{th}\)</span> observation is left out. A rough rule of thumb is to use the quantile of the <span class="math inline">\(F\)</span> distribution with numerator degrees of freedom <span class="math inline">\(p\)</span> and denominator degrees of freedom <span class="math inline">\(n-p\)</span> where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of parameters in the regression equation.</p>
<p>Dffits calculates the difference of fitted values for each point in the regression versus the fit of the regression line for that point if it was removed. Large values of dffits indicate that the point is influential in the calculation of the estimated regression line. As a general rule of thumb, a cutoff of <span class="math inline">\(2\sqrt{p/n}\)</span> is used to identify potential influential points.<br />
Dfbetas follows the same idea as dffits. The difference in the estimated betas is calculated for each observation (observation included in the estimated beta and observation NOT included in estimating the beta). This is done for each individual observation and each estimated beta. For small data sets, a value greater than 1 is suspect of an influential observation. For large data sets, the cutoff is <span class="math inline">\(\frac{2}{\sqrt{n}}\)</span>.<br />
The hat values <span class="math inline">\(H_{ii}\)</span> are the diagonal values of <span class="math display">\[\boldsymbol{X(X^{T}X)^{-1.}X^{T}}.\]</span> Hat values can identify high leverage points in a regression. A general rule of thumb are hat values greater than <span class="math inline">\(\frac{2p}{n}\)</span>.</p>
<p><strong>Example</strong></p>
<p>We will use the Scottish hill races as an example to illustrate how to calculate and visualize these values. The Scottish hill races include the following variables:</p>
<ul>
<li>Time: Record time to complete course</li>
<li>Distance: Distance in the course</li>
<li>Climb: Vertical climb in the course</li>
</ul>
<div id="r-code-27" class="section level4 hasAnchor" number="5.6.0.1">
<h4><span class="header-section-number">5.6.0.1</span> R code:<a href="diag.html#r-code-27" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="diag.html#cb72-1" tabindex="-1"></a>url <span class="ot">=</span> <span class="st">&#39;http://www.statsci.org/data/general/hills.txt&#39;</span> </span>
<span id="cb72-2"><a href="diag.html#cb72-2" tabindex="-1"></a>races_table <span class="ot">=</span> <span class="fu">read.table</span>(url, <span class="at">header=</span><span class="cn">TRUE</span>, <span class="at">sep=</span><span class="st">&#39;</span><span class="sc">\t</span><span class="st">&#39;</span>)</span>
<span id="cb72-3"><a href="diag.html#cb72-3" tabindex="-1"></a>n.index<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">nrow</span>(races_table))</span>
<span id="cb72-4"><a href="diag.html#cb72-4" tabindex="-1"></a>races.table<span class="ot">=</span><span class="fu">cbind</span>(races_table,n.index)</span>
<span id="cb72-5"><a href="diag.html#cb72-5" tabindex="-1"></a>lm.model<span class="ot">=</span><span class="fu">lm</span>(Time<span class="sc">~</span>Distance<span class="sc">+</span>Climb,<span class="at">data=</span>races_table)</span>
<span id="cb72-6"><a href="diag.html#cb72-6" tabindex="-1"></a></span>
<span id="cb72-7"><a href="diag.html#cb72-7" tabindex="-1"></a></span>
<span id="cb72-8"><a href="diag.html#cb72-8" tabindex="-1"></a><span class="do">##Plots of outliers</span></span>
<span id="cb72-9"><a href="diag.html#cb72-9" tabindex="-1"></a>a <span class="ot">=</span> <span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">rstandard</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="sc">-</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Internal Studentized Residuals&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb72-10"><a href="diag.html#cb72-10" tabindex="-1"></a></span>
<span id="cb72-11"><a href="diag.html#cb72-11" tabindex="-1"></a>b <span class="ot">=</span> <span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">rstudent</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="sc">-</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;External Studentized Residuals&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb72-12"><a href="diag.html#cb72-12" tabindex="-1"></a></span>
<span id="cb72-13"><a href="diag.html#cb72-13" tabindex="-1"></a><span class="do">##Influential points</span></span>
<span id="cb72-14"><a href="diag.html#cb72-14" tabindex="-1"></a>c <span class="ot">=</span> <span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">rstudent</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="sc">-</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="dv">3</span>)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;External Studentized Residuals&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb72-15"><a href="diag.html#cb72-15" tabindex="-1"></a></span>
<span id="cb72-16"><a href="diag.html#cb72-16" tabindex="-1"></a><span class="do">##Cook&#39;s D</span></span>
<span id="cb72-17"><a href="diag.html#cb72-17" tabindex="-1"></a>D.cut<span class="ot">=</span><span class="dv">4</span><span class="sc">/</span>(<span class="fu">nrow</span>(races_table)<span class="sc">-</span><span class="dv">3</span>)</span>
<span id="cb72-18"><a href="diag.html#cb72-18" tabindex="-1"></a></span>
<span id="cb72-19"><a href="diag.html#cb72-19" tabindex="-1"></a>d <span class="ot">=</span><span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">cooks.distance</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span>D.cut)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Cook&#39;s D&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;Cook&#39;s Distance&quot;</span>)</span>
<span id="cb72-20"><a href="diag.html#cb72-20" tabindex="-1"></a></span>
<span id="cb72-21"><a href="diag.html#cb72-21" tabindex="-1"></a><span class="do">##Dffit</span></span>
<span id="cb72-22"><a href="diag.html#cb72-22" tabindex="-1"></a>df.cut<span class="ot">=</span><span class="dv">2</span><span class="sc">*</span>(<span class="fu">sqrt</span>(<span class="dv">3</span><span class="sc">/</span><span class="fu">nrow</span>(races_table)))</span>
<span id="cb72-23"><a href="diag.html#cb72-23" tabindex="-1"></a></span>
<span id="cb72-24"><a href="diag.html#cb72-24" tabindex="-1"></a>e <span class="ot">=</span><span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">dffits</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span>df.cut)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="sc">-</span>df.cut)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;DFFITS&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;DFFITS&quot;</span>)</span>
<span id="cb72-25"><a href="diag.html#cb72-25" tabindex="-1"></a></span>
<span id="cb72-26"><a href="diag.html#cb72-26" tabindex="-1"></a>db.cut<span class="ot">=</span><span class="dv">2</span><span class="sc">/</span><span class="fu">sqrt</span>(<span class="fu">nrow</span>(races_table))</span>
<span id="cb72-27"><a href="diag.html#cb72-27" tabindex="-1"></a></span>
<span id="cb72-28"><a href="diag.html#cb72-28" tabindex="-1"></a></span>
<span id="cb72-29"><a href="diag.html#cb72-29" tabindex="-1"></a>f <span class="ot">=</span><span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">dfbetas</span>(lm.model)[,<span class="st">&#39;Climb&#39;</span>]))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span>db.cut)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="sc">-</span>db.cut)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;DFBETA for Climb&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;DFBETAS&quot;</span>)</span>
<span id="cb72-30"><a href="diag.html#cb72-30" tabindex="-1"></a></span>
<span id="cb72-31"><a href="diag.html#cb72-31" tabindex="-1"></a>g <span class="ot">=</span><span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">dfbetas</span>(lm.model)[,<span class="st">&#39;Distance&#39;</span>]))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span>db.cut)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span><span class="sc">-</span>db.cut)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;DFBETA for Distance&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;DFBETAS&quot;</span>)</span>
<span id="cb72-32"><a href="diag.html#cb72-32" tabindex="-1"></a></span>
<span id="cb72-33"><a href="diag.html#cb72-33" tabindex="-1"></a><span class="do">##Hat</span></span>
<span id="cb72-34"><a href="diag.html#cb72-34" tabindex="-1"></a>hat.cut<span class="ot">=</span><span class="dv">2</span><span class="sc">*</span>(<span class="dv">3</span>)<span class="sc">/</span><span class="fu">nrow</span>(races_table)</span>
<span id="cb72-35"><a href="diag.html#cb72-35" tabindex="-1"></a></span>
<span id="cb72-36"><a href="diag.html#cb72-36" tabindex="-1"></a>h <span class="ot">=</span> <span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span>n.index,<span class="at">y=</span><span class="fu">hatvalues</span>(lm.model)))<span class="sc">+</span><span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span><span class="fu">geom_line</span>(<span class="at">y=</span>hat.cut)<span class="sc">+</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Hat values&quot;</span>,<span class="at">x=</span><span class="st">&quot;Observation&quot;</span>,<span class="at">y=</span><span class="st">&quot;Hat Values&quot;</span>)</span>
<span id="cb72-37"><a href="diag.html#cb72-37" tabindex="-1"></a></span>
<span id="cb72-38"><a href="diag.html#cb72-38" tabindex="-1"></a><span class="fu">grid.arrange</span>(a,b,c,d,e,f,g,<span class="at">ncol=</span><span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:influence"></span>
<img src="bookdownproj_files/figure-html/influence-1.png" alt="Plots for Exploring Outliers and Influential Points" width="672" />
<p class="caption">
Figure 5.14: Plots for Exploring Outliers and Influential Points
</p>
</div>
<p>Figure <a href="diag.html#fig:influence">5.14</a> shows a number of useful graphics that help us explore outliers and influential points.</p>
<p>A good graph to explore is looking at the studentized residuals versus the hat values. An observation that is high leverage AND an outlier is one that needs to be explored.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="diag.html#cb73-1" tabindex="-1"></a><span class="fu">ggplot</span>(lm.model,<span class="fu">aes</span>(<span class="at">x=</span><span class="fu">hatvalues</span>(lm.model),<span class="at">y=</span><span class="fu">rstudent</span>(lm.model))) <span class="sc">+</span>  <span class="fu">geom_point</span>(<span class="at">color=</span><span class="st">&quot;orange&quot;</span>)<span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Hat values&quot;</span>,<span class="at">y=</span><span class="st">&quot;Residuals&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:influenceresid"></span>
<img src="bookdownproj_files/figure-html/influenceresid-1.png" alt="Influential and Outlier Observations" width="672" />
<p class="caption">
Figure 5.15: Influential and Outlier Observations
</p>
</div>
</div>
<div id="python-code-29" class="section level3 hasAnchor" number="5.6.1">
<h3><span class="header-section-number">5.6.1</span> Python Code<a href="diag.html#python-code-29" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit model:</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="diag.html#cb74-1" tabindex="-1"></a>races_table <span class="op">=</span> pd.read_table(<span class="st">&quot;http://www.statsci.org/data/general/hills.txt&quot;</span>)</span>
<span id="cb74-2"><a href="diag.html#cb74-2" tabindex="-1"></a></span>
<span id="cb74-3"><a href="diag.html#cb74-3" tabindex="-1"></a><span class="co"># Fit linear model</span></span>
<span id="cb74-4"><a href="diag.html#cb74-4" tabindex="-1"></a>X <span class="op">=</span> races_table[[<span class="st">&#39;Distance&#39;</span>, <span class="st">&#39;Climb&#39;</span>]]</span>
<span id="cb74-5"><a href="diag.html#cb74-5" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)  <span class="co"># adds intercept</span></span>
<span id="cb74-6"><a href="diag.html#cb74-6" tabindex="-1"></a>y <span class="op">=</span> races_table[<span class="st">&#39;Time&#39;</span>]</span>
<span id="cb74-7"><a href="diag.html#cb74-7" tabindex="-1"></a>lm_model <span class="op">=</span> sma.OLS(y, X).fit()</span>
<span id="cb74-8"><a href="diag.html#cb74-8" tabindex="-1"></a></span>
<span id="cb74-9"><a href="diag.html#cb74-9" tabindex="-1"></a></span>
<span id="cb74-10"><a href="diag.html#cb74-10" tabindex="-1"></a>model_io <span class="op">=</span> smf.ols(<span class="st">&quot;Time ~ Distance + Climb&quot;</span>, data <span class="op">=</span> races_table).fit()</span>
<span id="cb74-11"><a href="diag.html#cb74-11" tabindex="-1"></a>model_io.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>Time</td>       <th>  R-squared:         </th> <td>   0.919</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.914</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   181.7</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 15 Jul 2025</td> <th>  Prob (F-statistic):</th> <td>3.40e-18</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:34:13</td>     <th>  Log-Likelihood:    </th> <td> -142.11</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    35</td>      <th>  AIC:               </th> <td>   290.2</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    32</td>      <th>  BIC:               </th> <td>   294.9</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th> <td>   -8.9920</td> <td>    4.303</td> <td>   -2.090</td> <td> 0.045</td> <td>  -17.756</td> <td>   -0.228</td>
</tr>
<tr>
  <th>Distance</th>  <td>    6.2180</td> <td>    0.601</td> <td>   10.343</td> <td> 0.000</td> <td>    4.993</td> <td>    7.442</td>
</tr>
<tr>
  <th>Climb</th>     <td>    0.0110</td> <td>    0.002</td> <td>    5.387</td> <td> 0.000</td> <td>    0.007</td> <td>    0.015</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>47.910</td> <th>  Durbin-Watson:     </th> <td>   2.249</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 233.976</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 3.026</td> <th>  Prob(JB):          </th> <td>1.56e-51</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td>14.127</td> <th>  Cond. No.          </th> <td>4.20e+03</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.2e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
<p>If you need to get actual values of residuals/influence:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb75-1"><a href="diag.html#cb75-1" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.outliers_influence <span class="im">import</span> OLSInfluence</span>
<span id="cb75-2"><a href="diag.html#cb75-2" tabindex="-1"></a></span>
<span id="cb75-3"><a href="diag.html#cb75-3" tabindex="-1"></a>influence <span class="op">=</span> OLSInfluence(model_io)</span>
<span id="cb75-4"><a href="diag.html#cb75-4" tabindex="-1"></a></span>
<span id="cb75-5"><a href="diag.html#cb75-5" tabindex="-1"></a>influence.resid_studentized_internal</span></code></pre></div>
<pre><code>## 0     0.164547
## 1    -0.530157
## 2    -0.320258
## 3    -0.061540
## 4    -0.869429
## 5     0.055980
## 6     2.798195
## 7    -0.054764
## 8    -0.082497
## 9     0.296673
## 10    0.532907
## 11   -0.079673
## 12   -0.650019
## 13    0.720096
## 14   -0.599632
## 15   -0.981525
## 16   -0.280934
## 17    4.565581
## 18   -0.876961
## 19    0.051033
## 20    0.208624
## 21   -0.100766
## 22   -0.346860
## 23    0.319234
## 24    0.167320
## 25   -0.824225
## 26   -0.189338
## 27   -0.279657
## 28   -0.017885
## 29   -0.465994
## 30   -1.178913
## 31   -0.178834
## 32    0.738902
## 33   -0.019440
## 34   -0.816190
## dtype: float64</code></pre>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="diag.html#cb77-1" tabindex="-1"></a>influence.resid_studentized_external</span></code></pre></div>
<pre><code>## 0     0.162024
## 1    -0.524115
## 2    -0.315720
## 3    -0.060574
## 4    -0.866026
## 5     0.055101
## 6     3.168980
## 7    -0.053904
## 8    -0.081207
## 9     0.292403
## 10    0.526858
## 11   -0.078426
## 12   -0.644047
## 13    0.714568
## 14   -0.593532
## 15   -0.980945
## 16   -0.276851
## 17    7.610845
## 18   -0.873713
## 19    0.050231
## 20    0.205478
## 21   -0.099195
## 22   -0.342041
## 23    0.314708
## 24    0.164757
## 25   -0.819995
## 26   -0.186460
## 27   -0.275590
## 28   -0.017604
## 29   -0.460220
## 30   -1.186396
## 31   -0.176106
## 32    0.733549
## 33   -0.019134
## 34   -0.811830
## dtype: float64</code></pre>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="diag.html#cb79-1" tabindex="-1"></a>influence.cooks_distance</span></code></pre></div>
<pre><code>## (0     0.000513
## 1     0.004875
## 2     0.001365
## 3     0.000064
## 4     0.014741
## 5     0.000051
## 6     1.893349
## 7     0.000043
## 8     0.000095
## 9     0.001405
## 10    0.210521
## 11    0.000096
## 12    0.004704
## 13    0.009339
## 14    0.004834
## 15    0.014907
## 16    0.002386
## 17    0.407156
## 18    0.010265
## 19    0.000042
## 20    0.000870
## 21    0.000172
## 22    0.001661
## 23    0.002108
## 24    0.000499
## 25    0.013179
## 26    0.000511
## 27    0.001018
## 28    0.000003
## 29    0.003669
## 30    0.064123
## 31    0.000531
## 32    0.037695
## 33    0.000005
## 34    0.052422
## dtype: float64, array([0.99998359, 0.99952092, 0.99992875, 0.99999927, 0.99750547,
##        0.99999948, 0.15057304, 0.9999996 , 0.99999869, 0.99992561,
##        0.88836519, 0.99999867, 0.99954591, 0.99873536, 0.99952695,
##        0.9974636 , 0.99983562, 0.74889062, 0.99854403, 0.99999962,
##        0.99996374, 0.99999682, 0.99990442, 0.99986344, 0.99998426,
##        0.99788819, 0.99998366, 0.99995412, 0.99999999, 0.99968682,
##        0.97843259, 0.9999827 , 0.99002625, 0.99999998, 0.9838756 ]))</code></pre>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="diag.html#cb81-1" tabindex="-1"></a>influence.dffits</span></code></pre></div>
<pre><code>## (0     0.038618
## 1    -0.119560
## 2    -0.063095
## 3    -0.013674
## 4    -0.209472
## 5     0.012210
## 6     2.699091
## 7    -0.011150
## 8    -0.016632
## 9     0.063994
## 10    0.785688
## 11   -0.016716
## 12   -0.117701
## 13    0.166102
## 14   -0.119203
## 15   -0.211352
## 16   -0.083368
## 17    1.842375
## 18   -0.174838
## 19    0.011019
## 20    0.050318
## 21   -0.022336
## 22   -0.069613
## 23    0.078394
## 24    0.038085
## 25   -0.197816
## 26   -0.038570
## 27   -0.054459
## 28   -0.003092
## 29   -0.103619
## 30   -0.441381
## 31   -0.039310
## 32    0.333845
## 33   -0.003922
## 34   -0.394450
## dtype: float64, 0.5855400437691199)</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="diag.html#cb83-1" tabindex="-1"></a>influence.dfbetas</span></code></pre></div>
<pre><code>## array([[ 3.78114620e-02, -1.66142583e-02, -4.74356249e-03],
##        [-5.95797144e-02,  6.72153961e-02, -7.33958853e-02],
##        [-4.85768597e-02, -6.70654508e-03,  2.80327646e-02],
##        [-7.66497083e-03, -5.67519011e-03,  8.76365984e-03],
##        [-5.04605283e-02,  8.47092735e-02, -1.45004611e-01],
##        [ 3.48445633e-03, -4.31606470e-03,  7.57593895e-03],
##        [-8.90654684e-01, -7.12773548e-01,  2.36461849e+00],
##        [-8.44278394e-03, -1.64840934e-03,  5.56190747e-03],
##        [-1.43689115e-02,  9.13139595e-04,  6.16065598e-03],
##        [ 4.70341147e-02,  1.30569237e-02, -3.65191836e-02],
##        [-3.01182091e-01,  7.68715994e-01, -4.79849318e-01],
##        [-1.14916485e-02,  9.65572102e-03, -7.48775503e-03],
##        [-3.17290631e-02, -2.99106792e-02, -7.06675373e-04],
##        [ 1.18031242e-01,  4.20335396e-02, -1.04884058e-01],
##        [-1.00376388e-01,  5.77007540e-02, -2.23168727e-02],
##        [-1.85202935e-02,  6.78882683e-03, -9.98617172e-02],
##        [ 1.19637294e-02, -6.65049703e-02,  3.44553620e-02],
##        [ 1.75827483e+00, -4.06545270e-01, -6.55934189e-01],
##        [-1.58890179e-01,  4.43113962e-02,  2.94135680e-02],
##        [ 8.65836948e-03,  1.42439015e-03, -5.94640219e-03],
##        [ 4.77654621e-02, -1.00187391e-02, -1.91985978e-02],
##        [-1.88889123e-02,  1.38562806e-02, -6.46531589e-03],
##        [-4.13064821e-02,  3.40969664e-02, -3.30224386e-02],
##        [ 7.48332952e-02, -4.63850912e-02,  6.42781055e-03],
##        [ 3.69114627e-02, -1.26332955e-02, -8.25681544e-03],
##        [-1.37724315e-01,  1.36123898e-01, -1.01306082e-01],
##        [-2.92047355e-02, -5.70207164e-03,  1.92393928e-02],
##        [-4.76410803e-02,  6.93608846e-03,  1.49895347e-02],
##        [-2.13796696e-03,  6.46622392e-04, -3.28107642e-04],
##        [-8.53158805e-02, -7.70515002e-03,  5.48379624e-02],
##        [ 2.09938203e-02,  1.70124162e-01, -3.73633899e-01],
##        [-2.85790991e-02, -8.69351158e-03,  2.32754469e-02],
##        [-1.58227428e-01,  9.70139844e-02,  1.55701652e-01],
##        [-3.55632705e-03,  7.04242903e-04,  1.05420857e-03],
##        [ 2.08721643e-01, -1.99048204e-01, -1.00907222e-01]])</code></pre>
<p>Plot of external studentized residuals:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="diag.html#cb85-1" tabindex="-1"></a>resid_values<span class="op">=</span>influence.resid_studentized_external</span>
<span id="cb85-2"><a href="diag.html#cb85-2" tabindex="-1"></a></span>
<span id="cb85-3"><a href="diag.html#cb85-3" tabindex="-1"></a></span>
<span id="cb85-4"><a href="diag.html#cb85-4" tabindex="-1"></a>resid_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb85-5"><a href="diag.html#cb85-5" tabindex="-1"></a>    <span class="st">&#39;Observation&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(resid_values) <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb85-6"><a href="diag.html#cb85-6" tabindex="-1"></a>    <span class="st">&#39;Resid&#39;</span>: resid_values</span>
<span id="cb85-7"><a href="diag.html#cb85-7" tabindex="-1"></a>})</span>
<span id="cb85-8"><a href="diag.html#cb85-8" tabindex="-1"></a></span>
<span id="cb85-9"><a href="diag.html#cb85-9" tabindex="-1"></a></span>
<span id="cb85-10"><a href="diag.html#cb85-10" tabindex="-1"></a>p <span class="op">=</span> (</span>
<span id="cb85-11"><a href="diag.html#cb85-11" tabindex="-1"></a>    ggplot(resid_df, aes(x<span class="op">=</span><span class="st">&#39;Observation&#39;</span>, y<span class="op">=</span><span class="st">&#39;Resid&#39;</span>)) <span class="op">+</span></span>
<span id="cb85-12"><a href="diag.html#cb85-12" tabindex="-1"></a>    geom_point(color<span class="op">=</span><span class="st">&#39;orange&#39;</span>) <span class="op">+</span></span>
<span id="cb85-13"><a href="diag.html#cb85-13" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=-</span><span class="dv">2</span>, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb85-14"><a href="diag.html#cb85-14" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=</span><span class="dv">2</span>, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb85-15"><a href="diag.html#cb85-15" tabindex="-1"></a>    labs(</span>
<span id="cb85-16"><a href="diag.html#cb85-16" tabindex="-1"></a>        title<span class="op">=</span><span class="st">&#39;External Studentized residuals&#39;</span>,</span>
<span id="cb85-17"><a href="diag.html#cb85-17" tabindex="-1"></a>        x<span class="op">=</span><span class="st">&#39;Observation&#39;</span>,</span>
<span id="cb85-18"><a href="diag.html#cb85-18" tabindex="-1"></a>        y<span class="op">=</span><span class="st">&#39;Studentized Residuals&#39;</span></span>
<span id="cb85-19"><a href="diag.html#cb85-19" tabindex="-1"></a>    ) <span class="op">+</span></span>
<span id="cb85-20"><a href="diag.html#cb85-20" tabindex="-1"></a>    theme_minimal()</span>
<span id="cb85-21"><a href="diag.html#cb85-21" tabindex="-1"></a>)</span>
<span id="cb85-22"><a href="diag.html#cb85-22" tabindex="-1"></a></span>
<span id="cb85-23"><a href="diag.html#cb85-23" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-1.png" width="614" /></p>
<p>Plot of Cook’s D:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="diag.html#cb86-1" tabindex="-1"></a>cooks_value,_ <span class="op">=</span> influence.cooks_distance</span>
<span id="cb86-2"><a href="diag.html#cb86-2" tabindex="-1"></a></span>
<span id="cb86-3"><a href="diag.html#cb86-3" tabindex="-1"></a><span class="co"># Calculate Cook&#39;s D threshold</span></span>
<span id="cb86-4"><a href="diag.html#cb86-4" tabindex="-1"></a>n <span class="op">=</span> model_io.nobs</span>
<span id="cb86-5"><a href="diag.html#cb86-5" tabindex="-1"></a>p <span class="op">=</span> model_io.df_model <span class="op">+</span> <span class="dv">1</span>  <span class="co"># includes intercept</span></span>
<span id="cb86-6"><a href="diag.html#cb86-6" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="dv">4</span><span class="op">/</span>(n<span class="op">-</span>p)</span>
<span id="cb86-7"><a href="diag.html#cb86-7" tabindex="-1"></a></span>
<span id="cb86-8"><a href="diag.html#cb86-8" tabindex="-1"></a>cooks_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb86-9"><a href="diag.html#cb86-9" tabindex="-1"></a>    <span class="st">&#39;Observation&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(cooks_value) <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb86-10"><a href="diag.html#cb86-10" tabindex="-1"></a>    <span class="st">&#39;Cooks&#39;</span>: cooks_value</span>
<span id="cb86-11"><a href="diag.html#cb86-11" tabindex="-1"></a>})</span>
<span id="cb86-12"><a href="diag.html#cb86-12" tabindex="-1"></a></span>
<span id="cb86-13"><a href="diag.html#cb86-13" tabindex="-1"></a>p <span class="op">=</span> (</span>
<span id="cb86-14"><a href="diag.html#cb86-14" tabindex="-1"></a>    ggplot(cooks_df, aes(x<span class="op">=</span><span class="st">&#39;Observation&#39;</span>, y<span class="op">=</span><span class="st">&#39;Cooks&#39;</span>)) <span class="op">+</span></span>
<span id="cb86-15"><a href="diag.html#cb86-15" tabindex="-1"></a>    geom_point(color<span class="op">=</span><span class="st">&#39;orange&#39;</span>) <span class="op">+</span></span>
<span id="cb86-16"><a href="diag.html#cb86-16" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=</span>cutoff, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb86-17"><a href="diag.html#cb86-17" tabindex="-1"></a>    labs(</span>
<span id="cb86-18"><a href="diag.html#cb86-18" tabindex="-1"></a>        title<span class="op">=</span><span class="st">&#39;Cooks D Plot with Cutoff&#39;</span>,</span>
<span id="cb86-19"><a href="diag.html#cb86-19" tabindex="-1"></a>        x<span class="op">=</span><span class="st">&#39;Observation&#39;</span>,</span>
<span id="cb86-20"><a href="diag.html#cb86-20" tabindex="-1"></a>        y<span class="op">=</span><span class="st">&#39;CooksD&#39;</span></span>
<span id="cb86-21"><a href="diag.html#cb86-21" tabindex="-1"></a>    ) <span class="op">+</span></span>
<span id="cb86-22"><a href="diag.html#cb86-22" tabindex="-1"></a>    theme_minimal()</span>
<span id="cb86-23"><a href="diag.html#cb86-23" tabindex="-1"></a>)</span>
<span id="cb86-24"><a href="diag.html#cb86-24" tabindex="-1"></a></span>
<span id="cb86-25"><a href="diag.html#cb86-25" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-26-3.png" width="614" /></p>
<p>Plot of DFFITS:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb87-1"><a href="diag.html#cb87-1" tabindex="-1"></a>dffits_values, _ <span class="op">=</span> influence.dffits</span>
<span id="cb87-2"><a href="diag.html#cb87-2" tabindex="-1"></a></span>
<span id="cb87-3"><a href="diag.html#cb87-3" tabindex="-1"></a><span class="co"># Calculate DFFITS threshold</span></span>
<span id="cb87-4"><a href="diag.html#cb87-4" tabindex="-1"></a>n <span class="op">=</span> model_io.nobs</span>
<span id="cb87-5"><a href="diag.html#cb87-5" tabindex="-1"></a>p <span class="op">=</span> model_io.df_model <span class="op">+</span> <span class="dv">1</span>  <span class="co"># includes intercept</span></span>
<span id="cb87-6"><a href="diag.html#cb87-6" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (p <span class="op">/</span> n) <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb87-7"><a href="diag.html#cb87-7" tabindex="-1"></a></span>
<span id="cb87-8"><a href="diag.html#cb87-8" tabindex="-1"></a>dffits_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb87-9"><a href="diag.html#cb87-9" tabindex="-1"></a>    <span class="st">&#39;Observation&#39;</span>: <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(dffits_values) <span class="op">+</span> <span class="dv">1</span>),</span>
<span id="cb87-10"><a href="diag.html#cb87-10" tabindex="-1"></a>    <span class="st">&#39;DFFITS&#39;</span>: dffits_values</span>
<span id="cb87-11"><a href="diag.html#cb87-11" tabindex="-1"></a>})</span>
<span id="cb87-12"><a href="diag.html#cb87-12" tabindex="-1"></a></span>
<span id="cb87-13"><a href="diag.html#cb87-13" tabindex="-1"></a></span>
<span id="cb87-14"><a href="diag.html#cb87-14" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (p <span class="op">/</span> n) <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb87-15"><a href="diag.html#cb87-15" tabindex="-1"></a></span>
<span id="cb87-16"><a href="diag.html#cb87-16" tabindex="-1"></a>plot <span class="op">=</span> (</span>
<span id="cb87-17"><a href="diag.html#cb87-17" tabindex="-1"></a>    ggplot(dffits_df, aes(x<span class="op">=</span><span class="st">&#39;Observation&#39;</span>, y<span class="op">=</span><span class="st">&#39;DFFITS&#39;</span>)) <span class="op">+</span></span>
<span id="cb87-18"><a href="diag.html#cb87-18" tabindex="-1"></a>    geom_point(color<span class="op">=</span><span class="st">&#39;orange&#39;</span>) <span class="op">+</span></span>
<span id="cb87-19"><a href="diag.html#cb87-19" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=</span>cutoff, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb87-20"><a href="diag.html#cb87-20" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=-</span>cutoff, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;red&#39;</span>) <span class="op">+</span></span>
<span id="cb87-21"><a href="diag.html#cb87-21" tabindex="-1"></a>    labs(</span>
<span id="cb87-22"><a href="diag.html#cb87-22" tabindex="-1"></a>        title<span class="op">=</span><span class="st">&#39;DFFITS Plot with Cutoff&#39;</span>,</span>
<span id="cb87-23"><a href="diag.html#cb87-23" tabindex="-1"></a>        x<span class="op">=</span><span class="st">&#39;Observation&#39;</span>,</span>
<span id="cb87-24"><a href="diag.html#cb87-24" tabindex="-1"></a>        y<span class="op">=</span><span class="st">&#39;DFFITS&#39;</span></span>
<span id="cb87-25"><a href="diag.html#cb87-25" tabindex="-1"></a>    ) <span class="op">+</span></span>
<span id="cb87-26"><a href="diag.html#cb87-26" tabindex="-1"></a>    theme_minimal()</span>
<span id="cb87-27"><a href="diag.html#cb87-27" tabindex="-1"></a>)</span>
<span id="cb87-28"><a href="diag.html#cb87-28" tabindex="-1"></a></span>
<span id="cb87-29"><a href="diag.html#cb87-29" tabindex="-1"></a>plot.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-27-5.png" width="614" /></p>
<p>Plot studentized residuals and leverage:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="diag.html#cb88-1" tabindex="-1"></a>n <span class="op">=</span> model_io.nobs</span>
<span id="cb88-2"><a href="diag.html#cb88-2" tabindex="-1"></a>p <span class="op">=</span> model_io.df_model <span class="op">+</span> <span class="dv">1</span>  <span class="co"># includes intercept</span></span>
<span id="cb88-3"><a href="diag.html#cb88-3" tabindex="-1"></a>cutoff <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> (p <span class="op">/</span> n) </span>
<span id="cb88-4"><a href="diag.html#cb88-4" tabindex="-1"></a>leverage_values <span class="op">=</span> influence.hat_matrix_diag</span>
<span id="cb88-5"><a href="diag.html#cb88-5" tabindex="-1"></a>influence_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb88-6"><a href="diag.html#cb88-6" tabindex="-1"></a>    <span class="st">&#39;leverage&#39;</span>: leverage_values,</span>
<span id="cb88-7"><a href="diag.html#cb88-7" tabindex="-1"></a>    <span class="st">&#39;Resid&#39;</span>: resid_values,</span>
<span id="cb88-8"><a href="diag.html#cb88-8" tabindex="-1"></a>    <span class="st">&#39;observation&#39;</span>: np.arange(n),</span>
<span id="cb88-9"><a href="diag.html#cb88-9" tabindex="-1"></a>    <span class="st">&#39;Cooks&#39;</span>: cooks_value</span>
<span id="cb88-10"><a href="diag.html#cb88-10" tabindex="-1"></a>})</span>
<span id="cb88-11"><a href="diag.html#cb88-11" tabindex="-1"></a></span>
<span id="cb88-12"><a href="diag.html#cb88-12" tabindex="-1"></a>influence_plot <span class="op">=</span> (</span>
<span id="cb88-13"><a href="diag.html#cb88-13" tabindex="-1"></a>    ggplot(influence_df, aes(x<span class="op">=</span><span class="st">&#39;leverage&#39;</span>, y<span class="op">=</span><span class="st">&#39;Resid&#39;</span>, size<span class="op">=</span><span class="st">&#39;Cooks&#39;</span>)) <span class="op">+</span></span>
<span id="cb88-14"><a href="diag.html#cb88-14" tabindex="-1"></a>    geom_point(color<span class="op">=</span><span class="st">&#39;blue&#39;</span>) <span class="op">+</span></span>
<span id="cb88-15"><a href="diag.html#cb88-15" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=</span><span class="dv">2</span>, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;purple&#39;</span>) <span class="op">+</span></span>
<span id="cb88-16"><a href="diag.html#cb88-16" tabindex="-1"></a>    geom_hline(yintercept<span class="op">=-</span><span class="dv">2</span>, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;purple&#39;</span>) <span class="op">+</span></span>
<span id="cb88-17"><a href="diag.html#cb88-17" tabindex="-1"></a>    geom_vline(xintercept<span class="op">=</span>cutoff, linetype<span class="op">=</span><span class="st">&#39;dashed&#39;</span>, color<span class="op">=</span><span class="st">&#39;green&#39;</span>) <span class="op">+</span></span>
<span id="cb88-18"><a href="diag.html#cb88-18" tabindex="-1"></a>    scale_size_continuous(name<span class="op">=</span><span class="st">&quot;Cook&#39;s D&quot;</span>, <span class="bu">range</span><span class="op">=</span>[<span class="dv">1</span>, <span class="dv">20</span>]) <span class="op">+</span></span>
<span id="cb88-19"><a href="diag.html#cb88-19" tabindex="-1"></a>    labs(</span>
<span id="cb88-20"><a href="diag.html#cb88-20" tabindex="-1"></a>        title<span class="op">=</span><span class="st">&#39;Influence Plot with Cutoffs&#39;</span>,</span>
<span id="cb88-21"><a href="diag.html#cb88-21" tabindex="-1"></a>        x<span class="op">=</span><span class="st">&#39;Leverage&#39;</span>,</span>
<span id="cb88-22"><a href="diag.html#cb88-22" tabindex="-1"></a>        y<span class="op">=</span><span class="st">&#39;Studentized Residuals&#39;</span></span>
<span id="cb88-23"><a href="diag.html#cb88-23" tabindex="-1"></a>    ) <span class="op">+</span></span>
<span id="cb88-24"><a href="diag.html#cb88-24" tabindex="-1"></a>    theme_minimal()</span>
<span id="cb88-25"><a href="diag.html#cb88-25" tabindex="-1"></a>)</span>
<span id="cb88-26"><a href="diag.html#cb88-26" tabindex="-1"></a></span>
<span id="cb88-27"><a href="diag.html#cb88-27" tabindex="-1"></a><span class="co"># Mark influential or outlying points</span></span>
<span id="cb88-28"><a href="diag.html#cb88-28" tabindex="-1"></a>influence_df[<span class="st">&#39;label&#39;</span>] <span class="op">=</span> influence_df.<span class="bu">apply</span>(</span>
<span id="cb88-29"><a href="diag.html#cb88-29" tabindex="-1"></a>    <span class="kw">lambda</span> row: <span class="bu">str</span>(row[<span class="st">&#39;observation&#39;</span>]) <span class="cf">if</span> <span class="bu">abs</span>(row[<span class="st">&#39;Resid&#39;</span>]) <span class="op">&gt;</span> <span class="dv">2</span> <span class="kw">and</span> row[<span class="st">&#39;leverage&#39;</span>] <span class="op">&gt;</span> cutoff <span class="cf">else</span> <span class="st">&#39;&#39;</span>,</span>
<span id="cb88-30"><a href="diag.html#cb88-30" tabindex="-1"></a>    axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb88-31"><a href="diag.html#cb88-31" tabindex="-1"></a>)</span>
<span id="cb88-32"><a href="diag.html#cb88-32" tabindex="-1"></a></span>
<span id="cb88-33"><a href="diag.html#cb88-33" tabindex="-1"></a>influence_plot <span class="op">+=</span> geom_text(aes(label<span class="op">=</span><span class="st">&#39;label&#39;</span>), ha<span class="op">=</span><span class="st">&#39;left&#39;</span>, nudge_x<span class="op">=</span><span class="fl">0.01</span>, size<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb88-34"><a href="diag.html#cb88-34" tabindex="-1"></a></span>
<span id="cb88-35"><a href="diag.html#cb88-35" tabindex="-1"></a>influence_plot.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-28-7.png" width="614" /></p>
</div>
</div>
<div id="multicollinearity" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Multicollinearity<a href="diag.html#multicollinearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multicollinearity occurs when one or more predictor variables are linearly related to each other and will create issues with the regression. The parameter estimates will not be stable and the standard errors will be inflated (making it more difficult to find significant explantory variables). The two most common ways to identify multicollinearity is by looking at the correlation among the predictor variables and calculating the variance inflation factor.</p>
<p>The variance inflation factor (also referred to as VIF) will take each explanatory variable and model it as a linear regression of the remaining explanatory variables. For example, let’s say we have the following regression equation:<br />
<span class="math display">\[\widehat{Y}_{i}=b_{0} + b_{1}x_{1}+ b_{2}x_{2}+ b_{3}x_{3}.\]</span>
There are only 3 explanatory variables in this regression. A VIF will be calculated on each <span class="math inline">\(x_{i}\)</span> in the following manner:</p>
<ol style="list-style-type: decimal">
<li>A regression is fit on each <span class="math inline">\(x_{i}\)</span> with the remaining <span class="math inline">\(x_{i}\)</span>’s as the explanatory variables. For example, to calculate the VIF for <span class="math inline">\(x_{1}\)</span>, we fit the following model:</li>
</ol>
<p><span class="math display">\[\widehat{x}_{1}=b_{0} + b_{1}x_{2}+ b_{2}x_{3},\]</span>
and obtain the <span class="math inline">\(R^2\)</span> value from this model (call it <span class="math inline">\(R_{1}^2\)</span>).<br />
2. The VIF for <span class="math inline">\(x_{1}\)</span> is calculated by
<span class="math display">\[VIF=\frac{1}{1-R_{1}^2}.\]</span>
Repeat this process for each of the other explanatory variables. If a VIF value is larger than 10, then we say that multicollinearity is an issue.</p>
<p><strong>Example</strong></p>
<p>We will use the mtcars data set dealing with fuel consumption and automobile design. The data set consists of the following variables:</p>
<p>A data frame with 32 observations on 11 (numeric) variables.</p>
<ul>
<li>mpg: Miles/(US) gallon<br />
</li>
<li>cyl: Number of cylinders<br />
</li>
<li>disp: Displacement (cu.in.)<br />
</li>
<li>hp: Gross horsepower<br />
</li>
<li>drat: Rear axle ratio<br />
</li>
<li>wt: Weight (1000 lbs)<br />
</li>
<li>qsec: 1/4 mile time<br />
</li>
<li>vs: Engine (0 = V-shaped, 1 = straight)<br />
</li>
<li>am: Transmission (0 = automatic, 1 = manual)<br />
</li>
<li>gear: Number of forward gears</li>
</ul>
<div id="r-code-28" class="section level4 hasAnchor" number="5.7.0.1">
<h4><span class="header-section-number">5.7.0.1</span> R code:<a href="diag.html#r-code-28" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="diag.html#cb89-1" tabindex="-1"></a><span class="fu">cor</span>(mtcars)</span></code></pre></div>
<pre><code>##             mpg        cyl       disp         hp        drat         wt
## mpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594
## cyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958
## disp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799
## hp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479
## drat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406
## wt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000
## qsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159
## vs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157
## am    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953
## gear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870
## carb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059
##             qsec         vs          am       gear        carb
## mpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507
## cyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829
## disp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686
## hp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247
## drat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980
## wt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594
## qsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923
## vs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714
## am   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435
## gear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284
## carb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000</code></pre>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="diag.html#cb91-1" tabindex="-1"></a>lm.model<span class="ot">=</span><span class="fu">lm</span>(mpg<span class="sc">~</span>.,<span class="at">data=</span>mtcars)</span>
<span id="cb91-2"><a href="diag.html#cb91-2" tabindex="-1"></a><span class="fu">vif</span>(lm.model)</span></code></pre></div>
<pre><code>##       cyl      disp        hp      drat        wt      qsec        vs        am 
## 15.373833 21.620241  9.832037  3.374620 15.164887  7.527958  4.965873  4.648487 
##      gear      carb 
##  5.357452  7.908747</code></pre>
<p>From the correlation output and VIF output, it is clear that multicollinearity is an issue. To deal with multicollinearity, we can do either of the following:</p>
<ol style="list-style-type: decimal">
<li>Remove one or more variables that are co-linearly related to another variable(s).<br />
</li>
<li>Create new transformed variables (take linear combinations of variables; create ratio of variables, etc).</li>
</ol>
</div>
<div id="python-code-30" class="section level3 hasAnchor" number="5.7.1">
<h3><span class="header-section-number">5.7.1</span> Python Code<a href="diag.html#python-code-30" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="diag.html#cb93-1" tabindex="-1"></a>mtcars <span class="op">=</span> r.mtcars</span>
<span id="cb93-2"><a href="diag.html#cb93-2" tabindex="-1"></a></span>
<span id="cb93-3"><a href="diag.html#cb93-3" tabindex="-1"></a>np.corrcoef(mtcars, rowvar <span class="op">=</span> <span class="va">False</span>)</span></code></pre></div>
<pre><code>## array([[ 1.        , -0.85216196, -0.84755138, -0.77616837,  0.68117191,
##         -0.86765938,  0.41868403,  0.66403892,  0.59983243,  0.48028476,
##         -0.55092507],
##        [-0.85216196,  1.        ,  0.90203287,  0.83244745, -0.69993811,
##          0.78249579, -0.59124207, -0.8108118 , -0.52260705, -0.4926866 ,
##          0.52698829],
##        [-0.84755138,  0.90203287,  1.        ,  0.79094859, -0.71021393,
##          0.88797992, -0.43369788, -0.71041589, -0.59122704, -0.5555692 ,
##          0.39497686],
##        [-0.77616837,  0.83244745,  0.79094859,  1.        , -0.44875912,
##          0.65874789, -0.70822339, -0.72309674, -0.24320426, -0.12570426,
##          0.74981247],
##        [ 0.68117191, -0.69993811, -0.71021393, -0.44875912,  1.        ,
##         -0.71244065,  0.09120476,  0.44027846,  0.71271113,  0.69961013,
##         -0.0907898 ],
##        [-0.86765938,  0.78249579,  0.88797992,  0.65874789, -0.71244065,
##          1.        , -0.17471588, -0.55491568, -0.69249526, -0.583287  ,
##          0.42760594],
##        [ 0.41868403, -0.59124207, -0.43369788, -0.70822339,  0.09120476,
##         -0.17471588,  1.        ,  0.74453544, -0.22986086, -0.21268223,
##         -0.65624923],
##        [ 0.66403892, -0.8108118 , -0.71041589, -0.72309674,  0.44027846,
##         -0.55491568,  0.74453544,  1.        ,  0.16834512,  0.20602335,
##         -0.56960714],
##        [ 0.59983243, -0.52260705, -0.59122704, -0.24320426,  0.71271113,
##         -0.69249526, -0.22986086,  0.16834512,  1.        ,  0.79405876,
##          0.05753435],
##        [ 0.48028476, -0.4926866 , -0.5555692 , -0.12570426,  0.69961013,
##         -0.583287  , -0.21268223,  0.20602335,  0.79405876,  1.        ,
##          0.27407284],
##        [-0.55092507,  0.52698829,  0.39497686,  0.74981247, -0.0907898 ,
##          0.42760594, -0.65624923, -0.56960714,  0.05753435,  0.27407284,
##          1.        ]])</code></pre>
<div class="sourceCode" id="cb95"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb95-1"><a href="diag.html#cb95-1" tabindex="-1"></a>X <span class="op">=</span> mtcars.loc[:, mtcars.columns<span class="op">!=</span><span class="st">&#39;mpg&#39;</span>]</span>
<span id="cb95-2"><a href="diag.html#cb95-2" tabindex="-1"></a>X <span class="op">=</span> X.assign(const<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb95-3"><a href="diag.html#cb95-3" tabindex="-1"></a></span>
<span id="cb95-4"><a href="diag.html#cb95-4" tabindex="-1"></a>vif <span class="op">=</span> pd.DataFrame()</span>
<span id="cb95-5"><a href="diag.html#cb95-5" tabindex="-1"></a>vif[<span class="st">&#39;VIF&#39;</span>] <span class="op">=</span> [ss.outliers_influence.variance_inflation_factor(X.values, i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>])]</span>
<span id="cb95-6"><a href="diag.html#cb95-6" tabindex="-1"></a>vif[<span class="st">&#39;variable&#39;</span>] <span class="op">=</span> X.columns</span>
<span id="cb95-7"><a href="diag.html#cb95-7" tabindex="-1"></a></span>
<span id="cb95-8"><a href="diag.html#cb95-8" tabindex="-1"></a><span class="bu">print</span>(vif)</span></code></pre></div>
<pre><code>##             VIF variable
## 0     15.373833      cyl
## 1     21.620241     disp
## 2      9.832037       hp
## 3      3.374620     drat
## 4     15.164887       wt
## 5      7.527958     qsec
## 6      4.965873       vs
## 7      4.648487       am
## 8      5.357452     gear
## 9      7.908747     carb
## 10  1596.273030    const</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-selection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-building-and-scoring-for-prediction.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/IAA-Faculty/statistical_foundations.git/edit/master/05-diagnostics.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/IAA-Faculty/statistical_foundations.git/blob/master/05-diagnostics.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "toc": null
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
