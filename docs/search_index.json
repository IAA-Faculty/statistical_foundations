[["index.html", "Statistical Foundations Preface", " Statistical Foundations Preface Structure of the book The book is broken down into small sections that aim to demonstrate a single concept at a time. Chapter ?? introduces the basic components of R, and how you can assign values to variables. This book is a work in progress. Submit any issues here. Please check back for frequent updates. About the Authors Acknowledgements The author would like to thank the members of the faculty and the TAs at the Institute who provided feedback on this work. "],["intro-stat.html", "Chapter 1 Introduction to Statistics 1.1 Exploratory Data Analysis (EDA) 1.2 Point Estimates 1.3 Confidence Intervals 1.4 Hypothesis Testing 1.5 Two-Sample t-tests", " Chapter 1 Introduction to Statistics This first chapter will review some basic statistical concepts that we assume the reader has seen previously. We will demonstrate how to implement these concepts in R along the way. 1.1 Exploratory Data Analysis (EDA) The crucial first step to any data science problem is exploratory data analysis (EDA). Before you attempt to run any models, or jump towards any formal statistical analysis, you must explore your data. Many unexpected frustrations arise when exploratory analysis is overlooked; knowing your data is critical to your ability to make necessary assumptions about it. This preliminary analysis will help inform our decisions for data manipulation, give us a base-level understanding of our variables and the relationships between them, and help determine which statistical analyses might be appropriate for the questions we are trying to answer. Some of the questions we aim to answer through exploratory analysis are: What kind of variables to you have? Continuous Nominal Ordinal How are the attributes stored? Strings Integers Floats/Numeric Dates What do their distributions look like? Center/Location Spread Shape Are there any anomolies? Outliers Leverage points Missing values Low-frequency categories We will maintain an example data set throughout this text to demonstrate the various tools and techniques being discussed. It is a real-estate data set that contains the sale_price and physical attributes of nearly 3,000 homes in Ames, Iowa in the early 2000s. To access this data, we first install the AmesHousing package, add it to our library and create the nicely formatted data with the make_ordinal_ames() function. install.packages(&quot;AmesHousing&quot;) library(AmesHousing) ames &lt;- make_ordinal_ames() str(ames) 1.1.1 Types of Variables The columns of a data set are referred to by the following equivalent terms: Variables Features Attributes Predictors/Targets Factors Inputs/Outputs This book may use any of these words interchangeably to refer to a quality or quantity of interest in our data. Nominal Variables A nominal or categorical variable is a quality of interest whose values have no logical ordering. Color (“blue”, “red”, “green”…), ethnicity (“African-American”, “Asian”, “Caucasian”,…), and style of house (“ranch”, “two-story”, “duplex”, …) are all examples of nominal attributes. The categories or values that these variables can take on - those words listed in quotes and parenthesis - are called the levels of the variable. In modeling, nominal attributes are commonly transformed into dummy variables. Dummy variables are binary columns that indicate the presence or absence of a quality. There is more than one way to create dummy variables, and the treatment will be different depending on what type of model you are using. Linear regression models will use either reference-level or effects coding, whereas other machine learning models are more likely to use one-hot encoding or a variation thereof. One-hot encoding For machine learning applications, it is common to create a binary dummy column for each level of your categorical variable. This is the most intuitive representation of categorical information, answering indicative questions for each level of the variable: “is it blue?”, “is it red?” etc. The table below gives an example of some data, the original nominal variable (color) and the one-hot encoded color information. Observation Color Blue Red Yellow Other 1 Blue 1 0 0 0 2 Yellow 0 0 1 0 3 Blue 1 0 0 0 4 Red 0 1 0 0 5 Red 0 1 0 0 6 Blue 1 0 0 0 7 Yellow 0 0 1 0 8 Other 0 0 0 1 Table 1.1: One-hot dummy variable coding for the categorical attribute color We will demonstrate the creation of this data using some simple random categorical data: set.seed(41) data &lt;- data.frame(y = c(rnorm(10,2), rnorm(10,1),rnorm(10,0)), x1 = factor(rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 10)), x2 = factor(rep(c(&quot;Z&quot;, &quot;X&quot;, &quot;Y&quot;,&quot;W&quot;,&quot;V&quot;,&quot;U&quot;), each = 5))) View(data) Unlike reference and effects coding, which are typically specified within the lm() function as we will see in Chapter 2, one-hot encoding is most quickly achieved through use of the onehot package in R, which first creates an “encoder” to do the job quickly. The speed of this function has been tested against both the base R model.matrix() function and the dummyVars() function in the caret package and is substantially faster than either. install.packages(&quot;onehot&quot;) library(onehot) encoder = onehot(data) dummies = predict(encoder,data) head(dummies) ## y x1=A x1=B x1=C x2=U x2=V x2=W x2=X x2=Y x2=Z ## [1,] 1.205632 1 0 0 0 0 0 0 0 1 ## [2,] 2.197258 1 0 0 0 0 0 0 0 1 ## [3,] 3.001704 1 0 0 0 0 0 0 0 1 ## [4,] 3.288825 1 0 0 0 0 0 0 0 1 ## [5,] 2.905753 1 0 0 0 0 0 0 0 1 ## [6,] 2.493667 1 0 0 0 0 0 1 0 0 Reference-level coding Reference-level coding is similar to one-hot encoding except one of the levels of the attribute, called the reference level, is omitted. Notice that the 4 dummy columns from Table 1.1 collectively form a linearly dependent set; that is, if you know the values of 3 of the 4 dummy variables you can determine the \\(4^{th}\\) with complete certainty. This would be a problem for linear regression, where we assume our input attributes are not linearly dependent as we will discuss in Chapter 2. A reference level of the attribute is often specified by the user to be a particular level worthy of comparison (a baseline), as the regression output will be interpreted in a way that compares each non-reference level to the reference level. If a reference level is not specified by the user, one will be picked by the software by default either using the order in which the levels were encountered in the data, or their alphabetical ordering. Users should check the documentation of the associated function to understand what to expect. Table 1.2 transforms the one-hot encoding from Table 1.1 into reference-level coding with the color “blue” as the reference level. Notice the absence of the column indicating “blue” and how each blue observation exists as a row of zeros. Observation Color Red Yellow Other 1 Blue 0 0 0 2 Yellow 0 1 0 3 Blue 0 0 0 4 Red 1 0 0 5 Red 1 0 0 6 Blue 0 0 0 7 Yellow 0 1 0 8 Other 0 0 1 Table 1.2: Reference-level dummy variable coding for the categorical attribute color and the reference level of “blue” Effects coding Effects coding is useful for obtaining a more general comparative interpretation when you have approximately equal sample sizes across each level of your categorical attribute. Effects coding is designed to allow the user to compare each level to all the other levels. More specifically the mean of each level is compared to the overall mean of your data. However, the comparison is actually to the so-called grand mean, which is the mean of the means of each group. When sample sizes are equal, the grand mean and the overall sample mean are equivalent. When sample sizes are not equal, the parameter estimates for effects coding should not be used for interpretation or explanation. Effects coding still requires a reference level, however the purpose of the reference level is not the same as it was in reference-level coding. Here, the reference level is left out in the sense that no comparison is made between it and the overall mean. Table 1.3 shows our same example with effects coding. Again we notice the absence of the column indicating “blue” but now the reference level receives values of -1 rather than 0 for all 3 dummy columns. We will revisit the interpretation of linear regression coefficients under this coding scheme in Chapter 2. Observation Color Red Yellow Other 1 Blue -1 -1 -1 2 Yellow 0 1 0 3 Blue -1 -1 -1 4 Red 1 0 0 5 Red 1 0 0 6 Blue -1 -1 -1 7 Yellow 0 1 0 8 Other 0 0 1 Table 1.3: Effects coding for the categorical attribute color and the reference level of “blue” Interval Variables An interval variable is a quantity of interest on which the mathematical operations of addition, subtraction, multiplication and division can be performed. Time, temperature and age are all examples of interval attributes. To illustrate the definition, note that “15 minutes” divided by “5 minutes” is 3, which indicates that 15 minutes is 3 times as long as 5 minutes. The sensible interpretation of this simple arithmetic sentence demonstrates the nature of interval attributes. One should note that such arithmetic would not make sense in the treatment of nominal variables. Ordinal Variables Ordinal variables are attributes that are qualitative in nature but have some natural ordering. Level of education is a common example, with a level of ‘PhD’ indicating more education than ‘Bachelors’ but lacking a numerical framework to quantify how much more. The treatment of ordinal variables will depend on the application. Survey responses on a Likert scale are also ordinal - a response of 4=“somewhat agree” on a 1-to-5 scale of agreement cannot reliably be said to be twice as enthusiastic as a response of 2=“somewhat disagree”. These are not interval measurements, though they are often treated as such in a trade-off for computational efficiency. Ordinal variables will either be given some numeric value and treated as interval variables or they will be treated as categorical variables and dummy variables will be created. The choice of solution is up to the analyst. When numeric values are assigned to ordinal variables, the possibilities are many. For example, consider level of education. The simplest ordinal treatment for such an attribute might be something like Table 1.4. Level of Education Numeric Value No H.S. Diploma 1 H.S. Diploma or GED 2 Associates or Certificate 3 Bachelors 4 Graduate Certificate 5 Masters 6 PhD 7 Table 1.4: One potential approach to scaling the ordinal attribute level of education While numeric values have been assigned and this data could be used like an interval attribute, it’s important to realize that the notion of a “one-unit increase” is qualitative in nature rather than quantitative. However, if we’re interested in learning whether there is a linear type of relationship between education and another attribute (meaning as education level increases, the value of another attribute increases or decreases), this would be the path to get us there. However we’re making an assumption in this model that the difference between a H.S. Diploma and an Associates degree (a difference of “1 unit”) is the same as the difference between a Master’s degree and a PhD (also a difference of “1 unit”). These types of assumptions can be flawed, and it is often desirable to develop an alternative system of measurement based either on domain expertise or the target variable of interest. This is the notion behind optimal scaling and target-level encoding. Optimal Scaling The primary idea behind optimal scaling is to transform an ordinal attribute into an interval one in a way that doesn’t restrict the numeric values to simply the integers \\(1,2,3, \\dots\\). It’s reasonable for a data scientist to use domain expertise to develop an alternative scheme. For example, if analyzing movie theater concessions with ordinal drink sizes {small, medium, large}, one is not restricted to the numeric valuation of 1=small, 2=medium, and 3=large just because it’s an ordinal variable with 3 levels. Perhaps it would make more sense to use the drink size in fluid ounces to represent the ordinality. If the small drink is 12 ounces, the medium is 20 ounces, and the large is 48 ounces, then using those values as the numerical representation would be every bit as (if not more) reasonable than using the standard integers 1, 2, and 3. If we re-consider the ordinal attribute level of education, we might decide to represent the approximate years of post-secondary schooling required to obtain a given level. This might lead us to something like the attribute values in Table 1.5. Level of Education Numeric Value No H.S. Diploma -1 H.S. Diploma or GED 0 Associate’s or Certificate 2 Bachelor’s 4 Graduate Certificate 5 Master’s 6 PhD 8 Table 1.5: One potential approach to scaling the ordinal attribute level of education If we were modeling the effect of education on something like salary, it seems reasonable to assume that the jumps between levels should not have equal distance like they did in 1.4. It seems reasonable to assume that one would experience a larger salary lift from Associate’s to Bachelor’s degree than they would from No H.S. Diploma to GED. Target-level Encoding 1.1.2 Distributions After reviewing the types and formats of the data inputs, we move on to some basic univariate (one variable at a time) analysis. We start by describing the distribution of values that each variable takes on. For nominal variables, this amounts to frequency tables and bar charts of how often each level of the variable appears in the data set. We’ll begin by exploring one of our nominal features, Heating_QC which categorizes the quality and condition of a home’s heating system. To create plots in R, we will use the popular ggplot2 library. At the same time, we will load the tidyverse library which we will use often in this text. install.packages(&quot;ggplot2&quot;) install.packages(&quot;tidyverse&quot;) library(ggplot2) library(tidyverse) ggplot(data = ames) + geom_bar(mapping = aes(x = Heating_QC)) Figure 1.1: Distribution of Nominal Variable Heating_QC To summon the same information in tabular form, we can use the count() function to create a table: ames %&gt;% count(Heating_QC) ## # A tibble: 5 x 2 ## Heating_QC n ## &lt;ord&gt; &lt;int&gt; ## 1 Poor 3 ## 2 Fair 92 ## 3 Typical 864 ## 4 Good 476 ## 5 Excellent 1495 You’ll notice that very few houses (3) have heating systems in Poor condition, and the majority of houses have systems rated Excellent. It will likely make sense to combine the categories of Fair and Poor in our eventual analysis, a decision we will later revisit. Next we create a histogram for an interval attribute like Sale_Price: ggplot(data = ames) + geom_histogram(mapping = aes(x = Sale_Price/1000)) + labs(x = &quot;Sales Price (Thousands $)&quot;) Figure 1.2: Distribution of Interval Variable Sale_Price From this initial inspection, we can conclude that most of the houses sell for less than $200,000 and there are a number of expensive anomalies. To more concretely describe and quantify a statistical distribution, we use statistics that describe the location, spread, and shape of the data. Location The location of a distribution refers to the x-axis of a histogram like that in Figure 1.2. Where is most of the data located? The sample mean, median, and mode are the most common statistics of location, but percentiles and the interquartile range can also be seen in this light. We define each of these terms below for a variable \\(\\mathbf{x}\\) having n observations with values \\(\\{x_i\\}_{i=1}^n\\), sorted in order of magnitude such that \\(x_1 \\leq x_2 \\leq \\dots \\leq x_n\\): Mean: The average of the observations, \\(\\bar{\\mathbf{x}}= \\frac{1}{n}\\sum_{i=1}^n x_i\\) Median: The “middle value” of the data. Formally, when \\(n\\) is odd, the median is the observation value, \\(x_m = x_{\\frac{(n+1)}{2}}\\) for which \\(x_i &lt; x_m\\) for 50% of the observations (excluding \\(x_m\\)). When \\(n\\) is even, \\(x_m\\) is the average of \\(x_\\frac{n}{2}\\) and \\(x_{(\\frac{n}{2}+1)}\\). The median is also known as the \\(2^{nd}\\) quartile. Mode: The most commonly occurring value in the data. Most commonly used to describe nominal attributes. Percentiles: The 99 intermediate values of the data which divide the observations into 100 equally-sized groups. The \\(r^{th}\\) percentile of the data, \\(P_r\\) is the number for which \\(r\\%\\) of the data is less than \\(P_r\\). Quartiles: The quartiles of a variable are the \\(25^{th}\\), \\(50^{th}\\), and \\(75^{th}\\) percentiles. They are denoted as \\(Q_1\\) (\\(1^{st}\\) quartile), \\(Q_2\\) (\\(2^{nd}\\) quartile = median), and \\(Q_3\\) (\\(3^{rd}\\) quartile) respectively. Illustrative Example Suppose the following table contains the heights of 10 students randomly sampled from NC State’s campus. Compute the mean, median, mode and quartiles of this variable. height 60 62 63 65 67 67 67 68 68 69 Solution: The mean is (60+62+63+65+67+67+67+68+68+69)/10 = 65.6. The median (second quartile) is (67+67)/2 = 67. The mode is 67. The first quartile is (62+63)/2 = 62.5 The third quartile is (68+68)/2 = 68 Spread Once we have an understanding of where the bulk of the data is located, we move on to describing the spread (the dispersion or variation) of the data. Range, interquartile range, variance, and standard deviation are all statistics that describe spread. Range: The difference between the maximum and minimum data values. Interquartile range (IQR): The difference between the \\(25^{th}\\) and \\(75^{th}\\) percentiles. Sample variance: The sum of squared differences between each data point and the mean, divided by (n-1). \\(\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2\\) Standard deviation: The square root of the sample variance. One should note that standard deviation is more frequently reported than variance because it shares the same units as the original data, and because of the guidance provided by the empirical rule. If we’re exploring something like Sale_Price, which has the unit “dollars”, then the variance would be measured in “square-dollars”, which hampers the intuition. Standard deviation, on the other hand, would share the unit “dollars”, aiding our fundamental understanding. Illustrative Example Let’s again use the table of heights, this time computing the range, IQR, sample variance and standard deviation. height 60 62 63 65 67 67 67 68 68 69 Solution: The range 69-60 = 9. The IQR is 68 - 62.5 = 5.5. The variance is ((60-65.6)^2+(62-65.6)^2+(63-65.6)^2+(65-65.6)^2+(67-65.6)^2+(67-65.6)^2+(67-65.6)^2+(68-65.6)^2+(68-65.6)^2+(69-65.6)^2)/9 = 8.933 The standard deviation is sqrt(8.933) = 2.989 Shape The final description we will want to give to distributions regards their shape. Is the histogram symmetric? Is it unimodal (having a single large “heap” of data) or multimodal (having multiple heaps\")? Does it have a longer tail on one side than the other (skew)? Is there a lot more or less data in the tails than you might expect? We’ll formalize these ideas with some illustrations. A distribution is right (left) skewed if it has a longer tail on its right (left) side, as shown in Figure 1.3. knitr::include_graphics(&quot;img/skewdiagrams.png&quot;) Figure 1.3: Examples of Left-Skewed (Negative Skew) and Right-skewed (Positive Skew) distributions respectively A distribution is called bimodal if it has two “heaps”, as shown in Figure 1.4. Figure 1.4: Example of a Bimodal Distribution Summary Functions in R There are many ways to obtain all of the statistics described in the preceding sections, below we highlight 3: The describe function from the Hmisc package which can work on the entire dataset or a subset of columns. install.packages(&#39;Hmisc&#39;) library(Hmisc) Hmisc::describe(ames$Sale_Price) ## ames$Sale_Price ## n missing distinct Info Mean Gmd .05 .10 ## 2930 0 1032 1 180796 81960 87500 105450 ## .25 .50 .75 .90 .95 ## 129500 160000 213500 281242 335000 ## ## lowest : 12789 13100 34900 35000 35311, highest: 611657 615000 625000 745000 755000 The tidyverse summarise function, in this case obtaining statistics for each Exter_Qual separately. ames %&gt;% group_by(`Exter_Qual`) %&gt;% summarise(mean = mean(Sale_Price), sd = sd(Sale_Price), max = max(Sale_Price), min = min(Sale_Price)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 5 ## Exter_Qual mean sd max min ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Fair 89924. 38014. 200000 13100 ## 2 Typical 143374. 41504. 415000 12789 ## 3 Good 230756. 70411. 745000 52000 ## 4 Excellent 377919. 106988. 755000 160000 The base R summary function, which can work on the entire dataset or an individual variable summary(ames$Sale_Price) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12789 129500 160000 180796 213500 755000 1.1.3 The Normal Distribution The normal distribution, also known as the Gaussian distribution, is one of the most fundamental concepts in statistics. It is one that arises naturally out of many applications and settings. The normal distribution has the following characteristics: Symmetric Fully defined by mean and standard deviation (equivalently, variance) Bell-shaped/Unimodal Mean = Median = Mode Assymptotic to the x-axis (theoretical bounds are \\(-\\infty\\) to \\(\\infty\\)) Much of the normal distributions utility can be summarized in the empirical rule, which states that: \\(\\approx\\) 68% of data in normal distribution lies within 1 standard deviation of the mean. \\(\\approx\\) 95% of data in normal distribution lies within 2 standard deviations of the mean. \\(\\approx\\) 99.7% of data in normal distribution lies within 3 standard deviations of the mean. We can thus conclude that observations found outside of 3 standard deviations from the mean are quite rare, expected less than 1% of the time. 1.1.4 Skewness Skewness is a statistic that describes the symmetry (or lack thereof) of a distribution. A normal distribution is perfectly symmetric and has a skewness of 0. Distributions that are more right skewed will have positive values of skewness whereas distributions that are more left skewed will have negative values of skewness. 1.1.5 Kurtosis Kurtosis is a statistic that describes the tailedness of a distribution. The normal distribution has a kurtosis of 3. Distributions that are more tailed (leptokurtic or heavy-tailed) will have kurtosis values greater than 3 whereas distributions that are more less tailed (platykurtic or thin-tailed) will have values of kurtosis less than 3. For this reason, kurtosis is often reported in the form of excess kurtosis which is the raw kurtosis value minus 3. This is meant as a comparison to the normal distribution so that positive values indicate thicker tails and negative values indicate thinner tails than the normal. In Figure 1.5 below, we compare classical examples of leptokurtic and platykurtic distributions to a normal distribution with the same mean and variance. Figure 1.5: The Laplace distribution (top left) is leptokurtic because it has more data in its tails than the normal distribution with the same mean and variance. The uniform distribution (top right) is platykurtic because it has less data in its tails than the normal distribution with the same mean and variance (it effectively has no tails). 1.1.6 Graphical Displays of Distributions There are three types of plots for examining the distribution of your data values: Histograms Normal Probability Plots (QQ-plots) Box Plots Histograms A histogram shows the shape of a univariate distribution. Each bar in the histogram represents a group of values (a bin). The height of the bar represents the either the frequency of or the percent of values in the bin. The width and number of bins is determined automatically, but the user can adjust them to see more or less detail in the histogram. Figure 1.6 shows two histograms overlaid on the same axes. We can immediately see that there are many more houses that have central air than do not in this data. It appears as though the two distributions have different locations, with the pink distribution centered at a larger sale price. To normalize that quantity and compare the raw probability densitites, we can change our axes to density (which is analogous to percentage) as in Figure 1.7. ggplot(ames,aes(x=Sale_Price/1000)) + geom_histogram(data=subset(ames,Central_Air == &#39;Y&#39;),aes(fill=Central_Air), alpha = 0.2) + geom_histogram(data=subset(ames,Central_Air == &#39;N&#39;),aes(fill=Central_Air), alpha = 0.2) + labs(x = &quot;Sales Price (Thousands $)&quot;) + scale_fill_manual(name=&quot;Central_Air&quot;,values=c(&quot;red&quot;,&quot;blue&quot;),labels=c(&quot;No&quot;,&quot;Yes&quot;)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.6: Histogram: Frequency of Sale_Price for Each value of Central_Air ggplot(ames,aes(x=Sale_Price/1000)) + geom_histogram(data=subset(ames,Central_Air == &#39;Y&#39;),aes(y=..density..,fill=Central_Air), alpha = 0.2) + geom_histogram(data=subset(ames,Central_Air == &#39;N&#39;),aes(y=..density..,fill=Central_Air), alpha = 0.2) + labs(x = &quot;Sales Price (Thousands $)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.7: Histogram: Frequency of Sale_Price for varying qualities of home exterior To ease our differentiation of the histograms even further, we might employ a kernel density estimator with the geom_density plot as shown in Figure 1.8. This is an appealing alternative to the histogram for continuous data that is assumed to originate from some smooth underlying distribution. ggplot(ames,aes(x=Sale_Price/1000)) + geom_density(data=subset(ames,Central_Air == &#39;Y&#39;),aes(fill=Central_Air), alpha = 0.2) + geom_density(data=subset(ames,Central_Air == &#39;N&#39;),aes(fill=Central_Air), alpha = 0.2) + labs(x = &quot;Sales Price (Thousands $)&quot;) Figure 1.8: Histogram: Density of Sale_Price for varying qualities of home exterior Normal probability plots (QQ Plots) A normal probability plot graphs the sorted data values against the values that one would expect if the same number of observations came from a theoretical normal distribution. The resulting image would look close to a straight line if the data was generated by a normal distribution. Strong deviations from a straight line indicate that the data distribution is not normal. Figure 1.9 shows a QQ plot for Sale_Price, and we can conclude that the variable is not normally distributed. ggplot(data = ames, aes(sample = Sale_Price/1000)) + stat_qq() + stat_qq_line() Figure 1.9: QQ-Plot: Quantiles of Sale_Price vs. quantiles of a theoretical normal distribution with same mean and standard deviation. Conclusion: Sale_Price is not normally distributed. There are two main patterns that we expect to find when examining QQ-plots: A quadratic shape, as seen in Figure 1.9. This pattern indicates a deviation from normality due to skewness to the data. An S-shape (or cubic shape), as seen in Figure 1.10. This pattern indicates deviation from normality due to kurtosis. df &lt;- data.frame(j1 = rlaplace(10000,0,1)) ggplot(data = df, aes(sample=j1)) + stat_qq() + stat_qq_line() Figure 1.10: QQ-Plot: Quantiles of the Laplace distribution vs. quantiles of a theoretical normal distribution with same mean and standard deviation. Conclusion: Sale_Price is not normally distributed. Box Plots Box plots (sometimes called box-and-whisker plots) wont necessarily tell you about the shape of your distribution (for instance a bimodal distribution could have a similar box plot to a unimodal one), but it will give you a sense of the distribution’s location and spread. Many of us have become familiar with the idea of a box plot, but when pressed for the specific steps to create one, we realize our familiarity fades. The diagram in Figure 1.11 will remind the reader the precise information conveyed by a box plot. Figure 1.11: Anatomy of a Box Plot. Figure 1.12 shows the boxplot of Sale_Price. ggplot(data = ames, aes(y = Sale_Price/1000)) + geom_boxplot() + labs(y = &quot;Sales Price (Thousands $)&quot;) Figure 1.12: Box Plot of Sale_Price Furthermore, we might want to compare the boxplots of Sale_Price for different levels of a categorical variable, like Central_Air as we did with histograms and densities in Figures 1.6 and 1.7. The following code achieves this goal in Figure 1.13. ggplot(data = ames, aes(y = Sale_Price/1000, x = Central_Air, fill = Central_Air)) + geom_boxplot() + labs(y = &quot;Sales Price (Thousands $)&quot;, x = &quot;Central Air&quot;) + scale_fill_brewer(palette=&quot;Accent&quot;) + theme_classic() + coord_flip() Figure 1.13: Box Plots of Sale_Price for each level of Exter_Qual 1.2 Point Estimates All the statistics discussed so far have been point estimates. They are our best estimate at what the population parameter might be, but since we’ve taken a random sample of data from that population, there must be some uncertainty surrounding that estimate. In statistics, our real interest lies in drawing inferences about an entire population (which we couldn’t possibly observe due to time, cost, and/or feasibility constraints) and our approach is to take a representative sample and try to understand what it might tell us about the population. For the remainder of this text, we will assume our sample is representative of the population. Let’s review some common statistical notation of population parameters (the true values we couldn’t possibly observe) and sample statistics (those values we calculate based on our sample) Population Parameter Sample Statistic Mean (\\(\\mu\\)) Sample Average (\\(\\bar{x}\\)) Variance (\\(\\sigma^2\\)) Sample Variance (\\(s_x^2\\)) Standard Deviation (\\(\\sigma\\)) Sample Standard Deviation (\\(s_x\\)) Calculating point estimates should lead us to a natural question, one that embodies the field of statistics which aims to quantify uncertainty: What’s the margin of error for this estimate? This will be the subject of interest in the next two sections. 1.3 Confidence Intervals Let’s imagine that we want to calculate the average gas mileage of American cars on the road today in order to analyze the country’s carbon footprint. It should be clear to the reader that the calculation of the population mean would not be possible. The best we could do is take a large representative sample and calculate the sample average. Again, the next question should be: What is the margin of error for this estimate? If our sample average is 21.1 mpg, could the population mean reasonably be 21.2 mpg? how about 25 mpg? 42 mpg? To answer this question, we reach for the notion of confidence intervals. A confidence interval is an interval that we believe contains the population mean with some degree of confidence. A confidence interval is associated with a confidence level, a percentage, which indicates the strength of our confidence that the population mean is contained in the interval. It’s an important nuance to remember that the population mean is a fixed number. The source of randomness in our estimation is our sample. When we construct a 95% confidence interval, we are claiming that, upon repetition of the sampling and interval calculation process, we expect 95% of our created intervals to contain the population mean. To obtain a confidence interval for a mean in R, we can use the t.test() function, as shown below. t.test(ames$Sale_Price, conf.level = 0.95) ## ## One Sample t-test ## ## data: ames$Sale_Price ## t = 122.5, df = 2929, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 177902.3 183689.9 ## sample estimates: ## mean of x ## 180796.1 We can gather based on the output that our 95% confidence interval for the mean of Sale_Price is [177902.3, 183689.9]. This function also outputs some alternative information that relates to hypothesis testing which we will discuss in Section 1.4. For now, if we’d like to restrict the output, we can extract the confidence interval by adding $conf.int to our code: t.test(ames$Sale_Price, conf.level = 0.95)$conf.int ## [1] 177902.3 183689.9 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 To learn the labels of the various pieces of output, you can list them with the ls() function, or by saving the output as an object (below, results is the object that stores the output) and exploring it in your environment (upper right panel in RStudio): ls(t.test(ames$Sale_Price, conf.level = 0.95)) ## [1] &quot;alternative&quot; &quot;conf.int&quot; &quot;data.name&quot; &quot;estimate&quot; &quot;method&quot; ## [6] &quot;null.value&quot; &quot;p.value&quot; &quot;parameter&quot; &quot;statistic&quot; &quot;stderr&quot; results &lt;- t.test(ames$Sale_Price, conf.level = 0.95) 1.4 Hypothesis Testing A confidence interval can help us test a hypothesis about the population mean. A hypothesis is merely a statement that we wish to investigate scientifically through the process of statistical inference. In Section 1.3 we proposed some potential hypotheses in the form of questions: If the sample average gas mileage is 21.1, is it possible that the population mean is 21.2? How about 42? The statistical hypothesis test can help us answer these questions. To conduct a hypothesis test, we make an initial assumption. This initial assumption is called the null hypothesis and typically denoted as \\(H_0\\). We then analyze the data and determine whether out observations are likely, given our assumption of the null hypothesis. If we determine that our observed data was unlikely enough (beyond some threshold of confidence that we set before hand - or beyond a “reasonable doubt” in the justice system) then we reject our initial assumption in favor of the opposite statement, known as the alternative hypothesis denoted \\(H_a\\). The confidence threshold or confidence level that we use to determine how much evidence is required to reject the null hypothesis is a proportion, \\(\\alpha\\), which specifies how often we’re willing to incorrectly reject the null hypothesis. Remember, in applied statistics there are no proofs. Every decision we make comes with some degree of uncertainty. \\(\\alpha\\) quantifies our allowance for that uncertainty. In statistical textbooks of years past, \\(\\alpha = 0.05\\) was the norm. Later in this text we will propose much smaller values for \\(\\alpha\\) depending on your sample size. In order to quantify how unlikely it was that we observed a statistic as extreme or more extreme than we did, we calculate a p-value. The p-value is the area under the null distribution that represents the probability that we observed something as extreme or more extreme than we did (assuming the truth of the null hypothesis). If our p-value is less than our confidence level, \\(\\alpha\\), we have enough evidence to reject the null hypothesis in favor of the alternative. Let’s take an example and actually create a null distribution. Suppose we flip a fair coin, having equal probability of landing on heads or tails. We can actually simulate this experience with code! The following line of code does just that. Go ahead and run it a few times until you observe a coin flip of each type. sample(c(&#39;Heads&#39;,&#39;Tails&#39;), 1) ## [1] &quot;Heads&quot; Now, let’s suppose we do that many times and count the number of times we observe one outcome, say Heads. This can be done by sampling the values directly into a vector. Let n be the number of coin tosses. n=100 outcomes = sample(c(&#39;Heads&#39;,&#39;Tails&#39;), n, replace=T) We can count the number of Heads we obtained as follows: sum(outcomes==&quot;Heads&quot;) ## [1] 58 Every time you run the lines of code above you will find a different set of coin flips and a varying number of Heads; however the number of Heads will revolve around 50, because that is what we’d expect for a fair coin whose probability of Heads is 50% (Indeed, this simulates a draw from a binomial distribution with n=100 and p=0.5; the expected value of that distribution is \\(np=50\\) and the variance is \\(np(1-p)=25\\)). Thus, if were to do the above experiment thousands of times, we could map out a distribution of how many Heads one might reasonably receive by tossing a fair coin 100 times. Let’s do that, using a for loop. Let T be the number of simulated experiments (each experiment tosses the coin 100 times), and let number_heads be a vector that stores the number of heads for each experiment. We can initialize number_heads with an empty vector. Notice that our loop overwrites the coin toss data in each step, after recording the number of heads. T=10000 n=100 set.seed(11) number_heads = vector() for(i in 1:T){ outcomes = sample(c(&#39;Heads&#39;,&#39;Tails&#39;), n, replace=T) number_heads[i] = sum(outcomes==&quot;Heads&quot;) } df = data.frame(number_heads) ggplot(data = df) + geom_density(aes(x = number_heads)) + labs(x = &quot;Number of heads in 100 tosses&quot;) Figure 1.14: Null Distribution: Number of heads for fair coin tossed 100 times Figure 1.14 represents our null distribution of the number of heads from a fair coin tossed 100 times. What are the minimum and maximum values of this observed distribution? summary(df$number_heads) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 33.00 47.00 50.00 49.99 53.00 71.00 Example Suppose now we obtain a new coin from a friend and our hypothesis is that it “feels unfair”. We decide that we want to be 99% confident before we accuse our friend of cheating, so we conduct a hypothesis test. Our null hypothesis must generate a known distribution to which we can compare. Thus our null hypothesis is that the coin is fair: \\[H_0 = \\text{The coin is fair:}\\quad P(Heads) = 0.5\\] Our alternative hypothesis is the opposite of this: \\[H_0 = \\text{The coin is not fair:}\\quad P(Heads) \\neq 0.5\\] Suppose we flip the coin 100 times and count 65 heads. How likely is it that we would have obtained a result as extreme or more extreme than this if the coin was fair? Here we introduce the notion of a two-tailed hypothesis test. Since our hypothesis is that the coin is simply unfair, we want to know how likely it is that we obtained a result so different from 50. This is quantified by the absolute difference between what we observed and what we expected. Thus, when considering our null distribution, we want to look at the probability we’d obtain something greater than or equal to 65 (\\(=50+15\\)) heads, or less than or equal to 35 (\\(=50-15\\)) heads. Let’s take a look at this graphically through our simulated data: Figure 1.15: Null Distribution: Number of heads for fair coin tossed 100 times We can use this simulated distribution to estimate the p-value associated with obtaining 65 heads (the red area highlighted in Figure 1.15. We’d simply calculate the proportion of times we observed values equal to or more extreme than 65 - this is the very definition of a p-value. In the following line of code, | represents the logical “or” operator. sum(number_heads&gt;=65 | number_heads&lt;=35)/T ## [1] 0.0041 Conclusion: We said at the outset that we wanted to be 99% certain of our claim before we accused our friend of cheating. Based on our simulations, there is a 0.4% chance that we’d obtain the result we did, or something more extreme, if the coin was fair. Therefore, we have no choice but to reject our null hypothesis in favor of the alternative. Our friend has some explaining to do! Before we move on, we can compare the simulated result we just developed to one based on a theoretical distribution. This can be done using the prop.test() function to test a proportion. The formal test confirms our conclusion. prop.test(65, 100, p = 0.5, alternative = c(&quot;two.sided&quot;), conf.level = 0.99) ## ## 1-sample proportions test with continuity correction ## ## data: 65 out of 100 ## X-squared = 8.41, df = 1, p-value = 0.003732 ## alternative hypothesis: true p is not equal to 0.5 ## 99 percent confidence interval: ## 0.5162768 0.7643236 ## sample estimates: ## p ## 0.65 The nice thing about a simulation study like the one above is that it allows the user to explore how changes in the underlying procedure might affect the outcome. We’ll next consider two pieces that of the simulation study and how they affect the p-value: the sample size (the number of coin flips) and the effect size (the observed deviation from 50% heads). What happens if we increase/decrease the number of coin flips in our experiment, but keep the effect size the same, fixed at 65% heads? If we only flipped the coin 10 times, would 6-7 heads be improbable to witness from a fair coin? If we flipped the coin 1000 times, would 650 heads be more or less improbable than that same ratio in 10 tosses? In other words, which of these situations would entail a smaller p-value? We hope that the reader now has some intuition to answer this question. If not, we encourage them to answer it by altering the value of n in the simulation code, and seeing how the changes affect the distribution of the null hypothesis. What happens if we fix the sample size at 100 tosses and decrease the effect size from 65 heads to 60 heads? We’ve already generated the data to answer this question - our p-value would increase because it would be more probable to obtain a smaller effect size from a fair coin. On the flip side (pun intended) the p-value would decrease for a larger effect size. 1.4.1 One-Sample T-Test If we want to test whether the mean of continuous variable is equal to hypothesized value, we can use the t.test() function. The following code tests whether the average sale price of homes from Ames, Iowa over the data time period is $175,000. For now, we’ll use the classic \\(\\alpha=0.05\\) as our confidence level. If we have enough evidence to reject this null hypothesis, we will conclude that the mean sale price is either lower or higher for a two-tailed test (the default): t.test(ames$Sale_Price, mu = 178000) ## ## One Sample t-test ## ## data: ames$Sale_Price ## t = 1.8945, df = 2929, p-value = 0.05825 ## alternative hypothesis: true mean is not equal to 178000 ## 95 percent confidence interval: ## 177902.3 183689.9 ## sample estimates: ## mean of x ## 180796.1 Because our p-value is greater than our alpha level of 0.05, we fail to reject the null hypothesis. We do not quite have sufficient evidence to say the mean is different from 178,000. If we’re instead interested in testing whether the Sale_Price is higher than $178,000, we can specify this in the alternative= option. t.test(ames$Sale_Price, mu = 178000, alternative = &#39;greater&#39;) ## ## One Sample t-test ## ## data: ames$Sale_Price ## t = 1.8945, df = 2929, p-value = 0.02913 ## alternative hypothesis: true mean is greater than 178000 ## 95 percent confidence interval: ## 178367.7 Inf ## sample estimates: ## mean of x ## 180796.1 In this second test, we see that we actually do have enough evidence to claim that the true mean is greater than $178,000 at the \\(\\alpha=0.05\\) level. 1.5 Two-Sample t-tests If we have a hypothesis about a difference in the means of two groups of observations, a two-sample t-test can tell us whether that difference is statistically significant. By statistically significant, we mean the observed difference in sample means is greater than what we would expect to find if the population means were truly equal. In other words, statistical significance is a phrase that describes when our p-value falls below our confidence level, \\(\\alpha\\). Typically, the groups of interest are formed by levels of a binary variable, and the t-test is a way of testing whether there is a relationship between that binary variable and the continuous variable. To conduct a two-sample t-test, our data should satisfy 3 fundamental assumptions: The observations are independent The data from each group is normally distributed The variances for each group are equal If our data does not satisfy these assumptions, we must adapt our test to the situation. If the \\(3^{rd}\\) assumption of equal variances is not met, we simply add the option var.equal=F to the t.test() function to use the Welch or Satterthwaite approximation to degrees of freedom (it’s becoming increasingly common for practitioners to use this option even when variances are equal). If the \\(2^{nd}\\) assumption is not met, one must opt for a nonparametric test like the Mann-Whitney-U test (also called the Mann–Whitney–Wilcoxon or the Wilcoxon rank-sum test). The \\(1^{st}\\) assumption is not easily checked unless the data is generated over time (time-series) and is instead generally implied by careful data collect and the application of domain expertise. 1.5.1 Testing Normality of Groups The t-test is robust to deviations from normality, thus we can test the normality assumption either graphically or through formal statistical tests. The best graphical test of normality is a QQ-Plot, though histograms are often used as well. For formal tests of normality, we most often use the Shapiro-Wilk test, although many such formal tests exist, each with their own advantages. All of these tests have the null hypothesis of normality: \\[H_0: \\text{ the data is normally distributed}\\] \\[H_a: \\text{ the data is NOT normally distributed}\\] We saw in Figures 1.9 and 1.2 that the distribution of Sale_Price was not perfectly normal, however the deviations from normality were not egregious. We can also conduct a formal test as follows: shapiro.test(ames$Sale_Price) ## ## Shapiro-Wilk normality test ## ## data: ames$Sale_Price ## W = 0.87626, p-value &lt; 2.2e-16 The formal test rejects the null hypothesis of normality and confirms what we saw in our visual analysis. While this attribute is, indeed, NOT normally distributed, it is important to note that the t-test is robust to deviations from normality, provided the underlying distribution is unimodal, relatively symmetric and not severely skewed/kurtotic. For the purposes of illustration, we will proceed in this example under the assumption of normality, though a non-parametric approach will also be demonstrated to confirm our analysis in Section 1.5.4. 1.5.2 Testing Equality of Variances We can conduct a formal test to confirm that the \\(3^{rd}\\) assumption is met. This test for equal variances is known as an F-Test. The null hypothesis is that the variances are equal, the alternative being that they are not: \\[H_0: \\sigma_1^2 = \\sigma_2^2\\] \\[H_a: \\sigma_1^2 \\neq \\sigma_2^2\\] The F-Test is invoked with the var.test function, which takes as input a formula. A formula is an R object most often created using the ~ operator, for example y ~ x + z, interpreted as a specification that the response y is to be predicted with the inputs x and z. For our present discussion on two-sample t-tests, we might think of predicting our continuous attribute with our binary attribute, provided the means are different between the two groups. The following code checks whether the distributions of Sale_Price for houses with and without central air have the same variance. var.test(Sale_Price ~ Central_Air, data = ames) ## ## F test to compare two variances ## ## data: Sale_Price by Central_Air ## F = 0.2258, num df = 195, denom df = 2733, p-value &lt; 2.2e-16 ## alternative hypothesis: true ratio of variances is not equal to 1 ## 95 percent confidence interval: ## 0.1854873 0.2800271 ## sample estimates: ## ratio of variances ## 0.2257977 They do not. Thus, we must opt for the var.equal=FALSE option in the t.test() procedure. 1.5.3 Testing Equality of Means Assuming the first two assumptions are met, the two-sample t-test is performed by including a binary grouping variable with into the t.test() function. Below we test whether the mean sales price is different for houses with and without Central_Air, and we include the var.equal=FALSE option because we determined that the variances of the two groups were unequal in Section 1.5.2. The null hypothesis for the t-test is that the means of the two groups are equal: \\[H_0: \\mu_1 = \\mu_2\\] \\[H_a: \\mu_1 \\neq \\mu_2\\] t.test(Sale_Price ~ Central_Air, data = ames, var.equal = FALSE) ## ## Welch Two Sample t-test ## ## data: Sale_Price by Central_Air ## t = -27.433, df = 336.06, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -90625.69 -78498.92 ## sample estimates: ## mean in group N mean in group Y ## 101890.5 186452.8 Our final conclusion from the t-test is the rejection of the null hypothesis and the conclusion that houses with and without central air should be expected to sell for different prices. 1.5.4 Mann-Whitney-Wilcoxon Test As we pointed out in Section 1.5.1, the distribution of Sale_Price was not precisely normal. The most principled way to proceed in this case would be with a non-parametric test. The Mann-Whitney-Wilcoxon test is not identical to the t-test in its null and alternative hypotheses, but it aims to answer the same question about an association between the binary attribute and the continuous attribute. The null and alternative hypotheses for this test are as follows: \\[H_0: \\text{ the distributions of the two groups are equal } \\] \\[H_a: \\text{ the distributions of the two groups are NOT equal } \\] To perform this test, we use the wilcox.test() function, whose inputs are identical to the t.test() function. wilcox.test(Sale_Price ~ Central_Air, data = ames) ## ## Wilcoxon rank sum test with continuity correction ## ## data: Sale_Price by Central_Air ## W = 63164, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 Thus, we make the same conclusion with our non-parametric test. Houses with and without central air should be expected to sell for different prices. "],["slr.html", "Chapter 2 Introduction to ANOVA and Linear Regression 2.1 Prediction vs. Explanation 2.2 Honest Assessment 2.3 Bivariate EDA 2.4 One-Way ANOVA 2.5 ANOVA Post-hoc Testing 2.6 Pearson Correlation 2.7 Simple Linear Regression", " Chapter 2 Introduction to ANOVA and Linear Regression In this chapter, we introduce one of the most commonly used tools in data science: the linear model. We’ll start with some basic terminology. A linear model is an equation that typically takes the form \\[\\begin{equation} \\mathbf{y} = \\beta_0 + \\beta_1\\mathbf{x_1} + \\dots + \\beta_k\\mathbf{x_k} + \\boldsymbol \\epsilon \\tag{2.1} \\end{equation}\\] The left-hand side of this equation, \\(\\mathbf{y}\\) is equivalently called the target, response, or dependent variable. The right-hand side is a linear combination of the columns \\(\\{\\mathbf{x_i}\\}_{i=1}^{k}\\) which are commonly referred to as explanatory, input, predictor, or independent variables. 2.1 Prediction vs. Explanation The purpose of a linear model like Equation (2.1) is generally two-fold: The model is predictive in that it can estimate the value of \\(y\\) for a given combination of the \\(x\\) attributes. The model is explanatory in that it can estimate how \\(y\\) changes for a unit increase in a given \\(x\\) attribute, holding all else constant (via the slope parameters \\(\\beta\\)). However, it’s common for one of these purposes to be more aligned with the specific goals of your project, and it is common to approach the building of such a model differently for each purpose. In predictive modeling, you are most interested in how much error your model has on holdout data, that is, validation or test data. This is a notion that we introduce next in Section 2.2. If good predictions are all you want from your model, you are unlikely to care if the model satisfies all the assumptions necessary for hypothesis testing; and you are unlikely to care how many variables (including polynomial and interaction terms) are included in the final model. In explanatory modeling, you foremost want a model that is simple to interpret and doesn’t have too many input variables. It’s common to avoid many polynomial and interaction terms for explanatory models. It’s also imperative that the model fit the statistical assumptions required for inference (hypothesis testing). While the error rates on holdout data will still be useful reporting metrics for explanatory models, it will be more important to craft the model to fit assumptions and ease of interpretation. 2.2 Honest Assessment When performing predictive or explanatory modeling, we always divide our data into subsets for training, validation, and/or final testing. Because models are prone to discovering small, spurious patterns on the data that is used to create them (the training data), we set aside the validation and testing data to get a clear view of how they might perform on new data that they’ve never seen before. This is a concept that will be revisited several times throughout this text, highlighting its importance to honest assessment of models. There is no single right answer for how this division should occur for every data set - the answer depends on a multitude of factors that are beyond the scope of our present discussion. Generally speaking, one expects to keep about 70% of the data for model training purposes, and the remaining 30% for validation and testing. These proportions may change depending on the amount and of data available. If one has millions of observations, they can often get away with a much smaller proportion of training data to reduce computation time and increase confidence in validation. If one has substantially fewer observations, it may be necessary to increase the training proportion in order to build a sound model - trading validation confidence for proper training. Below we demonstrate one technique for separating the data into just two subsets: training and test. These two subsets will suffice for our analyses in this text. We’ll use 70% of our data for the training set and the remainder for testing. Since we are taking a random sample, each time you run this functions you will get a different result. This can be difficult for team members who wish to keep their analyses in sync. To avoid that variation of results, we can provide a “seed” to the internal random number generation process, which ensures that the randomly generated output is the same to all who use that seed. The following code demonstrates sampling via the tidyverse. This method requires the use of an id variable. If your data set has a unique identifier built in, you may omit the first line of code (after set.seed()) and use that unique identifier in the third line. library(tidyverse) set.seed(123) ames &lt;- ames %&gt;% mutate(id = row_number()) train &lt;- ames %&gt;% sample_frac(0.7) test &lt;- anti_join(ames, train, by = &#39;id&#39;) dim(train) ## [1] 2051 82 dim(test) ## [1] 879 82 2.3 Bivariate EDA As stated in Chapter 1, exploratory data analysis is the foundation of any successful data science project. As we move on to the discussion of modeling, we begin to explore bivariate relationships in our data. In doing so, we will often explore the input variables’ relationships with the target. Such exploration should only be done on the training data; we should never let insights from the validation or training data inform our decisions about modeling. Bivariate exploratory analysis is often used to assess relationships between two variables. An association or relationship exists when the expected value of one variable changes at different levels of the other variable. A linear relationship between two continuous variables can be inferred when the general shape of a scatter plot of the two variables is a straight line. 2.3.1 Continuous-Continuous Associations Let’s conduct a preliminary assessment of the relationship between the size of the house in square feet (via Gr_Liv_Area) and the Sale_Price by creating a scatter plot. Note that we call this a preliminary assessment because we should not declare a statistical relationship without a formal hypothesis test (see Section 2.6). ggplot(data = train) + geom_point(mapping = aes(x = Gr_Liv_Area, y = Sale_Price/1000)) + labs(y = &quot;Sales Price (Thousands $)&quot;, x = &quot;Greater Living Area (Sqft)&quot;) Figure 2.1: Scatter plot demonstrating a positive linear relationship 2.3.2 Continuous-Categorical Associations We’ll also revisit the plots that we created in Section 1.1, this time being careful to use only our training data since our goal is eventually to use a linear model to predict Sale_Price. We start by exploring the relationship between the external quality rating of the home (via the ordinal variable Exter_Qual and the Sale_Price. From this frequency histogram, we immediately see that much fewer of the homes have a rating of Excellent versus the other tiers, a fact that makes it difficult to compare the distributions. To normalize that quantity and compare the raw probability densities, we can change our axes to density (which is analogous to percentage) and employ a kernel density estimator with the geom_density plot as shown in Figure 2.3. We can then clearly see that as the exterior quality of the home “goes up” (in the ordinal sense, not in the linear sense), the sale price of the home also increases. ggplot(train,aes(x=Sale_Price/1000, fill=Exter_Qual)) + geom_histogram(alpha=0.2, position=&quot;identity&quot;) + labs(x = &quot;Sales Price (Thousands $)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 2.2: Histogram: Frequency of Sale_Price for varying qualities of home exterior ggplot(ames,aes(x=Sale_Price/1000, fill=Exter_Qual)) + geom_density(alpha=0.2, position=&quot;identity&quot;) + labs(x = &quot;Sales Price (Thousands $)&quot;) Figure 2.3: Histogram: Density of Sale_Price for varying qualities of home exterior To further explore the location and spread of the data, we can create box-plots for each group using the following code: ggplot(data = train, aes(y = Sale_Price/1000, x = `Exter_Qual`, fill = `Exter_Qual`)) + geom_boxplot() + labs(y = &quot;Sales Price (Thousands $)&quot;, x = &quot;Exterior Quality Category&quot;) + stat_summary(fun = mean, geom = &quot;point&quot;, shape = 20, size = 5, color = &quot;red&quot;, fill = &quot;red&quot;) + scale_fill_brewer(palette=&quot;Blues&quot;) + theme_classic() + coord_flip() Figure 2.4: Box Plots of Sale_Price for each level of Exter_Qual Notice that we’ve highlighted the mean on each box-plot for the purposes of comparison. We now have a hypothesis that we may want to formally test. After all, it is not good practice to look at Figures 2.3 and 2.4 and declare that a statistical difference exists. While we do, over time, get a feel for which visually apparent relationships turn out to be statistically significant, it’s imperative that we conduct formal testing before declaring such insights to a colleague or stakeholder. If we want to test whether the Sale_Price is different for the different values of Exter_Qual, we have to reach for the multi-group alternative to the 2-sample t-test. This is called Analysis of Variance, or ANOVA for short. 2.4 One-Way ANOVA One-way ANOVA aims to determine whether there is a difference in the mean of a continuous attribute across levels of a categorical attribute. Sound like a two-sample t-test? Indeed, it’s the extension of that test to more than two groups. Performing ANOVA with a binary input variable is mathematically identical to the two-sample t-test, as are it’s assumptions: The observations are independent The model residuals are normally distributed The variances for each group are equal While ANOVA typically refers to a single hypothesis test, it is truly a linear model with dummy variable inputs. Let \\(x_a\\), \\(x_b\\), and \\(x_c\\) be 3 reference-coded dummy variables for an attribute with 4 levels: a, b, c, and d. Note that we only have 3 dummy variables because one gets left out for the reference level, here d. ANOVA will create the following linear model: \\[\\begin{equation} y=\\beta_0 + \\beta_ax_a+\\beta_bx_b+\\beta_cx_c + \\epsilon \\tag{2.2} \\end{equation}\\] Now this model is especially easy to digest because \\(x_a\\), \\(x_b\\) and \\(x_c\\) are binary indicator variables that sum to 1 for each observation. That means if \\(x_a = 1\\) for a given observation then we know that \\(x_b\\) and \\(x_c\\) must be 0. The following facts can be easily derived for reference-level coding (they will change for effects-level coding) due to the fact that linear models are designed to predict the conditional expectation of y given x, \\(E[y|x]\\). Indeed, for this situation where there 4 levels of our categorical input, the ANOVA model will produce up to 4 distinct predictions. The prediction for each group is merely the group mean! \\(\\beta_0\\) represents the mean of reference group, group d. \\(\\beta_a, \\beta_b, \\beta_c\\) all represent the difference in the respective group means compared to the reference level. Positive values thus reflect a group mean that is higher than the reference group, and negative values reflect a group mean lower than the reference group. \\(\\epsilon\\) is called the residual. It is the error between the prediction and the actual response value. A one-way ANOVA model only contains a single input variable of interest. Equation (2.2), while it has 3 dummy variable inputs, only contains a single nominal attribute. In 3, we will add more inputs to the equation via two-way ANOVA and multivariate regression models. ANOVA is used to test the following hypothesis: \\[H_0: \\beta_a=\\beta_b=\\beta_c = 0 \\quad\\text{(i.e. all group means are equal)}\\] \\[H_0: \\beta_a\\neq0\\mbox{ or }\\beta_b\\neq0 \\mbox{ or } \\beta_c \\neq 0 \\quad\\text{(i.e. at least one is different)}\\] Both the lm() function and the aov() function will provide the p-values to test the hypothesis above, the only difference between the two functions is that lm() will also provide the user with the coefficient of determinination, \\(R^2\\), which tells you how much of the variation in \\(y\\) is accounted for by your categorical input. ames_lm &lt;- lm(Sale_Price ~ Exter_Qual, data = train) anova(ames_lm) ## Analysis of Variance Table ## ## Response: Sale_Price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Exter_Qual 3 6.6913e+12 2.2304e+12 701.83 &lt; 2.2e-16 *** ## Residuals 2047 6.5054e+12 3.1780e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(ames_lm) ## ## Call: ## lm(formula = Sale_Price ~ Exter_Qual, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -215904 -32910 -6147 24793 516090 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 207785 3176 65.416 &lt; 2e-16 *** ## Exter_Qual.L 215078 8353 25.749 &lt; 2e-16 *** ## Exter_Qual.Q 44553 6353 7.013 3.15e-12 *** ## Exter_Qual.C 6994 3308 2.114 0.0346 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56370 on 2047 degrees of freedom ## Multiple R-squared: 0.507, Adjusted R-squared: 0.5063 ## F-statistic: 701.8 on 3 and 2047 DF, p-value: &lt; 2.2e-16 # ames_aov &lt;- aov(Sale_Price ~ Exter_Qual, data = train) # Same thing with aov() function instead # summary(ames_aov) # R-squared not reported here. The p-value for the ANOVA hypothesis that all the groups have the same mean sale price is incredibly small, at \\(2.2\\times10^{-16}\\). This means it is extraordinarily improbable that we would have observed these differences in means, or a more extreme difference, if the population group means were equal. Thus, we reject our null hypothesis and conclude that there is an association between the exterior quality of a home and the price of the home. We note, based on the \\(R^2\\) statistics, that the exterior quality rating can account for almost half the variation in sales price! Adjusted \\(R^2\\) is a statistic that takes into account the number of variables in the model. The difference between \\(R^2\\) and adjusted \\(R^2\\) will be more thoroughly discussed in Chapter 3. We can also confirm what we know about the predictions from ANOVA, that there are only \\(k\\) unique predictions from an ANOVA with \\(k\\) groups (the predictions being the group means), using the predict function. train$pred_anova &lt;- predict(ames_lm, data = train) train$resid_anova &lt;- resid(ames_lm, data = train) (model_output = train %&gt;% select(Sale_Price, pred_anova, resid_anova)) ## # A tibble: 2,051 x 3 ## Sale_Price pred_anova resid_anova ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 232600 228910. 3690. ## 2 166000 228910. -62910. ## 3 170000 142107. 27893. ## 4 252000 228910. 23090. ## 5 134000 142107. -8107. ## 6 164700 228910. -64210. ## 7 193500 142107. 51393. ## 8 118500 142107. -23607. ## 9 94000 142107. -48107. ## 10 111250 142107. -30857. ## # … with 2,041 more rows 2.4.1 Testing Assumptions We can use the default plots from the lm() function to check the normality assumption. par(mfrow=c(2,2)) plot(ames_lm) Figure 2.5: Of the 4 default plots from lm(), we are presently interested in the top-right QQ plot that tests our assumption of normally distributed residuals. par(mfrow=c(1,1)) In the top-right plot in Figure 2.5 we verify again the approximate normality of sale price. To test for the third assumption of equal variances, we opt for a formal test like Levene’s (which depends on normality and can be found in the car package) or Fligner’s (which does not depend on normality and exists in the stats package). In both cases, the null hypothesis is equal variances: \\[H_0: \\sigma_a^2 =\\sigma_b^2 =\\sigma_c^2 \\quad \\text{i.e., the groups have equal variance}\\] \\[H_a: \\text{at least one group&#39;s variance is different}\\] #install.packages(&#39;car&#39;) #install.packages(&#39;stats&#39;) library(car) library(stats) leveneTest(Sale_Price ~ Exter_Qual, data = train) # Most popular, but depends on Normality ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 76.879 &lt; 2.2e-16 *** ## 2047 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 fligner.test(Sale_Price ~ Exter_Qual, data = train) # DOES NOT depend on Normality ## ## Fligner-Killeen test of homogeneity of variances ## ## data: Sale_Price by Exter_Qual ## Fligner-Killeen:med chi-squared = 206.26, df = 3, p-value &lt; 2.2e-16 And in both cases, we’re forced to reject the null hypothesis of equal variances. A non-parametric version of the ANOVA test, the Kruskal-Wallis test, is more suitable to this particular case. Non-parametric tests do not have the same statistical power to detect differences between groups. Statistical power is the probability of detecting an effect, if there is a true effect present to detect. We should opt for these tests in situations where our data is ordinal or otherwise violates the assumptions of normality or equal variances in ways that cannot be fixed by logarithmic or other similar transformation. 2.4.2 Kruskal-Wallis The Kruskal-Wallis test, proposed in 1952, is equivalent to a parametric one-way ANOVA where the data values have been replaced with their ranks (i.e. largest value = 1, second largest value = 2, etc.). When the data is not normally distributed but is identically distributed (having the same shape and variance), the Kruskal-Wallis test can be considered a test for differences in medians. If those identical distributions are also symmetric, then Kruskal-Wallis can be interpretted as testing for a difference in medians. When the data is not identically distributed, or when the distributions are not symmetric, Kruskal-Wallis is a test of dominance between distributions. Distributional dominance is the notion that one group’s distribution is located at larger values than another, probabilistically speaking. Formally, a random variable A has dominance over random variable B if \\(P(A\\geq ) \\geq P(B\\geq x)\\) for all \\(x\\), and for some \\(x\\), \\(P(A&gt; ) \\geq P(B&gt; x)\\). We summarize this information in the following table: Conditions Interpretation of Significant Kruskal-Wallis Test Group distributions are identical in shape, variance, and symmetric Difference in means Group distributions are identical in shape, variance, but not symmetric Difference in medians Group distributions are identical in shape, variance, but not symmetric Difference in location. (distributional dominance) 2.5 ANOVA Post-hoc Testing After performing an ANOVA and learning that there is a difference between the groups of data, our next natural question ought to be which groups of data are different, and how? In order to explore this question, we must first consider the notion of experimentwise error. When conducting multiple hypothesis tests simultaneously, the experimentwise error rate is the proportion of time you expect to make an error in at least one test. Let’s suppose we are comparing grocery spending on 4 different credit card rewards programs. If we’d like to compare the rewards programs pairwise, that entails 6 different hypothesis tests (each is a two-sample t-test). If we keep a confidence level of \\(\\alpha = 0.05\\) and subsequently view “being wrong in one test” as a random event happening with probability \\(p=0.05\\) then our probability of being wrong in at least one test out of 6 could be as great as 0.26! To control this experiment-wise error rate, we must lower our confidence thresholds to account for it. Alternatively, we can view this as an adjustment of our p-values higher while keeping our confidence threshold fixed as usual. This is typically the approach taken, as we prefer to fix our confidence thresholds in accordance with previous literature or industry standards. There are many methods of adjustment that have been proposed over the years for this purpose. Here, we consider two popular methods: Tukey’s test for pairwise comparisons and Dunnett’s test for control group comparisons. If the reader finds themselves in a situation that doesn’t fit the presciption of either of these methods, we suggest looking next at the modified Bonferroni correction or the notion of false discovery rates proposed by Benjamini and Hochberg in 1995. 2.5.1 Tukey-Kramer If our objective is to compare each group to every other group then Tukey’s test of honest significant differences, also known as the Tukey-Kramer test is probably the most widely-available and popular corrections in practice. However, it should be noted that Tukey’s test should not be used if one does not plan to make all pairwise comparisons. If only a subset of comparisons are of interest to the user (like comparisons only to a control group) then one should opt for the Dunnett or a modified Bonferroni correction. To employ Tukey’s HSD in R, we must use the aov() function to create our ANOVA object rather than the lm() function. The output of the test shows the difference in means and the p-value for testing the null hypothesis that the means are equal (i.e. that the differences are equal to 0). ames_aov &lt;- aov(Sale_Price ~ Exter_Qual, data = train) tukey.ames &lt;- TukeyHSD(ames_aov) print(tukey.ames) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sale_Price ~ Exter_Qual, data = train) ## ## $Exter_Qual ## diff lwr upr p adj ## Typical-Fair 57887.91 30194.31 85581.52 5e-07 ## Good-Fair 144690.25 116739.87 172640.63 0e+00 ## Excellent-Fair 291684.79 259752.41 323617.16 0e+00 ## Good-Typical 86802.34 79910.03 93694.64 0e+00 ## Excellent-Typical 233796.87 216886.62 250707.12 0e+00 ## Excellent-Good 146994.54 129666.98 164322.10 0e+00 plot(tukey.ames, las = 1) Figure 2.6: Confidence intervals for mean differences adjusted via Tukey-Kramer The p-values in this table have been adjusted higher to account for the possible experimentwise error rate. For every pairwise comparison shown, we reject the null hypothesis and conclude that the mean sales price of the homes is different for each level of Exter_Qual. Furthermore, Figure 2.6 shows us experiment-wise (family-wise) adjusted confidence intervals for the differences in means for each pair. 2.5.2 Dunnett’s Test If the plan is to make fewer comparisons, specifically just \\(k-1\\) comparisons where \\(k\\) is the number of groups in your data (indicating you plan to compare all the groups to one specific group, usually the control group), then Dunnett’s test would be preferrable to the Tukey-Kramer test. If all pairwise comparisons are not made, the Tukey-Kramer test is overly conservative, creating a confidence level that is much lower than specified by the user. Dunnett’s test factors in fewer comparisons and thus should not be used for tests of all pairwise comparisons. To use Dunnett’s test, we must install the DescTools package. The control group to which all other groups will be compared is designated by the control= option. #install.packages(&#39;DescTools&#39;) library(DescTools) DunnettTest(x = train$Sale_Price, g = train$Exter_Qual, control = &#39;Typical&#39;) ## ## Dunnett&#39;s test for comparing several treatments with a control : ## 95% family-wise confidence level ## ## $Typical ## diff lwr.ci upr.ci pval ## Fair-Typical -57887.91 -83628.55 -32147.28 2.6e-07 *** ## Good-Typical 86802.34 80396.08 93208.59 &lt; 2e-16 *** ## Excellent-Typical 233796.87 218079.15 249514.60 &lt; 2e-16 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the output from Dunnett’s test, we notice the p-value comparing Fair to Typical exterior qualities is lower than it was in the Tukey-Kramer test. This is consistent with our expectations for a test that isn’t controlling for as many comparisons; it makes a smaller upward adjustment of p-values to satisfy a given experiment-wise error rate. 2.6 Pearson Correlation ANOVA is used to formally test the relationship between a categorical variable and a continuous variable. To formally test the (linear) relationship between two continuous attributes, we introduce Pearson correlation, commonly referred to as simply correlation. Correlation is a number between -1 and 1 which measures the strength of a linear relationship between two continuous attributes. Negative values of correlation indicate a negative linear relationship, meaning that as one of the variables increases, the other tends to decrease. Similarly, positive values of correlation indicate a positive linear relationship meaning that as one of the variables increases, the other tends to increase. Absolute values of correlation equal to 1 indicate a perfect linear relationship. For example, if our data had a column for “mile time in minutes” and a column for “mile time in seconds”, these two columns would have a correlation of 1 due to the fact that there are 60 seconds in a minute. A correlation value near 0 indicates that the variables have no linear relationship. It’s important to emphasize that Pearson correlation is only designed to detect linear associations between variables. Even when a correlation between two variables is 0, the two variables may still have a very clear association, whether it be quadratic, cyclical, or some other nonlinear pattern of association. Figure 2.7 illustrates all of these statements. On top of each scatter plot, the correlation coefficient is shown. The middle row of this figure aims to illustrate that a perfect correlation has nothing to do with the magnitude or slope of the relationship. In the center image, middle row, we note that the correlation is undefined for any pair that includes a constant variable. In that image, the value of \\(y\\) is constant across the sample. Equation (2.3) makes this mathematically clear. Figure 2.7: Examples of relationships and their associated correlations The population correlation parameter is denoted \\(\\rho\\) and estimated by the sample correlation, denoted as \\(r\\). The formula for the sample correlation between columns of data \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is \\[\\begin{equation} r = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{x})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2\\sum_{i=1}^n(y_i-\\bar{x})^2}}. \\tag{2.3} \\end{equation}\\] Note that with centered variable vectors \\(\\mathbf{x_c}\\) and \\(\\mathbf{y_c}\\) this formula becomes much cleaner with linear algebra notation: \\[\\begin{equation} r = \\frac{\\mathbf{x_c}^T\\mathbf{y_c}}{\\|\\mathbf{x_c}\\|\\|\\mathbf{y_c}\\|}. \\tag{2.4} \\end{equation}\\] It is interesting to note that Equation (2.4) is identical to the formula for the cosine of the angle between to vectors. While this geometrical relationship does not benefit our intuition1, it is noteworthy nonetheless. Pearson’s correlation can be calculated in R with the built in cor() function, with the two continuous variables as input: cor(train$Gr_Liv_Area,train$Sale_Price) ## [1] 0.698509 2.6.1 Statistical Test To test the statistical significance of correlation, we use a t-test with the null hypothesis that the correlation is equal to 0: \\[H_0: \\rho = 0\\] \\[H_a: \\rho \\neq 0\\] If we can reject the null hypothesis, then we declare a significant linear association between the two variables. The cor.test() function in R will perform the test: cor.test(train$Gr_Liv_Area,train$Sale_Price) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 44.185, df = 2049, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.6756538 0.7200229 ## sample estimates: ## cor ## 0.698509 We conclude that Gr_Liv_Area has a linear association with Sale_Price. It must be noted that this t-test for Pearson’s correlation is not free from assumptions. In fact, there are 4 assumptions that must be met, and they are detailed in Section @ref(#slrassumptions). 2.6.2 Anomalous Observations One final nuance that is important to note is the effect of anomalous observations on correlation. In Figure 2.8 we display 30 random 2-dimensional data points \\((x,y)\\) with no linear relationship. set.seed(11) x &lt;- rnorm(30) y &lt;- rnorm(30) plot(x,y) Figure 2.8: The variables x and y have no correlation The correlation is not exactly zero (we wouldn’t expect perfection from random data) but it is very close at 0.002. cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 0.012045, df = 28, p-value = 0.9905 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3582868 0.3622484 ## sample estimates: ## cor ## 0.002276214 Next, we’ll add a single anomalous observation to our data and see how it affects both the correlation value and the correlation test. x[31] = 4 y[31] = 50 cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = 5.803, df = 29, p-value = 2.738e-06 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5115236 0.8631548 ## sample estimates: ## cor ## 0.7330043 The correlation jumps to 0.73 from 0.002 and is declared strongly significant! Figure 2.9 illustrates the new data. This simple example shows why exploratory data analysis is so important! If we don’t explore our data and detect anomalous observations, we might improperly declare relationships are significant when they are driven by a single observation or a small handful of observations. plot(x,y) Figure 2.9: A single anomalous observation creates strong correlation (0.73) where there previously was none 2.7 Simple Linear Regression After learning that two variables share a linear relationship, the next question is natural: what is that relationship? How should we expect one variable to change as the other increases? Simple linear regression answers this question by creating a linear equation that best represents the relationship in the sense that it minimizes the squared error between the observed data and the model predictions (i.e. the sum of the squared residuals). The simple linear regression equation is typically written \\[\\begin{equation} y=\\beta_0 + \\beta_1x + \\epsilon \\tag{2.5} \\end{equation}\\] where \\(\\beta_0\\), the intercept, gives the expected value of \\(y\\) when \\(x=0\\) and \\(\\beta_1\\), the slope gives the expected change in \\(y\\) for a one-unit increase in \\(x\\). The residual, \\(\\epsilon\\) is the error between our predicted value, \\(\\hat{y}=\\beta_0 + \\beta_1x\\) and our actual value \\(y\\). As discussed in Section 2.1, a simple linear regression serves two purposes: to predict the expected value of \\(y\\) for each value of \\(x\\) and to explain how \\(y\\) is expected to change for a unit change in \\(x\\). 2.7.1 Assumptions of Linear Regression Linear regression, in particular the hypothesis tests that are generally performed as part of linear regression, has 4 assumptions: The expected value of \\(y\\) is linear in \\(x\\) (proper model specification). The random errors are independent. The random errors are normally distributed. The random errors have equal variance (heteroskedasticity). It must now be noted that these assumptions are also in effect for the test of Pearson’s correlation in Section 2.6.1, because the tests in simple linear regression are mathematically equivalent to that test. When these assumptions are not met, another approach to testing the significance of a linear relationship should be considered. The most common non-parametric approach to testing for an association between two continuous variables is Spearman’s correlation. Spearman’s correlation does not limit its findings to linear relationships; any monotonic relationship (one that is always increasing or always decreasing) will cause Spearman’s correlation to be significant. Similar to the approach taken by Kruskal-Wallis, Spearman’s correlation replaces the data with its ranks and computes Pearson’s correlation on the ranks. The same cor and cor.test() functions can be used; simply specify the method='spearman' option. cor.test(train$Gr_Liv_Area,train$Sale_Price, method = &#39;spearman&#39;) ## Warning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 408364087, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7160107 2.7.2 Testing for Association The statistical test of correlation is mathematically equivalent to testing the hypothesis that the slope parameter in Equation (2.5) is zero. This t-test is part of the output from any linear regression function, like lm() which we saw in Section 2.4. Let’s confirm this using the example from the Section 2.6.1 where we investigate the relationship between Gr_Liv_Area and Sale_Price. Again, the t-test in the output tests the following hypothesis: \\[H_0: \\beta_1=0\\] \\[H_a: \\beta_1 \\neq 0\\] The first thing we will do after creating the linear model is check our assumption using the default plots from lm() . From these four plots we will be most interested in the first two. In the top left plot, we are visually checking for heteroskedasticity. We’d like to see the variability of the points remain constant from left to right on this chart, indicating that the errors have constant variance for each value of y. We do not want to see any fan shapes in this chart. Unfortunately, we do see just that: the variability of the errors is much smaller for smaller values of Sale Price than it is for larger values of Sale Price. In the top right plot, we are visually checking for normality of errors. We’d like to see the QQ-plot indicate normality with all the points roughly following the line. Unfortunately, we do not see that here. The errors do not appear to be normally distributed. slr &lt;- lm(Sale_Price ~ Gr_Liv_Area, data=train) par(mfrow=c(2,2)) plot(slr) Figure 2.10: The variables x and y have no correlation par(mfrow=c(1,1)) Despite the violation of assumptions, let’s continue examining the output from this regression in order to practice our interpretation of it. summary(slr) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -478762 -30030 -1405 22273 335855 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14045.872 3942.503 3.563 0.000375 *** ## Gr_Liv_Area 110.726 2.506 44.185 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 57430 on 2049 degrees of freedom ## Multiple R-squared: 0.4879, Adjusted R-squared: 0.4877 ## F-statistic: 1952 on 1 and 2049 DF, p-value: &lt; 2.2e-16 The first thing we’re likely to examine in the coefficient table is the p-value for Gr_Liv_Area. It is strongly significant (in fact, it’s the same t-value and p-value as we saw for the cor.test as these tests are mathematically equivalent), indicating that there is an association between the size of the home and the sale price. Furthermore, the parameter estimate is 115.5 indicating that we’d expect the price of a home to increase by $115.5 for every additional square foot of living space. Because of the linearity of the model, we can extend this slope estimate to any unit change in \\(x\\). For example, it might be difficult to think in terms of single square feet when comparing houses, so we might prefer to use a 100 square-foot change and report our conclusion as follows: For each additional 100 square feet of living area, we expect the house price to increase by $11,550. The n-dimensional vectors and live in the vast sample space where a single point/vector is one possible sample of n objects; this space can be difficult to grasp mentally↩︎ "],["mlr.html", "Chapter 3 Complex ANOVA and Multiple Linear Regression 3.1 Two-Way ANOVA 3.2 Two-Way ANOVA with Interactions 3.3 Randomized Block Design 3.4 Multiple Linear Regression", " Chapter 3 Complex ANOVA and Multiple Linear Regression In in Chapter 2 we were introduced to the One-Way ANOVA and simple linear regression models. These models only contained a single variable - categorical for ANOVA and continuous for simple linear regression - to explain and predict our target variable. Rarely do we believe that only a single variable will suffice in predicting a variable of interest. Here in this Chapter we will generalize these models to the \\(n\\)-Way ANOVA and multiple linear regression models. These models contain multiple sets of variables to explain and predict our target variable. This Chapter aims to answer the following questions: How do we include multiple variables in ANOVA? Exploration Assumptions Predictions What is an interaction between two predictor variables? Interpretation Evaluation Within Category Effects What is blocking in ANOVA? Nuisance Factors Differences Between Blocking and Two-Way ANOVA How do we include multiple variables in regression? Model Structure Global &amp; Local Inference Assumptions Adjusted \\(R^2\\) Categorical Variables in Regression 3.1 Two-Way ANOVA Section 2 details the One-Way ANOVA model using a single categorical predictor variable with \\(k\\) levels to predict our continuous target variable. Now we will generalize this model to include \\(n\\) categorical variables that each have different numbers of levels (\\(k_1, k_2, ..., k_n\\)). 3.1.1 Exploration Let’s use the basic example of two categorical predictor variables in a Two-Way ANOVA. Previously, we talked about using heating quality as a factor to explain and predict sale price of homes in Ames, Iowa. Now, we also consider whether the home has central air. Although similar in nature, these two factors potentially provide important, unique pieces of information about the home. Similar to previous data science problems, let us first explore our variables and their potential relationships. Now that we have two variables that we will use to explain and predict sale price, here are some summary statistics (mean, standard deviation, minimum, and maximum) for each combination of category. We will use the group_by function on both predictor variables of interest to split the data and then the summarise function to calculate the metrics we are interested in. train %&gt;% group_by(Heating_QC, Central_Air) %&gt;% summarise(mean = mean(Sale_Price), sd = sd(Sale_Price), max = max(Sale_Price), min = min(Sale_Price)) ## `summarise()` regrouping output by &#39;Heating_QC&#39; (override with `.groups` argument) ## # A tibble: 10 x 6 ## # Groups: Heating_QC [5] ## Heating_QC Central_Air mean sd max min ## &lt;ord&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Poor N 50050 52255. 87000 13100 ## 2 Poor Y 107000 NA 107000 107000 ## 3 Fair N 84748. 28267. 158000 37900 ## 4 Fair Y 145165. 38624. 230000 50000 ## 5 Typical N 103469. 34663. 209500 12789 ## 6 Typical Y 142003. 39657. 375000 60000 ## 7 Good N 110811. 38455. 214500 59000 ## 8 Good Y 160113. 54158. 415000 52000 ## 9 Excellent N 115062. 33271. 184900 64000 ## 10 Excellent Y 216401. 88518. 745000 58500 We can already see above that there appears to be some differences in average sale price across the categories overall. Within each grouping of heating quality, homes with central air appear to have a higher sale price than homes without. Also, similar to before, homes with higher heating quality appear to have higher sale prices compared to homes with lower heating quality. We also see these relationships in the bar chart in Figure 3.1. CA_heat &lt;- train %&gt;% group_by(Heating_QC, Central_Air) %&gt;% summarise(mean = mean(Sale_Price), sd = sd(Sale_Price), max = max(Sale_Price), min = min(Sale_Price)) ## `summarise()` regrouping output by &#39;Heating_QC&#39; (override with `.groups` argument) ggplot(data = CA_heat, aes(x = Heating_QC, y = mean/1000, fill = Central_Air)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge()) + labs(y = &quot;Sales Price (Thousands $)&quot;, x = &quot;Heating Quality Category&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + theme_minimal() Figure 3.1: Distribution of Variables Heating_QC and Central_Air As before, visually looking at bar charts and mean calculations only goes so far. We need to statistically be sure of any differences that exist between average sale price in categories. 3.1.2 Model We are going to do this with the same approach as in the One-Way ANOVA of Chapter 2, just with more variables as shown in the following equation: \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\varepsilon_{ijk} \\] where \\(\\mu\\) is the average baseline sales price of a home in Ames, Iowa, \\(\\alpha_i\\) is the variable representing the impacts of the levels of heating quality, and \\(\\beta_j\\) is the variable representing the impacts of the levels of central air. As mentioned previously, the unexplained error in this model is represented as \\(\\varepsilon_{ijk}\\). The same F test approach is also used, just for each one of the variables. Each variable’s test has a null hypothesis assuming all categories have the same mean. The alternative for each test is that at least one category’s mean is different. Let’s view the results of the aov function. ames_aov2 &lt;- aov(Sale_Price ~ Heating_QC + Central_Air, data = train) summary(ames_aov2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Heating_QC 4 2.891e+12 7.228e+11 147.60 &lt; 2e-16 *** ## Central_Air 1 2.903e+11 2.903e+11 59.28 2.11e-14 *** ## Residuals 2045 1.002e+13 4.897e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the above results, we have low p-values for each of the variables’ F test. Heating quality had 4 degrees of freedom, derived from the 5 categories \\((4 = 5-1)\\). Similarly, central air’s 2 categories produce 1 \\((= 2-1)\\) degree of freedom. The F values are calculated the exact same way as described before with the mean square for each variable divided by the mean square error. Based on these tests, at least one category in each variable is statistically different than the rest. 3.1.3 Post-Hoc Testing As with the One-Way ANOVA, the next logical question is which of these categories is different. We will use the same post-hoc tests as before with the TukeyHSD function. tukey.ames2 &lt;- TukeyHSD(ames_aov2) print(tukey.ames2) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sale_Price ~ Heating_QC + Central_Air, data = train) ## ## $Heating_QC ## diff lwr upr p adj ## Fair-Poor 49176.42 -63650.448 162003.29 0.7571980 ## Typical-Poor 67781.01 -42800.320 178362.35 0.4506761 ## Good-Poor 87753.89 -23040.253 198548.03 0.1945181 ## Excellent-Poor 146288.89 35818.859 256758.92 0.0028361 ## Typical-Fair 18604.59 -6326.425 43535.61 0.2484556 ## Good-Fair 38577.47 12718.894 64436.04 0.0004622 ## Excellent-Fair 97112.47 72679.867 121545.07 0.0000000 ## Good-Typical 19972.87 7050.230 32895.52 0.0002470 ## Excellent-Typical 78507.88 68746.678 88269.07 0.0000000 ## Excellent-Good 58535.00 46602.229 70467.78 0.0000000 ## ## $Central_Air ## diff lwr upr p adj ## Y-N 43256.57 31508.27 55004.87 0 plot(tukey.ames2, las = 1) Starting with the variable for central air, we can see there is a statistical difference between the two categories. This is the exact same result as the overall F test for the variable since there are only two categories. For the heating quality variable, we can see some categories are different from each other, while others are not. Noticeably, the combination of poor with fair, good, and typical categories are not statistically different. Notice also the different widths of these confidence intervals do to the different combinations of sample sizes for the categories being tested. 3.2 Two-Way ANOVA with Interactions What if the relationship between a predictor and target variable changed depending on the value of another predictor variable? For our example, we would say that the average difference in sales price between having central air and not having central changed depending on what level of heating quality the home had. In the bar chart in Figure 3.1, an interaction effect is displayed when the differences between the two bars within each heating category is different across heating category. If the difference, was the same, then there is no interaction present. In other words, no matter the heating quality rating of the home, the average difference in sales price between having central air and not having central air is the same. This interaction model is represented as follows: \\[ Y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha \\beta)_{ij} + \\varepsilon_{ijk} \\] with the interaction effect, \\((\\alpha \\beta)_{ij}\\), as the multiplication of the two variables involved in the interaction. Interactions can occur between more than two variables as well. Interactions are good to evaluate as they can mask the effects of individual variables. For example, imagine a hypothetical example as shown in Figure 3.2 where the impact of having central air is opposite depending on which category of heating quality you have. fake_HQ &lt;- c(&quot;Poor&quot;, &quot;Poor&quot;, &quot;Excellent&quot;, &quot;Excellent&quot;) fake_CA &lt;- c(&quot;N&quot;, &quot;Y&quot;, &quot;N&quot;, &quot;Y&quot;) fake_mean &lt;- c(100, 150, 150, 100) fake_df &lt;- as.data.frame(cbind(fake_HQ, fake_CA, fake_mean)) ggplot(data = fake_df, aes(x = fake_HQ, y = as.numeric(fake_mean), fill = fake_CA)) + geom_bar(stat = &quot;identity&quot;, position = position_dodge()) + labs(y = &quot;Fake Sales Price (Thousands $)&quot;, x = &quot;Fake Heating Quality Category&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + theme_minimal() Figure 3.2: Distribution of Variables Heating_QC and Central_Air If you were to only look at the average sales price across heating quality, you would notice no difference between the two groups (average for both heating categories is 125). However, when the interaction is accounted for, you can clearly see in the bar heights that sales price is different depending on the value of central air. Let’s evaluate the interaction term in our actual data. To do so, we just incorporate the multiplication of the two variables in the model statement by using the formula Sale_Price ~ Heating_QC + Central_Air + Heating_QC:Central_Air. You could also use the shorthand version of this by using the formula Sale_Price ~ Heating_QC*Central_Air. The * will include both main effects (the individual variables) and the interaction between them. ames_aov_int &lt;- aov(Sale_Price ~ Heating_QC*Central_Air, data = train) summary(ames_aov_int) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Heating_QC 4 2.891e+12 7.228e+11 147.897 &lt; 2e-16 *** ## Central_Air 1 2.903e+11 2.903e+11 59.403 1.99e-14 *** ## Heating_QC:Central_Air 4 3.972e+10 9.930e+09 2.032 0.0875 . ## Residuals 2041 9.975e+12 4.887e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As seen by the output above, the interaction effect between heating quality and central air is significant at the 0.05 level. Again, this implies that the average difference in sale price of the home between having central air and not differs depending on which category of heating quality the home belongs to. If our interaction was not significant (say a 0.02 significance level instead) then we would remove the interaction term from our model and rerun the analysis. 3.2.1 Post-Hoc Testing Post-hoc tests are also available for interaction models as well to determine where the statistical differences exist in all the combinations of possible categories. We evaluate these post-hoc tests using the same TukeyHSD function and its corresponding plot element. tukey.ames_int &lt;- TukeyHSD(ames_aov_int) print(tukey.ames_int) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sale_Price ~ Heating_QC * Central_Air, data = train) ## ## $Heating_QC ## diff lwr upr p adj ## Fair-Poor 49176.42 -63536.973 161889.81 0.7565086 ## Typical-Poor 67781.01 -42689.104 178251.13 0.4496163 ## Good-Poor 87753.89 -22928.822 198436.60 0.1936558 ## Excellent-Poor 146288.89 35929.963 256647.82 0.0027979 ## Typical-Fair 18604.59 -6301.351 43510.54 0.2474998 ## Good-Fair 38577.47 12744.901 64410.03 0.0004543 ## Excellent-Fair 97112.47 72704.440 121520.50 0.0000000 ## Good-Typical 19972.87 7063.227 32882.52 0.0002425 ## Excellent-Typical 78507.88 68756.495 88259.26 0.0000000 ## Excellent-Good 58535.00 46614.230 70455.77 0.0000000 ## ## $Central_Air ## diff lwr upr p adj ## Y-N 43256.57 31520.09 54993.05 0 ## ## $`Heating_QC:Central_Air` ## diff lwr upr p adj ## Fair:N-Poor:N 34698.276 -127178.615 196575.17 0.9996249 ## Typical:N-Poor:N 53419.220 -105046.645 211885.08 0.9876643 ## Good:N-Poor:N 60760.870 -102472.555 223994.29 0.9755371 ## Excellent:N-Poor:N 65011.727 -105195.635 235219.09 0.9709555 ## Poor:Y-Poor:N 56950.000 -214233.733 328133.73 0.9996829 ## Fair:Y-Poor:N 95114.833 -65743.496 255973.16 0.6876708 ## Typical:Y-Poor:N 91952.772 -64912.040 248817.58 0.6985070 ## Good:Y-Poor:N 110062.553 -46997.028 267122.13 0.4435353 ## Excellent:Y-Poor:N 166351.347 9630.224 323072.47 0.0271785 ## Typical:N-Fair:N 18720.944 -29117.117 66559.00 0.9659507 ## Good:N-Fair:N 26062.594 -35761.358 87886.55 0.9454988 ## Excellent:N-Fair:N 30313.451 -48093.155 108720.06 0.9685428 ## Poor:Y-Fair:N 22251.724 -202954.108 247457.56 0.9999995 ## Fair:Y-Fair:N 60416.557 5167.556 115665.56 0.0193847 ## Typical:Y-Fair:N 57254.496 15021.578 99487.41 0.0007697 ## Good:Y-Fair:N 75364.278 32413.584 118314.97 0.0000014 ## Excellent:Y-Fair:N 131653.071 89957.021 173349.12 0.0000000 ## Good:N-Typical:N 7341.650 -44902.998 59586.30 0.9999894 ## Excellent:N-Typical:N 11592.508 -59505.300 82690.32 0.9999620 ## Poor:Y-Typical:N 3530.780 -219235.844 226297.41 1.0000000 ## Fair:Y-Typical:N 41695.614 -2573.500 85964.73 0.0848888 ## Typical:Y-Typical:N 38533.553 12248.163 64818.94 0.0001576 ## Good:Y-Typical:N 56643.334 29219.541 84067.13 0.0000000 ## Excellent:Y-Typical:N 112932.128 87518.295 138345.96 0.0000000 ## Excellent:N-Good:N 4250.858 -76919.452 85421.17 1.0000000 ## Poor:Y-Good:N -3810.870 -229993.738 222372.00 1.0000000 ## Fair:Y-Good:N 34353.964 -24751.665 93459.59 0.7089196 ## Typical:Y-Good:N 31191.903 -15974.214 78358.02 0.5315494 ## Good:Y-Good:N 49301.684 1491.797 97111.57 0.0369201 ## Excellent:Y-Good:N 105590.478 58904.465 152276.49 0.0000000 ## Poor:Y-Excellent:N -8061.727 -239327.991 223204.54 1.0000000 ## Fair:Y-Excellent:N 30103.106 -46178.414 106384.63 0.9640522 ## Typical:Y-Excellent:N 26941.045 -40512.921 94395.01 0.9611660 ## Good:Y-Excellent:N 45050.826 -22854.846 112956.50 0.5267698 ## Excellent:Y-Excellent:N 101339.620 34220.481 168458.76 0.0000809 ## Fair:Y-Poor:Y 38164.833 -186309.978 262639.65 0.9999458 ## Typical:Y-Poor:Y 35002.772 -186627.795 256633.34 0.9999711 ## Good:Y-Poor:Y 53112.553 -168655.909 274881.02 0.9990789 ## Excellent:Y-Poor:Y 109401.347 -112127.544 330930.24 0.8652766 ## Typical:Y-Fair:Y -3162.061 -41305.131 34981.01 0.9999999 ## Good:Y-Fair:Y 14947.720 -23988.593 53884.03 0.9699661 ## Excellent:Y-Fair:Y 71236.514 33688.745 108784.28 0.0000001 ## Good:Y-Typical:Y 18109.781 2387.068 33832.49 0.0101482 ## Excellent:Y-Typical:Y 74398.575 62524.140 86273.01 0.0000000 ## Excellent:Y-Good:Y 56288.794 42071.027 70506.56 0.0000000 plot(tukey.ames_int, las = 1) In the giant table above as well as the confidence interval plots, you are able to inspect which combination of categories are statistically different. With interactions present in ANOVA models, post-hoc tests might get overwhelming in trying to find where differences exist. To help guide the exploration of post-hoc tests with interactions, we can do slicing. Slicing is when you perform One-Way ANOVA on subsets of data within categories of other variables. For example, to help discover differences in the interaction between central air and heating quality, we could subset the data into two groups - homes with central air and homes without. Within these two groups we perform One-Way ANOVA across heating quality to find where differences might exist within subgroups. This can easily be done with the group_by function to subset the data. The nest and mutate functions are also used to applied the aov function to each subgroup. Here we run a One-Way ANOVA for heating quality within each subset of central air being present or not. CA_aov &lt;- train %&gt;% group_by(Central_Air) %&gt;% nest() %&gt;% mutate(aov = map(data, ~summary(aov(Sale_Price ~ Heating_QC, data = .x)))) CA_aov ## # A tibble: 2 x 3 ## # Groups: Central_Air [2] ## Central_Air data aov ## &lt;fct&gt; &lt;list&gt; &lt;list&gt; ## 1 Y &lt;tibble [1,904 × 83]&gt; &lt;summry.v&gt; ## 2 N &lt;tibble [147 × 83]&gt; &lt;summry.v&gt; print(CA_aov$aov) ## [[1]] ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Heating_QC 4 2.242e+12 5.606e+11 108.5 &lt;2e-16 *** ## Residuals 1899 9.809e+12 5.165e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## [[2]] ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Heating_QC 4 1.774e+10 4.435e+09 3.793 0.00582 ** ## Residuals 142 1.660e+11 1.169e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see that both of these results are significant at the 0.05 level. However, if our significance level was 0.02 instead, only the first One-Way ANOVA would be significant. This implies that there are statistical differences in average sale price across heating quality within homes that have central air, but not in homes that do not (again at the 0.02 level). 3.2.2 Assumptions The assumptions for the \\(n\\)-Way ANOVA are the same as with the One-Way ANOVA - independence of observations, normality for each category of variable, and equal variances. With the inclusion of two or more variables (\\(n\\)-Way ANOVA with \\(n &gt; 1\\)), these assumptions can be harder to evaluate and test. These assumptions are then applied to the residuals of the model. For equal variances, we can still apply the Levene Test for equal variances using the same leveneTest function as in Chapter 2. leveneTest(Sale_Price ~ Heating_QC*Central_Air, data = train) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 9 24.17 &lt; 2.2e-16 *** ## 2041 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From this test, we can see that we do not meet our assumption of equal variance. Let’s explore the normality assumption. Again, we will assume this normality on the random error component, \\(\\varepsilon_{ijk}\\), of the ANOVA model. More details are found for diagnostic testing using the error component in Chapter ??. We can check normality using the same approaches of the QQ-plot or statistical testing as in Chapter 1.1. Here we will use the plot function on the aov object. Specifically, we want the second plot which is why we have a 2 in the plot function option. We then use the shapiro.test function on the error component. The estimate of the error component is calculated using the residuals function. plot(ames_aov_int, 2) ## Warning: not plotting observations with leverage one: ## 1917 ames_res &lt;- residuals(ames_aov_int) shapiro.test(x = ames_res) ## ## Shapiro-Wilk normality test ## ## data: ames_res ## W = 0.8838, p-value &lt; 2.2e-16 Neither of the normality or equal variance assumptions appear to be met here. This would be a good scenario to have a non-parametric approach. Unfortunately, the Kruskal-Wallis approach is not applicable to \\(n\\)-Way ANOVA where \\(n &gt; 1\\). These approaches would need more non-parametric versions of multiple regression models to account for them. 3.3 Randomized Block Design There are two typical groups of data analysis studies that are conducted. The first are observational/retrospective studies which are the typical data problems people try to solve. The primary characteristic of these analysis are looking at what already happened (retrospective) and potentially inferring those results to further data. These studies have little control over other factors contributing to the target of interest because data is collected after the fact. The other type of data analysis study are controlled experiments. In these situations, you often want to look at the outcome measure prospectively. The focus of the controlled experiment is to control for other factors that might contribute to the target variable. By manipulating these factors of interest, one can more reasonably claim causation. We can more reasonably claim causation when random assignment is used to eliminate potential nuisance factors. Nuisance factors are variables that can potentially impact the target variable, but are not of interest in the analysis directly. 3.3.1 Garlic Bulb Weight Example For this analysis we will use a new dataset. This dataset contains the average garlic bulb weight from different plots of land. We want to compare the effects of fertilizer on average bulb weight. However, different plots of land could have different levels of sun exposure, pH for the soil, and rain amounts. Since we cannot alter the pH of the soil easily, or control the sun and rain, we can use blocking to account for these nuisance factors. Each fertilizer was randomly applied in quadrants of 8 plots of land. These 8 plots have different values for sun exposure, pH, and rain amount. Therefore, if we only put one fertilizer on each plot, we would not know if the fertilizer was the reason the garlic crop grew or if it was one of the nuisance factors. Since we blocked these 8 plots and applied all four fertilizers in each we have essentially accounted for (or removed the effect of) the nuisance factors. Let’s briefly explore this new dataset by looking at all 32 values using the print function. block &lt;- read_csv(file = &quot;garlic_block.csv&quot;) ## Parsed with column specification: ## cols( ## Sector = col_double(), ## Position = col_double(), ## Fertilizer = col_double(), ## BulbWt = col_double(), ## Cloves = col_double(), ## BedId = col_double() ## ) head(block, n = 32) ## # A tibble: 32 x 6 ## Sector Position Fertilizer BulbWt Cloves BedId ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 3 0.259 11.6 22961 ## 2 1 2 4 0.207 12.6 23884 ## 3 1 3 1 0.275 12.1 19642 ## 4 1 4 2 0.245 12.1 20384 ## 5 2 1 3 0.215 11.6 20303 ## 6 2 2 4 0.170 12.7 21004 ## 7 2 3 1 0.225 12.0 16117 ## 8 2 4 2 0.168 11.9 19686 ## 9 3 1 4 0.217 12.4 26527 ## 10 3 2 3 0.226 11.7 23574 ## # … with 22 more rows How do we account for this blocking in an ANOVA model context? This blocking ANOVA model is the exact same as the Two-Way ANOVA model. The variable that identifies which sector (block) an observation is in serves as another variable in the model. Think about this variable as the variable that accounts for all the nuisance factors in your ANOVA. That means we have two variables in this ANOVA model - fertilizer and sector (the block that accounts for sun exposure, pH level of soil, rain amount, etc.). For this we can use the same aov function we described above. block_aov &lt;- aov(BulbWt ~ factor(Fertilizer) + factor(Sector), data = block) summary(block_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(Fertilizer) 3 0.005086 0.0016954 4.307 0.016222 * ## factor(Sector) 7 0.017986 0.0025695 6.527 0.000364 *** ## Residuals 21 0.008267 0.0003937 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Using the summary function we can see that both the sector (block) and fertilizer variables are significant in the model at the 0.05 level. What are the interpretations of this? First, let’s address the blocking variable. Whether it is significant or not, it should always be included in the model. This is due to the fact that the data is structured in that way. It is a construct of the data that should be accounted for regardless of the significance. However, since the blocking variable (sector) was significant, that implies that different plots of land have different impacts of the average bulb weight of garlic. Again, this is most likely due to the differences between the plots of land - namely sun exposure, pH of soil, rain fall, etc. Second, the variable of interest is the fertilizer variable. It is significant, implying that there is a difference in the average bulb weight of garlic for different fertilizers. To examine which fertilizer pairs are statistically difference we can use post-hos testing as described in the previous parts of this chapter using the TukeyHSD function. tukey.block &lt;- TukeyHSD(block_aov) print(tukey.block) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = BulbWt ~ factor(Fertilizer) + factor(Sector), data = block) ## ## $`factor(Fertilizer)` ## diff lwr upr p adj ## 2-1 -0.02509875 -0.05275125 0.002553751 0.0840024 ## 3-1 -0.01294875 -0.04060125 0.014703751 0.5698678 ## 4-1 -0.03336125 -0.06101375 -0.005708749 0.0144260 ## 3-2 0.01215000 -0.01550250 0.039802501 0.6186232 ## 4-2 -0.00826250 -0.03591500 0.019390001 0.8382800 ## 4-3 -0.02041250 -0.04806500 0.007240001 0.1995492 ## ## $`factor(Sector)` ## diff lwr upr p adj ## 2-1 -0.0520675 -0.099126544 -5.008456e-03 0.0234315 ## 3-1 -0.0145075 -0.061566544 3.255154e-02 0.9634255 ## 4-1 -0.0450550 -0.092114044 2.004044e-03 0.0669646 ## 5-1 -0.0616250 -0.108684044 -1.456596e-02 0.0051483 ## 6-1 -0.0196650 -0.066724044 2.739404e-02 0.8466335 ## 7-1 0.0084950 -0.038564044 5.555404e-02 0.9984089 ## 8-1 -0.0393325 -0.086391544 7.726544e-03 0.1469768 ## 3-2 0.0375600 -0.009499044 8.461904e-02 0.1841786 ## 4-2 0.0070125 -0.040046544 5.407154e-02 0.9995370 ## 5-2 -0.0095575 -0.056616544 3.750154e-02 0.9966777 ## 6-2 0.0324025 -0.014656544 7.946154e-02 0.3337758 ## 7-2 0.0605625 0.013503456 1.076215e-01 0.0061094 ## 8-2 0.0127350 -0.034324044 5.979404e-02 0.9819446 ## 4-3 -0.0305475 -0.077606544 1.651154e-02 0.4025951 ## 5-3 -0.0471175 -0.094176544 -5.845586e-05 0.0495704 ## 6-3 -0.0051575 -0.052216544 4.190154e-02 0.9999400 ## 7-3 0.0230025 -0.024056544 7.006154e-02 0.7227812 ## 8-3 -0.0248250 -0.071884044 2.223404e-02 0.6454690 ## 5-4 -0.0165700 -0.063629044 3.048904e-02 0.9286987 ## 6-4 0.0253900 -0.021669044 7.244904e-02 0.6208608 ## 7-4 0.0535500 0.006490956 1.006090e-01 0.0186102 ## 8-4 0.0057225 -0.041336544 5.278154e-02 0.9998793 ## 6-5 0.0419600 -0.005099044 8.901904e-02 0.1034664 ## 7-5 0.0701200 0.023060956 1.171790e-01 0.0012997 ## 8-5 0.0222925 -0.024766544 6.935154e-02 0.7514897 ## 7-6 0.0281600 -0.018899044 7.521904e-02 0.5004099 ## 8-6 -0.0196675 -0.066726544 2.739154e-02 0.8465530 ## 8-7 -0.0478275 -0.094886544 -7.684559e-04 0.0446174 plot(tukey.block, las = 1) 3.3.2 Assumptions Outside of the typical assumptions for ANOVA that still hold here, there are two additional assumptions to be met: - Treatments are randomly assigned within each block - The effects of the treatment variable are constant across the levels of the blocking variable The first, new assumption of randomness deals with the reliability of the analysis. Randomness is key to removing the impact of the nuisance factors. The second, new assumption implies there is no interaction between the treatment variable and the blocking variable. For example, we are implying that the fertilizers’ impacts ob garlic bulb weight are not changed depending on what block you are on. In other words, fertilizers have the same impact regardless of sun exposure, pH levels, rain fall, etc. We are not saying these nuisance factors do not impact the target variable or bulb weight of garlic, just that they do not change the effect of the fertilizer on bulb weight. 3.4 Multiple Linear Regression Most practical applications of of regression modeling involve using more complicated models than a simple linear regression with only one predictor variable to predict your target. Additional variables in a model can lead to better explanations and predictions of the target. These linear regressions with more than one variable are called multiple linear regression models. However, as we will see in this section and the following chapters, with more variables comes much more complication. 3.4.1 Model Structure Multiple linear regression models have the same structure as simple linear regression models, only with more variables. The multiple linear regression model with \\(k\\) variables is structured like the following: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\varepsilon \\] This model has the predictor variables \\(x_1, x_2, ..., x_k\\) trying to either explain or predict the target variable \\(y\\). The intercept, \\(\\beta_0\\), still gives the expected value of \\(y\\), when all of the predictor variables take a value of 0. With the addition of multiple predictors, the interpretation of the slope coefficients change slightly. The slopes, \\(\\beta_1, \\beta_2, ..., \\beta_k\\), give the expected change in \\(y\\) for a one unit change in the respective predictor variable, holding all other predictor variables constant. The random error term, \\(\\varepsilon\\), is the error between our predicted value, \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\cdots + \\hat{\\beta}_k x_k\\), and our actual value of \\(y\\). Unlike simple linear regression that can be visualized as a line through a 2-dimensional scatterplot of data, a multiple linear regression is better thought of as a multi-dimensional plane through a multi-dimensional scatterplot of data. Let’s visual an example with two predictor variables - the square footage of greater living area and the total number of rooms. These will predict sale price of a home. When none of the variables have any relationship with the target variable, we get a horizontal plane like the one shown below. This is similar in concept to a horizontal line in simple linear regression having a slope of 0, implying that the target variable does not change as the predictor variable changes. Much like if the slope of a simple linear regression line is not 0 (a relationship exists between the predictor and target variable), then a relationship between any of the predictor variables and the target variable shifts and rotates the plane around like the one shown below. To the naive viewer, the shifted plane would still make sense because of the model naming convention of multiple linear regression. However, the linear in linear regression doesn’t have to deal with the visualization of the fitted plane (or line in two dimensions), but instead refers to the linear combination of variables. A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results. For example, \\(ax + by\\) is a linear combination of \\(x\\) and \\(y\\). Therefore, the linear model \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_k x_k + \\varepsilon \\] is a linear combination of predictor variables in their relationship with the target variable \\(y\\). These predictor variables do not all have to contain linear effects though. For example, let’s look at a linear regression model with four predictor variables: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_4 + \\varepsilon \\] One would not be hard pressed to call this model a linear regression. However, what if we defined \\(x_3 = x_1^2\\) and \\(x_4 = x_2^2\\)? This model is still a linear regression model. The structure of the model did not change. The model is still a linear combination of predictor variables related to the target variable. The predictor variables just do not all have a linear effect in terms of their relationship with \\(y\\). However, mathematically, it is still a linear combination and a linear regression model. Logistic regression on the other hand, which will be covered in Chapter ?? is an example of a nonlinear regression model. 3.4.2 Global &amp; Local Inference In simple linear regression we could just look at the t-test for our slope parameter estimate to determine the utility of our model. With multiple parameter estimates comes multiple t-tests. Instead of looking at every individual parameter estimate initially, there is a way to determine the model adequacy for predicting the target variable overall. The utility of a multiple regression model can be tested with a single test that encompasses all the coefficients from the model. This kind of test is called a global test since it tests all \\(\\beta\\)’s simultaneously. The Global F-Test uses the F-distribution to do just that for multiple linear regression models. The hypotheses for this test are the following: \\[ H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0 \\\\ H_A: \\text{at least one } \\beta \\text{ is nonzero} \\] In simpler terms, the null hypothesis is that none of the variables are useful in predicting the target variable. The alternative hypothesis is that at least one of these variables is useful in predicting the target variable. The F-distribution is a distribution that has the following characteristics: - Bounded below by 0 - Right-skewed - Both numerator and denominator degrees of freedom A plot of a variety of F distributions is shown here: ## Warning: Removed 1500 row(s) containing missing values (geom_path). If the global test is significant, the next step would be to examine the individual t-tests to see which variables are significant and which ones are not. This is similar to post-hoc testing in ANOVA where we explored which of the categories was statistically different when we knew at least one was. These tests are all available using the summary function on an lm function for linear regression. To build a multiple linear regression in R using the lm function, you just add another variable to the formula element. Here we will predict the sales price (Sale_Price) based on the square footage of the greater living area of the home (Gr_Liv_Area) as well as total number of rooms above ground (TotRms_AbvGrd). ames_lm2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train) summary(ames_lm2) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -528656 -30077 -1230 21427 361465 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42562.657 5365.721 7.932 3.51e-15 *** ## Gr_Liv_Area 136.982 4.207 32.558 &lt; 2e-16 *** ## TotRms_AbvGrd -10563.324 1370.007 -7.710 1.94e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56630 on 2048 degrees of freedom ## Multiple R-squared: 0.5024, Adjusted R-squared: 0.5019 ## F-statistic: 1034 on 2 and 2048 DF, p-value: &lt; 2.2e-16 At the bottom of the above output is the result of the global F-test. Since the p-value on this test is lower than the significance level of 0.05, we have statistical evidence that at least of the two variables - Gr_Liv_Area and TotRms_AbvGrd - is significant at predicting the sale price of the home. By looking at the individual t-tests in the output above, we can see that both variables are actually significant. 3.4.3 Assumptions The main assumptions for the multiple linear regression model are the same as with the simple linear regression model: 1. The expected value of \\(y\\) is linear in the \\(x\\)’s (proper model specification). 2. The random errors are independent. 3. The random errors are normally distributed. 4. The random errors have equal variance (homoskedasticity). However, with multiple variables there is an additional assumption that people tend to add to multiple linear regression modeling: 5. No perfect collinearity (also called multicollinearity) The new assumption means that no combination of predictor variables is a perfect linear combination with any other predictor variables. Collinearity, also called multicollinearity, occurs when predictor variables are correlated with each other. People often misstate this additional assumption as having no collinearity at all. This is too restrictive and basically impossible to meet in a realistic setting. Only when collinearity has a drastic impact on the linear regression do we need to concern ourselves. In fact, linear regression only completely breaks when that collinearity is perfect. Dealing with multicollinearity is the subject of Chapter ??. Similar to simple linear regression, we can evaluate the assumptions by looking at residual plots. The plot function on the lm object provides these. par(mfrow=c(2,2)) plot(ames_lm2) par(mfrow=c(1,1)) These will again be covered in much more detail in Chapter ??. 3.4.4 Multiple Coefficients of Determination One of the main advantages of multiple linear regression is that the complexity of the model enables us to investigate the relationship among \\(y\\) and several predictor variables simultaneously. However, this increased complexity makes it more difficult to not only interpret the models, but also ascertain which model is “best.” One example of this would be the coefficient of determination, \\(R^2\\), that we discussed in Chapter 2. The calculation for \\(R^2\\) is the exact same: \\[ R^2 = 1 - \\frac{SSE}{TSS} \\] However, the problem with the calculation of \\(R^2\\) in a multiple linear regression is that the addition of any variable (useful or not) will never make the \\(R^2\\) decrease. In fact, it typically increases even with the addition of a useless variable. The reason is rather intuitive. When adding information to a regression model, your predictions can only get better, not worse. If a new predictor variable has no impact on the target variable, then the predictions can not get any worse than what they already were before the addition of the useless variable. Therefore, the \\(SSE\\) would never increase, making the \\(R^2\\) never decrease. To account for this problem, there is the adjusted coefficient of determination, \\(R^2_a\\). The calculation is the following: \\[ R^2_a = 1 - [(\\frac{n-1}{n-(k+1)})\\times (\\frac{SSE}{TSS})] \\] Notice what the calculation is doing. It takes the original ratio on the right hand side of the \\(R^2\\) equation, \\(SSE/TSS\\), and penalizes it. It multiplies this number by a ratio that is always greater than 1 if \\(k &gt; 0\\). Remember, \\(k\\) is the number of variables in the model. Therefore, as the number of variables increases, the calculation penalizes the model more and more. However, if the reduction of SSE from adding a useful variable is low enough, then even with the additional penalization, the \\(R^2_a\\) will increase if the variable is a useful addition to the model. If the variable is not a useful addition to the model, the \\(R^2_a\\) will decrease. The \\(R^2_a\\) is only one of many ways to select the “best” model for multiple linear regression. Many more metrics will be discussed in model selection in Chapter ??. One downside of this new metric is that the \\(R^2_a\\) loses its interpretation. Since \\(R^2_a \\le R^2\\), it is no longer bounded below by zero. Therefore, it can no longer be the proportion of variation explained in the target variable by the model. However, we can easily use \\(R^2_a\\) to select a model correctly and interpret that model with \\(R^2\\). Both of these numbers can be found using the summary function on the lm object from the previous model. summary(ames_lm2) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -528656 -30077 -1230 21427 361465 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42562.657 5365.721 7.932 3.51e-15 *** ## Gr_Liv_Area 136.982 4.207 32.558 &lt; 2e-16 *** ## TotRms_AbvGrd -10563.324 1370.007 -7.710 1.94e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56630 on 2048 degrees of freedom ## Multiple R-squared: 0.5024, Adjusted R-squared: 0.5019 ## F-statistic: 1034 on 2 and 2048 DF, p-value: &lt; 2.2e-16 From this output we can say that the combination of Gr_Liv_Area and TotRmsAbvGrd account for 50.95% of the variation in Sale_Price. Now let’s add a random variable to the model. This random variable will take random values from a normal distribution with mean of 0 and standard deviation of 1 and has no impact on the target variable. set.seed(1234) ames_lm3 &lt;- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 0, 1), data = train) summary(ames_lm3) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), ## 0, 1), data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -527926 -29943 -1298 21427 363925 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 42589.091 5364.877 7.939 3.34e-15 *** ## Gr_Liv_Area 136.927 4.207 32.548 &lt; 2e-16 *** ## TotRms_AbvGrd -10552.425 1369.808 -7.704 2.05e-14 *** ## rnorm(length(Sale_Price), 0, 1) 1629.854 1259.478 1.294 0.196 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 56620 on 2047 degrees of freedom ## Multiple R-squared: 0.5028, Adjusted R-squared: 0.502 ## F-statistic: 689.9 on 3 and 2047 DF, p-value: &lt; 2.2e-16 Notice that the \\(R^2\\) of this model actually increased to 0.5097 from 0.5095. However, the \\(R^2_a\\) value dropped from 0.5091 to 0.5089 since the addition of this new variable did not provide enough predictive power to outweigh the penalty of adding it. 3.4.5 Categorical Predictor Variables As mentioned in Chapter 1.1, there are two types of variables typically used in modeling: - Quantitative (or numeric) - Qualitative )or categorical) Categorical variables need to be coded differently because they are not numerical in nature. As mentioned in Chapter 1.1, two common coding techniques for linear regression are reference and effects coding. The interpretation of the coefficients (\\(\\beta\\)’s) of these variables in a regression model depend on the specific coding used. The predictions from the model, however, will remain the same regardless of the specific coding that is used. Let’s use the example of the Central_Air variable with 2 categories - Y and N. Using reference coding, the reference coded variable to describe these 2 categories (with N as the reference level) would be the following: Central Air X1 Y 1 N 0 Table 3.1: Reference variable coding for the categorical attribute Central Air The linear regression equation would be: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 \\] Let’s see the mathematical interpretation of the coefficient \\(\\hat{\\beta}_1\\). To do this, let’s get the average sale price of a home prediction for a home with central air (\\(\\hat{y}_Y\\)) and without central air (\\(\\hat{y}_N\\)): \\[ \\hat{y}_Y = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\\\ \\hat{y}_N = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0 = \\hat{\\beta}_0 \\] By subtracting these two equations (\\(\\hat{y}_Y - \\hat{y}_N = \\hat{\\beta}_1\\)), we can get the prediction for the average difference in price between a home with central air and without central air. This shows that in reference coding, the coefficient on each dummy variable is the average difference between that category and the reference category (the category not represented with its own variable). The math can be extended to as many categories as needed. Using effects coding, the effects coded variable to describe these 2 categories (with N as the reference level) would be the following: Central Air X1 Y 1 N -1 Table 3.1: Effects variable coding for the categorical attribute Central Air The linear regression equation would be: \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_1 \\] Let’s see the mathematical interpretation of the coefficient \\(\\hat{\\beta}_1\\). To do this, let’s get the average sale price of a home prediction for a home with central air (\\(\\hat{y}_Y\\)) and without central air (\\(\\hat{y}_N\\)): \\[ \\hat{y}_Y = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\\\ \\hat{y}_N = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot -1 = \\hat{\\beta}_0 - \\hat{\\beta}_1 \\] Similar to reference coding, the coefficient \\(\\hat{\\beta}_1\\) is the average difference between homes with central air and \\(\\hat{\\beta}_0\\). However, what is \\(\\hat{\\beta}_0\\)? By taking the average of our two predictions: \\[ \\frac{1}{2} \\times (\\hat{y}_Y + \\hat{y}_N) = \\frac{1}{2} \\times (\\hat{\\beta}_0 + \\hat{\\beta}_1 + \\hat{\\beta}_0 - \\hat{\\beta}_1) = \\frac{1}{2} \\times (2\\hat{\\beta}_0) = \\hat{\\beta}_0 \\] From this average we can get the prediction for the average difference in price between a home with central air and the average price across all homes. This shows that in effects coding, the coefficient on each dummy variable is the average difference between that category and the average price across all homes (including both with and without central air). The math can be extended to as many categories as needed. Let’s see an example with Central_Air as a variable added to our multiple linear regression model as a reference coded variable. ames_lm4 &lt;- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Central_Air, data = train) summary(ames_lm4) ## ## Call: ## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Central_Air, ## data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -510745 -28984 -2317 20273 356742 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7169.259 6778.879 -1.058 0.29 ## Gr_Liv_Area 129.594 4.131 31.374 &lt; 2e-16 *** ## TotRms_AbvGrd -8980.938 1335.669 -6.724 2.29e-11 *** ## Central_AirY 54513.082 4762.926 11.445 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 54910 on 2047 degrees of freedom ## Multiple R-squared: 0.5323, Adjusted R-squared: 0.5316 ## F-statistic: 776.6 on 3 and 2047 DF, p-value: &lt; 2.2e-16 With these results we estimate the average difference in sales price between homes with central air and without central air to be $56,672.41. "],["model-building-scoring-for-prediction.html", "Chapter 4 Model Building &amp; Scoring for Prediction 4.1 Model Complexity 4.2 Regularized Regression 4.3 Modeling Scoring 4.4 Model Metrics", " Chapter 4 Model Building &amp; Scoring for Prediction 4.1 Model Complexity 4.2 Regularized Regression 4.3 Modeling Scoring 4.4 Model Metrics "],["model-selection.html", "Chapter 5 Model Selection 5.1 Selection Criteria 5.2 Stepwise Selection 5.3 Significance Levels", " Chapter 5 Model Selection When creating a multiple linear regression model, your goal is to find the best model possible. However, within the data, there most likely exists variables that are informative and ones that are uninformative in predicting the response. With many explanatory variables, it could be extremely time consuming to try all potential models by hand, and the use of automatic procedures can greatly assist in obtaining subsets of variables in which to focus your attention. CAUTION: you should NEVER just use the final model created from an automatic procedure! Always double check the variables (look at those included and those not included). Use content knowledge, common sense, diagnostics, etc to decide on your final model. In this Chapter, we will focus on two techniques for automatic variable selection: stepwise procedures and LASSO/Ridge. Within the stepwise procedures, we will focus on forward, backward and stepwise searches using several different selection criteria. We will end this section discussing important considerations in use of p-values when dealing with large data sets. 5.1 Selection Criteria when trying to find the best model, we can use different criteria to choose. For exmaple, you have already seen \\(R^{2}\\) (the larger value of \\(R^{2}\\) the better the model). However, when comparing models, the adjusted \\[R^{2}\\] is better due to the fact that \\(R^{2}\\) can potentially increase even when adding noise. The adjusted \\(R^{2}\\) can be thought of as \\(R^{2}\\) with penalty (for every additional variable added to the model, we add a higher penalty). This allows us to weigh the contribution of adding new variables against the added complexity of more variables in the model. There are other selection criteria that are also used in selecting the “best model” (or variable selection). As you will notice, these selection criteria also have a penalty to take into account the addition of variables. We wil focus on the two most common ones….AIC and BIC (or also referred to as SBC). The AIC, or Akaike information criterion, was developed by statistician Hirotugu Akaike back in the 1970’s and is defined by \\[ AIC = -2log(Likelihood) + 2p. \\] In this case, the “penalty” is 2p, where p is the number of parameters in the model. BIC, also known as the Bayesian Information Criterion (also called SBC or Schwarz Bayesian Information) was first developed by Gideon E. Schwarz and is defined by \\[BIC = nlog(SSE/n) + plog(n). \\] In this case, the “penalty” is plog(n), where p is the number of parameters in the model and n is the sample size. Notice that both penalties are multiplied by p. Keep these quantities (2 for AIC and log(n) for BIC) in your mind for later…you will be seeing them again. 5.2 Stepwise Selection We will talk about three different algorithms in the stepwise selection search: forward, backward and stepwise. Each of these algorithms either add or take away one variable at a time based on a given criterion until this criterion can no longer be met. At which point the algorithm stops. &gt; library(AmesHousing) &gt; library(tidyverse) &gt; library(car) &gt; library(DescTools) &gt; library(corrplot) &gt; library(glmnet) &gt; library(leaps) Forward For forward selection, we start with a null model (only the intercept) and adds one variable at a time until no other variables can be added based on a given criterion. The algorithm is as follows Start with a null model Create p simple linear regressions Loop starts here See which linear regression is best (based on criterion) is this regression better than the regression in the previous step? If yes, keep this variable. If no, algorithm stops. &gt; # k = 2 for AIC selection &gt; for.model &lt;- step(empty.model, + scope = list(lower = empty.model, + upper = full.model), + direction = &quot;forward&quot;, k = 2) &gt; # k = log(n) for BIC selection &gt; for.model2 &lt;- step(empty.model, + scope = list(lower = empty.model, + upper = full.model), + direction = &quot;forward&quot;, k = log(nrow(train_sel))) # k = qchisq(alpha, 1, lower.tail = FALSE) for p-value with alpha selection &gt; alpha.f=0.05 &gt; for.model3 &lt;- step(empty.model, + scope = list(lower = empty.model, + upper = full.model), + direction = &quot;forward&quot;, k = qchisq(alpha.f, 1, lower.tail = FALSE)) Backward Stepwise LASSO 5.3 Significance Levels "],["diagnostics.html", "Chapter 6 Diagnostics 6.1 Examining Residuals 6.2 Misspecified Model 6.3 Constant Variance 6.4 Normality 6.5 Correlated Errors 6.6 Influential Observations and Outliers 6.7 Multicollinearity", " Chapter 6 Diagnostics 6.1 Examining Residuals 6.2 Misspecified Model 6.3 Constant Variance 6.4 Normality 6.5 Correlated Errors 6.6 Influential Observations and Outliers 6.7 Multicollinearity "],["categorical-data-analysis.html", "Chapter 7 Categorical Data Analysis 7.1 Describing Categorical Data 7.2 Tests of Association 7.3 Measures of Association 7.4 Introduction to Logistic Regression 7.5 Adding Categorical Variables and Interactions", " Chapter 7 Categorical Data Analysis 7.1 Describing Categorical Data 7.2 Tests of Association 7.3 Measures of Association 7.4 Introduction to Logistic Regression 7.5 Adding Categorical Variables and Interactions "]]
