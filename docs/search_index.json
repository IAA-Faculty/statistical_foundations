[["index.html", "Statistical Foundations Preface", " Statistical Foundations Preface Structure of the book The book is broken down into small sections that aim to demonstrate a single concept at a time. Chapter ?? introduces the basic components of R, and how you can assign values to variables. This book is a work in progress. Submit any issues here. Please check back for frequent updates. About the Authors Acknowledgements The author would like to thank the members of the faculty and the TAs at the Institute who provided feedback on this work. "],["eda.html", "Chapter 1 Exploratory Data Analysis (EDA) 1.1 Types of Variables 1.2 Honest Assessment 1.3 Distributions 1.4 Confidence Intervals 1.5 Hypothesis Testing 1.6 Two-Sample t-tests", " Chapter 1 Exploratory Data Analysis (EDA) The crucial first step to any data science problem is exploratory data analysis (EDA). Before you attempt to run any models, or jump towards any formal statistical analysis, you must explore your data. Many unexpected frustrations arise when exploratory analysis is overlooked; knowing your data is critical to your ability to make necessary assumptions about it. This preliminary analysis will help inform our decisions for data manipulation, give us a base-level understanding of our variables and the relationships between them, and help determine which statistical analyses might be appropriate for the questions we are trying to answer. Some of the questions we aim to answer through exploratory analysis are: What kind of variables to you have? Continuous Nominal Ordinal How are the attributes stored? Strings Integers Floats/Numeric Dates What do their distributions look like? Center/Location Spread Shape Are there any anomolies? Outliers Leverage points Missing values Low-frequency categories We will maintain an example data set throughout this text to demonstrate the various tools and techniques being discussed. It is a real-estate data set that contains the sale_price and physical attributes of nearly 3,000 homes in Ames, Iowa in the early 2000s. To access this data, we first install the AmesHousing package, add it to our library and create the nicely formatted data with the make_ames() function. install.packages(&quot;AmesHousing&quot;) library(AmesHousing) ames &lt;- make_ames() str(ames) 1.1 Types of Variables The columns of a data set are referred to by a number of equivalent terms: Variables Features Attributes Predictors/Targets Factors Inputs/Outputs This book may use any of these words interchangeably to refer to a quality or quantity of interest in our data. 1.1.1 Nominal Variables A nominal or categorical variable is a quality of interest whose values have no logical ordering. Color (“blue”, “red”, “green”…), ethnicity (“African-American”, “Asian”, “Caucasian”,…), and style of house (“ranch”, “two-story”, “duplex”, …) are all examples of nominal attributes. The categories or values that these variables can take on - those words listed in quotes and parenthesis - are called the levels of the variable. In modeling, nominal attributes are commonly transformed into dummy variables. Dummy variables are binary columns that indicate the presence or absence of a quality. There is more than one way to create dummy variables, and the treatment will be different depending on what type of model you are using. Linear regression models will use either reference-level or effects coding, whereas other machine learning models are more likely to use one-hot encoding or a variation thereof. One-hot encoding For machine learning applications, it is common to create a binary dummy column for each level of your categorical variable. This is the most intuitive representation of categorical information, answering indicative questions for each level of the variable: “is it blue?”, “is it red?” etc. The table below gives an example of some data, the original nominal variable (color) and the one-hot encoded color information. Observation Color Blue Red Yellow Other 1 Blue 1 0 0 0 2 Yellow 0 0 1 0 3 Blue 1 0 0 0 4 Red 0 1 0 0 5 Red 0 1 0 0 6 Blue 1 0 0 0 7 Yellow 0 0 1 0 8 Other 0 0 0 1 Table 1.1: One-hot dummy variable coding for the categorical attribute color We will demonstrate the creation of this data using some simple random categorical data: set.seed(41) data &lt;- data.frame(y = c(rnorm(10,2), rnorm(10,1),rnorm(10,0)), x1 = factor(rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 10)), x2 = factor(rep(c(&quot;Z&quot;, &quot;X&quot;, &quot;Y&quot;,&quot;W&quot;,&quot;V&quot;,&quot;U&quot;), each = 5))) View(data) Unlike reference and effects coding, which are typically specified within the lm() function as we will see in Chapter 2, one-hot encoding is most quickly achieved through use of the onehot package in R, which first creates an “encoder” to do the job quickly. The speed of this function has been tested against both the base R model.matrix() function and the dummyVars() function in the caret package and is substantially faster than either. install.packages(&quot;onehot&quot;) library(onehot) encoder = onehot(data) dummies = predict(encoder,data) head(dummies) ## y x1=A x1=B x1=C x2=U x2=V x2=W x2=X x2=Y x2=Z ## [1,] 1.205632 1 0 0 0 0 0 0 0 1 ## [2,] 2.197258 1 0 0 0 0 0 0 0 1 ## [3,] 3.001704 1 0 0 0 0 0 0 0 1 ## [4,] 3.288825 1 0 0 0 0 0 0 0 1 ## [5,] 2.905753 1 0 0 0 0 0 0 0 1 ## [6,] 2.493667 1 0 0 0 0 0 1 0 0 Reference-level coding Reference-level coding is similar to one-hot encoding except one of the levels of the attribute, called the reference level, is omitted. Notice that the 4 dummy columns from Table 1.1 collectively form a linearly dependent set; that is, if you know the values of 3 of the 4 dummy variables you can determine the \\(4^{th}\\) with complete certainty. This would be a problem for linear regression, where we assume our input attributes are not linearly dependent as we will discuss in Chapter 2. A reference level of the attribute is often specified by the user to be a particular level worthy of comparison (a baseline), as the regression output will be interpreted in a way that compares each non-reference level to the reference level. If a reference level is not specified by the user, one will be picked by the software by default either using the order in which the levels were encountered in the data, or their alphabetical ordering. Users should check the documentation of the associated function to understand what to expect. Table 1.2 transforms the one-hot encoding from Table 1.1 into reference-level coding with the color “blue” as the reference level. Notice the absence of the column indicating “blue” and how each blue observation exists as a row of zeros. Observation Color Red Yellow Other 1 Blue 0 0 0 2 Yellow 0 1 0 3 Blue 0 0 0 4 Red 1 0 0 5 Red 1 0 0 6 Blue 0 0 0 7 Yellow 0 1 0 8 Other 0 0 1 Table 1.2: Reference-level dummy variable coding for the categorical attribute color and the reference level of “blue” Effects coding Effects coding is useful for obtaining a more general comparative interpretation when you have approximately equal sample sizes across each level of your categorical attribute. Effects coding is designed to allow the user to compare each level to all the other levels. More specifically the mean of each level is compared to the overall mean of your data. However, the comparison is actually to the so-called grand mean, which is the mean of the means of each group. When sample sizes are equal, the grand mean and the overall sample mean are equivalent. When sample sizes are not equal, the parameter estimates for effects coding should not be used for interpretation or explanation. Effects coding still requires a reference level, however the purpose of the reference level is not the same as it was in reference-level coding. Here, the reference level is left out in the sense that no comparison is made between it and the overall mean. Table 1.3 shows our same example with effects coding. Again we notice the absence of the column indicating “blue” but now the reference level receives values of -1 rather than 0 for all 3 dummy columns. We will revisit the interpretation of linear regression coefficients under this coding scheme in Chapter 2. Observation Color Red Yellow Other 1 Blue -1 -1 -1 2 Yellow 0 1 0 3 Blue -1 -1 -1 4 Red 1 0 0 5 Red 1 0 0 6 Blue -1 -1 -1 7 Yellow 0 1 0 8 Other 0 0 1 Table 1.3: Effects coding for the categorical attribute color and the reference level of “blue” 1.1.2 Interval Variables An interval variable is a quantity of interest on which the mathematical operations of addition, subtraction, multiplication and division can be performed. Time, temperature and age are all examples of interval attributes. To illustrate the definition, note that “15 minutes” divided by “5 minutes” is 3, which indicates that 15 minutes is 3 times as long as 5 minutes. The sensible interpretation of this simple arithmetic sentence demonstrates the nature of interval attributes. One should note that such arithmetic would not make sense in the treatment of nominal variables. 1.1.3 Ordinal Variables Ordinal variables are attributes that are qualitative in nature but have some natural ordering. Level of education is a common example, with a level of ‘PhD’ indicating more education than ‘Bachelors’ but lacking a numerical framework to quantify how much more. The treatment of ordinal variables will depend on the application. Survey responses on a Likert scale are also ordinal - a response of 4=“somewhat agree” on a 1-to-5 scale of agreement cannot reliably be said to be twice as enthusiastic as a response of 2=“somewhat disagree”. These are not interval measurements, though they are often treated as such in a trade-off for computational efficiency. Ordinal variables will either be given some numeric value and treated as interval variables or they will be treated as categorical variables and dummy variables will be created. The choice of solution is up to the analyst. When numeric values are assigned to ordinal variables, the possibilities are many. For example, consider level of education. The simplest ordinal treatment for such an attribute might be something like: Level of Education Numeric Value No H.S. Diploma 1 H.S. Diploma or GED 2 Associates or Certificate 3 Bachelors 4 Graduate Certificate 5 Masters 6 PhD 7 While numeric values have been assigned and this data could be used like an interval attribute, it’s important to realize that the notion of a “one-unit-increase” is qualitative in nature rather than quantitative. However, if we’re interested in learning whether there is a linear type of relationship between education and another attribute (meaning as education level increases, the value of another attribute increases or decreases), this would be the path to get us there. However we’re making an assumption in this model that the difference between a H.S. Diploma and an Associates degree (a difference of “1 unit”) is the same as the difference between a Master’s degree and a PhD (also a difference of “1 unit”). These types of assumptions can be flawed, and it is often desirable to develop an alternative system of measurement based either on domain expertise or the target variable of interest. This is the notion behind optimal scaling and target-level encoding. Optimal Scaling Target-level Encoding 1.2 Honest Assessment When performing predictive modeling, we always divide our data into subsets for training, validation, and/or final testing. This is a concept that will be revisited several times throughout the introductory curriculum, highlighting its importance to honest assessment of models. There is no single right answer for how this division should occur for every data set - the answer depends on a multitude of factors that are beyond the scope of our present discussion. Generally speaking, one expects to keep about 70% of the data for model training purposes, and the remaining 30% for validation and testing. These proportions may change depending on the amount and of data available. If one has millions of observations, they can often get away with a much smaller proportion of training data to reduce computation time and increase confidence in validation. If one has substantially fewer observations, it may be necessary to increase the training proportion in order to build a sound model - trading validation confidence for proper training. Below we demonstrate two techniques for separating the data into just two subsets: training and test. These two subsets will suffice for our analyses in this text. We’ll use 70% of our data for the training set and the remainder for testing. Since we are taking a random sample, each time you run these functions you will get a different result. This can be difficult for team members who wish to keep their analyses in sync. To avoid that variation of results, we can provide a “seed” to the internal random number generation process, which ensures that the randomly generated output is the same to all who use that seed. Sampling via the tidyverse. This method requires the use of an id variable. If your data set has a unique identifier built in, you may omit the first line of code (after set.seed()) and use that unique identifier in the third line. library(tidyverse) set.seed(123) ames &lt;- ames %&gt;% mutate(id = row_number()) train &lt;- ames %&gt;% sample_frac(0.7) test &lt;- anti_join(ames, train, by = &#39;id&#39;) dim(train) ## [1] 2051 82 dim(test) ## [1] 879 82 Sampling via old-fashioned local indexing. This method creates a logical vector (containing the values TRUE or FALSE) that indicates the training set observations. The opposite vector, !train_obs then indicates the test set. The benefit of this method is the storage of this vector that permanently identifies which observations were placed in the training set. set.seed(123) train_obs = sample(c(T,F), nrow(ames),replace=T, prob = c(70,30)) train = ames[train_obs, ] test = ames[!train_obs, ] dim(train) ## [1] 2049 82 dim(test) ## [1] 881 82 It’s important to note that even with the same seed, these two methods will not produce the same result as the randomization contained within them is fundamentally different. 1.3 Distributions After reviewing the types and formats of the data inputs, we move on to some basic univariate (one variable at a time) analysis. We start by describing the distribution of values that each variable takes on. For nominal variables, this amounts to frequency tables and bar charts of how often each level of the variable appears in the data set. We’ll begin by exploring one of our nominal features, Heating_QC which categorizes the quality and condition of a home’s heating system. ggplot(data = ames) + geom_bar(mapping = aes(x = Heating_QC)) Figure 1.1: Distribution of Nominal Variable Heating_QC To summon the same information in tabular form, we can use the count() function to create a table: ames %&gt;% count(Heating_QC) ## # A tibble: 5 x 2 ## Heating_QC n ## &lt;fct&gt; &lt;int&gt; ## 1 Excellent 1495 ## 2 Fair 92 ## 3 Good 476 ## 4 Poor 3 ## 5 Typical 864 You’ll notice that very few houses (3) have heating system in Poor condition, and the majority of houses have systems rated Excellent. It will likely make sense to combine the categories of Fair and Poor in our eventual analysis, a decision we will revisit later. Next we create a histogram for an interval attribute like Sale_Price: ggplot(data = ames) + geom_histogram(mapping = aes(x = Sale_Price/1000)) + labs(x = &quot;Sales Price (Thousands $)&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Figure 1.2: Distribution of Interval Variable Sale_Price From this initial inspection, we can conclude that most of the houses sell for less than $200,000 and there are a number of expensive anomalies. There are a number of more concrete ways that we can describe and quantify a statistical distribution; statistics that describe the location, spread, and shape of the data. Location The location of a distribution refers to the x-axis of a histogram like that in Figure 1.2. Where is most of the data located? The sample mean, median, and mode are the most common statistics of location, but percentiles and the interquartile range can also be seen in this light. We define each of these terms below for a variable \\(\\mathbf{x}\\) having n observations with values \\(\\{x_i\\}_{i=1}^n\\), sorted in order of magnitude such that \\(x_1 \\leq x_2 \\leq \\dots \\leq x_n\\): Mean: The average of the observations, \\(\\frac{1}{n}\\sum_{i=1}^n x_i\\) Median: The “middle value” of the data. Formally, when \\(n\\) is odd, the median is the observation value, \\(x_m = x_{\\frac{(n+1)}{2}}\\) for which \\(x_i &lt; x_m\\) for 50% of the observations (excluding \\(x_m\\)). When \\(n\\) is even, \\(x_m\\) is the average of \\(x_\\frac{n}{2}\\) and \\(x_{(\\frac{n}{2}+1)}\\). The median is also known as the \\(2^{nd}\\) quartile. Mode: The most commonly occurring value in the data. Most commonly used to describe nominal attributes. Percentiles: The 99 intermediate values of the data which divide the observations into 100 equally-sized groups. The \\(r^{th}\\) percentile of the data, \\(P_r\\) is the number for which \\(r\\%\\) of the data is less than \\(P_r\\). Quartiles: The quartiles of a variable are the \\(25^{th}\\), \\(50^{th}\\), and \\(75^{th}\\) percentiles. They are denoted as \\(Q_1\\) (\\(1^{st}\\) quartile), \\(Q_2\\) (\\(2^{nd}\\) quartile = median), and \\(Q_3\\) (\\(3^{rd}\\) quartile) respectively. Illustrative Example Suppose the following table contains the heights of 10 students randomly sampled from NC State’s campus. Compute the mean, median, mode and quartiles of this variable. height 60 62 63 65 67 67 67 68 68 69 Solution: The mean is (60+62+63+65+67+67+67+68+68+69)/10 = 65.6. The median (second quartile) is (67+67)/2 = 67. The mode is 67. The first quartile is (62+63)/2 = 62.5 The third quartile is (68+68)/2 = 68 Spread Once we have an understanding of where the bulk of the data is located, we move on to describing the spread (the dispersion or variation) of the data. Range, interquartile range, variance, and standard deviation are all statistics that describe spread. Range: The difference between the maximum and minimum data values. Interquartile range (IQR): The difference between the $25^{th} and 75^{th} percentiles. Sample variance: The sum of squared differences between each data point and the mean, divided by (n-1). \\(\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2\\) Standard deviation: The square root of the sample variance. One should note that standard deviation is more frequently reported than variance because it shares the same units as the original data, and because of the guidance provided by the empirical rule. If we’re exploring something like Sale_Price, which has the unit “dollars”, then the variance would be measured in “square-dollars”, which hampers the intuition. Standard deviation, on the other hand, would share the unit “dollars”, aiding our fundamental understanding. Illustrative Example Let’s again use the table of heights, this time computing the range, IQR, sample variance and standard deviation. height 60 62 63 65 67 67 67 68 68 69 Solution: (60+62+63+65+67+67+67+68+68+69)/10 The range 69-60 = 9. The IQR is 68 - 62.5 = 5.5. The variance is ((60-65.6)^2+(62-65.6)^2+(63-65.6)^2+(65-65.6)^2+(67-65.6)^2+(67-65.6)^2+(67-65.6)^2+(68-65.6)^2+(68-65.6)^2+(69-65.6)^2)/9 = 8.933 The standard deviation is sqrt(8.933) = 2.989 Shape The final description we will want to give to distributions regards their shape. Is the histogram symmetric? Is it unimodal (having a single large “heap” of data) or multimodal (having multiple heaps\")? Does it have a longer tail on one side than the other (skew)? Is there a lot more or less data in the tails than you might expect? We’ll formalize these ideas with some illustrations. A distribution is right (left) skewed if it has a longer tail on its right (left) side, as shown in Figure 1.3. Figure 1.3: Examples of Left-Skewed (Negative Skew) and Right-skewed (Positive Skew) distributions respectively A distribution is called bimodal if it has two “heaps”, as shown in Figure 1.4. Figure 1.4: Example of a Bimodal Distribution Summary Functions There are many ways to obtain all of the statistics described in the preceding sections, below we highlight 3: The describe function from the Hmisc package which can work on the entire dataset or a subset of columns. install.packages(&#39;Hmisc&#39;) library(Hmisc) Hmisc::describe(ames$Sale_Price) ## ames$Sale_Price ## n missing distinct Info Mean Gmd .05 .10 ## 2930 0 1032 1 180796 81960 87500 105450 ## .25 .50 .75 .90 .95 ## 129500 160000 213500 281242 335000 ## ## lowest : 12789 13100 34900 35000 35311, highest: 611657 615000 625000 745000 755000 The tidyverse summarise function, in this case obtaining statistics for each House_Style separately. ames %&gt;% group_by(`House_Style`) %&gt;% summarise(mean = mean(Sale_Price), sd = sd(Sale_Price), max = max(Sale_Price), min = min(Sale_Price)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 8 x 5 ## House_Style mean sd max min ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 One_and_Half_Fin 137530. 47226. 410000 37900 ## 2 One_and_Half_Unf 109663. 20570. 139400 64500 ## 3 One_Story 178700. 81067. 615000 12789 ## 4 SFoyer 143473. 31220. 224500 70000 ## 5 SLvl 165527. 34348. 345000 91000 ## 6 Two_and_Half_Fin 220000 118212. 475000 104000 ## 7 Two_and_Half_Unf 177158. 76115. 415000 97500 ## 8 Two_Story 206990. 85350. 755000 40000 The base R summary function, which can work on the entire dataset or an individual variable summary(ames$Sale_Price) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 12789 129500 160000 180796 213500 755000 1.3.1 Normal Distribution The normal distribution, also known as the Gaussian distribution, is one of the most fundamental concepts in statistics. It is one that arises naturally out of many applications and settings. The normal distribution has the following characteristics: Symmetric Fully defined by mean and standard deviation (equivalently, variance) Bell-shaped/Unimodal Mean = Median = Mode Assymptotic to the x-axis (theoretical bounds are \\(-\\infty\\) to \\(\\infty\\)) Much of the normal distributions utility can be summarized in the empirical rule, which states that: 68% of data in normal distribution lies within 1 standard deviation of the mean. 95% of data in normal distribution lies within 2 standard deviations of the mean. 99% of data in normal distribution lies within 3 standard deviations of the mean. We can thus conclude that observations found outside of 3 standard deviations from the mean are quite rare, expected less than 1% of the time. Skewness Skewness is a statistic that describes the symmetry (or lack thereof) of a distribution. A normal distribution is perfectly symmetric and has a skewness of 0. Distributions that are more right skewed will have positive values of skewness whereas distributions that are more left skewed will have negative values of skewness. Kurtosis Kurtosis is a statistic that describes the tailedness of a distribution. The normal distribution has a kurtosis of 3. Distributions that are more tailed (leptokurtic or heavy-tailed) will have kurtosis values greater than 3 whereas distributions that are more less tailed (platykurtic or thin-tailed) will have values of kurtosis less than 3. For this reason, kurtosis is often reported in the form of excess kurtosis which is the raw kurtosis value minus 3. This is meant as a comparison to the normal distribution so that positive values indicate thicker tails and negative values indicate thinner tails than the normal. In Figure 1.5 below, we compare classical examples of leptokurtic and platykurtic distributions to a normal distribution with the same mean and variance. Figure 1.5: The Laplace distribution (top left) is leptokurtic because it has more data in its tails than the normal distribution with the same mean and variance. The uniform distribution (top right) is platykurtic because it has less data in its tails than the normal distribution with the same mean and variance (it effectively has no tails). Graphical Displays of Distributions There are three types of plots for examining the distribution of your data values: Histograms Normal Probability Plots (QQ-plots) Box Plots Histograms were previously discussed, so we move on to probability plots (QQ-plots) and box plots. Normal probability plots (QQ Plots) A normal probability plot graphs the sorted data values against the values that one would expect if the same number of observations came from a theoretical normal distribution. The resulting image would look close to a straight line if the data was generated by a normal distribution. Strong deviations from a straight line indicate that the data distribution is not normal. Figure 1.6 shows a QQ plot for Sale_Price, and we can conclude that the variable is not normally distributed. ggplot(data = ames, aes(sample = Sale_Price/1000)) + stat_qq() + stat_qq_line() Figure 1.6: QQ-Plot: Quantiles of Sale_Price vs. quantiles of a theoretical normal distribution with same mean and standard deviation. Conclusion: Sale_Price is not normally distributed. There are two main patterns that we expect to find when examining QQ-plots. One is quadratic shape, as seen in Figure 1.6. This pattern indicates a deviation from normality due to skewness to the data. The other is an S-shape (or cubic shape), as seen in Figure 1.7. This pattern indicates deviation from normality due to kurtosis. df &lt;- data.frame(j1 = rlaplace(10000,0,1)) ggplot(data = df, aes(sample=j1)) + stat_qq() + stat_qq_line() Figure 1.7: QQ-Plot: Quantiles of the Laplace distribution vs. quantiles of a theoretical normal distribution with same mean and standard deviation. Conclusion: Sale_Price is not normally distributed. 1.4 Confidence Intervals 1.5 Hypothesis Testing 1.6 Two-Sample t-tests "],["slr.html", "Chapter 2 Introduction to ANOVA and Linear Regression 2.1 Explanation vs. Prediction 2.2 Exploratory Data Analysis 2.3 One-Way ANOVA 2.4 ANOVA Post-hoc Testing 2.5 Pearson Correlation 2.6 Simple Linear Regression", " Chapter 2 Introduction to ANOVA and Linear Regression 2.1 Explanation vs. Prediction 2.2 Exploratory Data Analysis 2.3 One-Way ANOVA 2.4 ANOVA Post-hoc Testing 2.5 Pearson Correlation 2.6 Simple Linear Regression Some sample data: set.seed(42) data &lt;- data.frame(y = c(rnorm(10,2), rnorm(10,1),rnorm(10,0)), x = rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 10)) head(data) The mean of each group: (group.means = tapply(data$y, data$x, mean)) The grand mean: mean(group.means) To specify reference coding: lm(a ~ b, x, contrasts = list(b = contr.sum)) Note that the parameter estimates reflect differences from the grand mean. "],["complex-anova-and-multiple-linear-regression.html", "Chapter 3 Complex ANOVA and Multiple Linear Regression 3.1 Two-Way ANOVA 3.2 Two-Way ANOVA with Interactions 3.3 Randomized Block Design 3.4 Multiple Linear Regression", " Chapter 3 Complex ANOVA and Multiple Linear Regression 3.1 Two-Way ANOVA 3.2 Two-Way ANOVA with Interactions 3.3 Randomized Block Design 3.4 Multiple Linear Regression "],["model-building-scoring-for-prediction.html", "Chapter 4 Model Building &amp; Scoring for Prediction 4.1 Model Complexity 4.2 Regularized Regression 4.3 Modeling Scoring 4.4 Model Metrics", " Chapter 4 Model Building &amp; Scoring for Prediction 4.1 Model Complexity 4.2 Regularized Regression 4.3 Modeling Scoring 4.4 Model Metrics "],["model-selection.html", "Chapter 5 Model Selection 5.1 Stepwise Selection 5.2 Selection Criteria 5.3 Significance Levels", " Chapter 5 Model Selection 5.1 Stepwise Selection Forward Backward Stepwise LASSO 5.2 Selection Criteria 5.3 Significance Levels "],["diagnostics.html", "Chapter 6 Diagnostics 6.1 Examining Residuals 6.2 Misspecified Model 6.3 Constant Variance 6.4 Normality 6.5 Correlated Errors 6.6 Influential Observations and Outliers 6.7 Multicollinearity", " Chapter 6 Diagnostics 6.1 Examining Residuals 6.2 Misspecified Model 6.3 Constant Variance 6.4 Normality 6.5 Correlated Errors 6.6 Influential Observations and Outliers 6.7 Multicollinearity "],["categorical-data-analysis.html", "Chapter 7 Categorical Data Analysis 7.1 Describing Categorical Data 7.2 Tests of Association 7.3 Measures of Association 7.4 Introduction to Logistic Regression 7.5 Adding Categorical Variables and Interactions", " Chapter 7 Categorical Data Analysis 7.1 Describing Categorical Data 7.2 Tests of Association 7.3 Measures of Association 7.4 Introduction to Logistic Regression 7.5 Adding Categorical Variables and Interactions "]]
