[["categorical-data-analysis.html", "Chapter 7 Categorical Data Analysis 7.1 Describing Categorical Data 7.2 Tests of Association 7.3 Measures of Association 7.4 Introduction to Logistic Regression", " Chapter 7 Categorical Data Analysis Everything analysis covered so far has used a continuous variable as a target variable of interest. What if our target variable was categorical instead of continuous? Our analysis must change to adjust. This Chapter aims to answer the following questions: How do you explore categorical variables? Nominal vs. Ordinal Tests of Association Measures of Association How do you model a categorical target variable? Logistic Regression Interpreting Logistic Regression Assessing Logistic Regression 7.1 Describing Categorical Data We need to first explore our data before building any models to try and explain/predict our categorical target variable. With categorical variables, we can look at the distribution of the categories as well as see if this distribution has any association with other variables. For this analysis we are going to still use our Ames housing data. Imagine you worked for a real estate agency and got a bonus check if you sold a house above $175,000 in value. Let’s create this variable in our data: library(dplyr) train &lt;- train %&gt;% mutate(Bonus = ifelse(Sale_Price &gt; 175000, 1, 0)) You are interested in what variables might be associated with obtaining a higher chance of getting a bonus (selling a house above $175,000). An association exists between two categorical variables if the distribution of one variable changes when the value of the other categorical changes. If there is no association, the distribution of the first variable is the same regardless of the value of the other variable. For example, if we wanted to know if obtaining a bonus on selling a house in Ames, Iowa was associated with whether the house had central air we could look at the distribution of bonus eligible houses. If we observe that 42% of homes with central air are bonus eligible and 42% of homes without central air are bonus eligible, then it appears that central air has no bearing on whether the home is bonus eligible. However, if instead we observe that only 3% of homes without central air are bonus eligible, but 44% of home with central air are bonus eligible, then it appears that having central air might be related to a home being bonus eligible. To understand the distribution of categorical variables we need to look at frequency tables. A frequency table shows the number of observations that occur in certain categories or intervals. A one way frequency table examines all the categories of one variable. These are easily visualized with bar charts. Let’s look at the distribution of both bonus eligibility and central air using the table function. The ggplot function with the geom_bar function allows us to view our data in a bar chart. table(train$Bonus) ## ## 0 1 ## 1168 883 ggplot(data = train) + geom_bar(mapping = aes(x = Bonus)) table(train$Central_Air) ## ## N Y ## 135 1916 ggplot(data = train) + geom_bar(mapping = aes(x = Central_Air)) Frequency tables show single variables, but if we want to explore two variables together we look at cross-tabulation tables. A cross-tabulation table shows the number of observations for each combination of the row and column variables. Let’s again examine bonus eligibility, but this time across levels of central air. Again, we can use the table function. The prop.table function allows us to compare two variables in terms of proportions instead of frequencies. table(train$Central_Air, train$Bonus) ## ## 0 1 ## N 130 5 ## Y 1038 878 prop.table(table(train$Central_Air, train$Bonus)) ## ## 0 1 ## N 0.063383715 0.002437835 ## Y 0.506094588 0.428083862 ggplot(data = train) + geom_bar(mapping = aes(x = Bonus, fill = Central_Air)) From the above output we can see that 147 homes have no central air with only 5 of them being bonus eligible. However, there are 1904 homes that have central air with 835 of them being bonus eligible. For an even more detailed breakdown we can use the CrossTable function. library(gmodels) CrossTable(train$Central_Air, train$Bonus) ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 2051 ## ## ## | train$Bonus ## train$Central_Air | 0 | 1 | Row Total | ## ------------------|-----------|-----------|-----------| ## N | 130 | 5 | 135 | ## | 36.704 | 48.551 | | ## | 0.963 | 0.037 | 0.066 | ## | 0.111 | 0.006 | | ## | 0.063 | 0.002 | | ## ------------------|-----------|-----------|-----------| ## Y | 1038 | 878 | 1916 | ## | 2.586 | 3.421 | | ## | 0.542 | 0.458 | 0.934 | ## | 0.889 | 0.994 | | ## | 0.506 | 0.428 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 1168 | 883 | 2051 | ## | 0.569 | 0.431 | | ## ------------------|-----------|-----------|-----------| ## ## The advantage of the CrossTable function is that we can easily get not only the frequencies, but the cell, row, and column proportions. For example, the third number in each cell gives us the row proportion. For homes without central air, 96.6% of them are not bonus eligible, while 3.4% of them are. For homes with central air, 56.1% of the homes are not bonus eligible, while 43.9% of them are. This would appear that the distribution of bonus eligible homes changes across levels of central air - a relationship between the two variables. This expected relationship needs to be tested statistically for verification. 7.1.1 Python Code train[&#39;Bonus&#39;] = np.where(train[&#39;Sale_Price&#39;] &gt; 175000, 1, 0) train[&#39;Bonus&#39;].value_counts() ## 0 1168 ## 1 883 ## Name: Bonus, dtype: int64 ax = sns.countplot(x = &quot;Bonus&quot;, data = train, color = &quot;blue&quot;) ax.set(xlabel = &#39;Bonus Eligible&#39;, ylabel = &#39;Frequency&#39;, title = &#39;Bar Graph of Bonus Eligibility&#39;) plt.show() train[&#39;Central_Air&#39;].value_counts() ## Y 1916 ## N 135 ## Name: Central_Air, dtype: int64 plt.cla() ax = sns.countplot(x = &quot;Central_Air&quot;, data = train, color = &quot;blue&quot;) ax.set(xlabel = &#39;Central Air&#39;, ylabel = &#39;Frequency&#39;, title = &#39;Bar Graph of Central Air Availability&#39;) plt.show() plt.cla() ax = sns.countplot(x = &quot;Bonus&quot;, data = train, hue = &quot;Central_Air&quot;) ax.set(xlabel = &#39;Central Air&#39;, ylabel = &#39;Frequency&#39;, title = &#39;Bar Graph of Central Air Availability&#39;) plt.show() pd.crosstab(index = train[&#39;Central_Air&#39;], columns = train[&#39;Bonus&#39;]) ## Bonus 0 1 ## Central_Air ## N 130 5 ## Y 1038 878 7.2 Tests of Association Much like in Chapter 2 we have statistical tests to evaluate relationships between two categorical variables. The null hypothesis for these statistical tests is that the two variables have no association - the distribution of one variable does not change across levels of another variable. The alternative hypothesis is an association between the two variables - the distribution of one variable changes across levels of another variable. These statistical tests follow a \\(\\chi^2\\)-distribution. The \\(\\chi^2\\)-distribution is a distribution that has the following characteristics: Bounded below by 0 Right-skewed One set of degrees of freedom A plot of a variety of \\(\\chi^2\\)-distributions is shown here: Two common \\(\\chi^2\\) tests are the Pearson and Likelihood Ratio \\(\\chi^2\\) tests. They compare the observed count of observations in each cell of a cross-tabulation table between two variables to their expected count if there was no relationship. The expected cell count applies the overall distribution of one variable across all the levels of the other variable. For example, overall 59% of all homes are not bonus eligible. If that were to apply to every level of central air, then the 140 homes without central air would be expected to have 86.73 ( $ = 147 $ ) of them would be bonus eligible while 60.27 ( $ = 147 $ ) of them would not be bonus eligible. We actually observe 142 and 5 homes for each of these categories respectively. The further the observed data is from the expected data, the more evidence we have that there is a relationship between the two variables. The test statistic for the Pearson \\(\\chi^2\\) test is the following: \\[ \\chi^2_P = \\sum_{i=1}^R \\sum_{j=1}^C \\frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}} \\] From the equation above, the closer that the observed count of each cross-tabulation table cell to the expected count, the smaller the test statistic. As with all previous hypothesis tests, the smaller the test statistic, the larger the p-value, implying less evidence for the alternative hypothesis. Let’s examine the relationship between central air and bonus eligibility using the chisq.test function. chisq.test(table(train$Central_Air, train$Bonus)) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: table(train$Central_Air, train$Bonus) ## X-squared = 89.552, df = 1, p-value &lt; 2.2e-16 The above results shows an extremely small p-value that is below any reasonable significance level. This implies that we have statistical evidence for a relationship between having central air and bonus eligibility of homes. The p-value comes from a \\(\\chi^2\\)-distribution with degrees of freedom that equal the product of the number of rows minus one and the number of columns minus one. Another common test is the Likelihood Ratio test. The test statistic for this is the following: \\[ \\chi^2_L = 2 \\times \\sum_{i=1}^R \\sum_{j=1}^C Obs_{i,j} \\times \\log(\\frac{Obs_{i,j}}{Exp_{i,j}}) \\] The p-value comes from a \\(\\chi^2\\)-distribution with degrees of freedom that equal the product of the number of rows minus one and the number of columns minus one. Both of the above tests have a sample size requirement. The sample size requirement is 80% or more of the cells in the cross-tabulation table need expected count larger than 5. For smaller sample sizes, this might be hard to meet. In those situations, we can use a more computationally expensive test called Fisher’s exact test. This test calculates every possible permutation of the data being evaluated to calculate the p-value without any distributional assumptions. To perform this test we can use the fisher.test function. fisher.test(table(train$Central_Air, train$Bonus)) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table(train$Central_Air, train$Bonus) ## p-value &lt; 2.2e-16 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 9.115466 69.125214 ## sample estimates: ## odds ratio ## 21.97444 We see the same results as with the Pearson test because the assumptions were met for sample size. Both the Pearson and Likelihood Ratio \\(\\chi^2\\) tests can handle any type of categorical variable either ordinal, nominal, or both. However, ordinal variables provide us extra information since the order of the categories actually matters compared to nominal. We can test for even more with ordinal variables against other ordinal variables whether two ordinal variables have a linear relationship as compared to just a general one. An ordinal test for association is the Mantel-Haenszel \\(\\chi^2\\) test. The test statistic for the Mantel-Haenszel \\(\\chi^2\\) test is the following: \\[ \\chi^2_{MH} = (n-1)r^2 \\] where \\(r^2\\) is the Pearson correlation between the column and row variables. This test follows a \\(\\chi^2\\)-distribution with only one degree of freedom. Since both the central air and bonus eligibility variables are binary, they are ordinal. Since they are both ordinal, we should use the Mantel-Haenszel \\(\\chi^2\\) test with the CMHtest function. In the main output table, the first row is the Mantel-Haenszel \\(\\chi^2\\) test. library(vcdExtra) CMHtest(table(train$Central_Air, train$Bonus))$table[1,] ## Chisq Df Prob ## 9.121694e+01 1.000000e+00 1.287531e-21 From here we can see another extremely small p-value as we saw in earlier, more general \\(\\chi^2\\) tests. 7.2.1 Python Code from scipy.stats import chi2_contingency chi2_contingency(pd.crosstab(index = train[&#39;Central_Air&#39;], columns = train[&#39;Bonus&#39;]), correction = True) ## (89.55151374239, 2.987571259286868e-21, 1, array([[ 76.87957094, 58.12042906], ## [1091.12042906, 824.87957094]])) from scipy.stats import fisher_exact fisher_exact(pd.crosstab(index = train[&#39;Central_Air&#39;], columns = train[&#39;Bonus&#39;])) ## (21.992292870905587, 6.316364105830547e-27) No real Mantel-Haenszel options in Python that work for anything more than a 2x2 table so I wouldn’t trust them. 7.3 Measures of Association Tests of association are best designed for just that, testing the existence of an association between two categorical variables. However, just like we saw in Chapter 1.1, hypothesis tests are impacted by sample size. When we have the same sample size, tests of association can rank significance of variables with p-values. However, when sample sizes are not the same between two tests, the tests of association are not best for comparing the strength of an association. In those scenarios, we have measures of strength of association that can be compared across any sample size. Measures of association were not designed to test if an association exists, as that is what statistical testing is for. They are designed to measure the strength of association. There are dozens of these measures. Three of the most common are the following: Odds Ratios (only for comparing two binary variables) Cramer’s V (able to compare nominal variables with any number of categories) Spearman’s Correlation (able to compare ordinal variables with any number of categories) An odds ratio indicates how much more likely, with respect to odds, a certain event occurs in one group relative to its occurrence in another group. The odds of an event occurring is not the same as the probability that an event occurs. The odds of an event occurring is the probability the event occurs divided by the probability that event does not occur. \\[ Odds = \\frac{p}{1-p} \\] Let’s again examine the cross-tabulation table between central air and bonus eligibility. ## ## ## Cell Contents ## |-------------------------| ## | N | ## | Chi-square contribution | ## | N / Row Total | ## | N / Col Total | ## | N / Table Total | ## |-------------------------| ## ## ## Total Observations in Table: 2051 ## ## ## | train$Bonus ## train$Central_Air | 0 | 1 | Row Total | ## ------------------|-----------|-----------|-----------| ## N | 130 | 5 | 135 | ## | 36.704 | 48.551 | | ## | 0.963 | 0.037 | 0.066 | ## | 0.111 | 0.006 | | ## | 0.063 | 0.002 | | ## ------------------|-----------|-----------|-----------| ## Y | 1038 | 878 | 1916 | ## | 2.586 | 3.421 | | ## | 0.542 | 0.458 | 0.934 | ## | 0.889 | 0.994 | | ## | 0.506 | 0.428 | | ## ------------------|-----------|-----------|-----------| ## Column Total | 1168 | 883 | 2051 | ## | 0.569 | 0.431 | | ## ------------------|-----------|-----------|-----------| ## ## Let’s look at the row without central air. The probability that a home without central air is not bonus eligible is 96.6%. That implies that the odds of not being bonus eligible in homes without central air is 28.41 (= 0.966/0.034). For homes with central air, the odds of not being bonus eligible are 1.28 (= 0.561/0.439). The odds ratio between these two would be approximately 22.2 (= 28.41/1.28). In other words, homes without central air are 22.2 times more likely (in terms of odds) to not be bonus eligible as compared to homes with central air. This relationship is intuitive based on the numbers we have seen. Without going into details, it can also be shown that homes with central air are 22.2 times as likely (in terms of odds) to be bonus eligible. We can use the OddsRatio function to get these same results. library(DescTools) OddsRatio(table(train$Central_Air, train$Bonus)) ## [1] 21.99229 Cramer’s V is another measure of strength of association. Cramer’s V is calculated as follows: \\[ V = \\sqrt{\\frac{\\chi^2_P/n}{\\min(Rows-1, Columns-1)}} \\] Cramer’s V is bounded between 0 and 1 for every comparison other than two binary variables. For two binary variables being compared the bounds are -1 to 1. The idea is still the same for both. The further the value is from 0, the stronger the relationship. Unfortunately, unlike \\(R^2\\), Cramer’s V has no interpretative value. It can only be used for comparison. We use the assocstats function to get the Cramer’s V value. This function also provides the Pearson and Likelihood Ratio \\(\\chi^2\\) tests as well. assocstats(table(train$Central_Air, train$Bonus)) ## X^2 df P(&gt; X^2) ## Likelihood Ratio 118.025 1 0 ## Pearson 91.261 1 0 ## ## Phi-Coefficient : 0.211 ## Contingency Coeff.: 0.206 ## Cramer&#39;s V : 0.211 Lastly, we have Spearman’s correlation. Much like the Mantel-Haenszel test of association was specifically designed for comparing two ordinal variables, Spearman correlation measures the strength of association between two ordinal variables. Spearman is not limited to only categorical data analysis as it was also seen back in Chapter 5 with detecting heteroskedasticity. Remember, Spearman correlation is a correlation on the ranks of the observations as compared to the actual values of the observations. The cor.test function that gave us Pearson’s correlation also provides Spearman’s correlation. cor.test(x = as.numeric(ordered(train$Central_Air)), y = as.numeric(ordered(train$Bonus)), method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 1134632325, p-value &lt; 2.2e-16 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.2109409 As previously mentioned, these are only a few of the dozens of different measures of association that exist. However, they are the most used ones. 7.3.1 Python Code Odds Ratios are the statistic calculated from the Fisher’s Exact test from the previous code: from scipy.stats import fisher_exact fisher_exact(pd.crosstab(index = train[&#39;Central_Air&#39;], columns = train[&#39;Bonus&#39;])) ## (21.992292870905587, 6.316364105830547e-27) from scipy.stats import fisher_exact fisher_exact(pd.crosstab(index = train[&#39;Central_Air&#39;], columns = train[&#39;Bonus&#39;])) ## (21.992292870905587, 6.316364105830547e-27) from scipy.stats.contingency import association association(pd.crosstab(index = train[&#39;Central_Air&#39;], columns = train[&#39;Bonus&#39;]), method = &quot;cramer&quot;) ## 0.21094091464325487 from scipy.stats import spearmanr spearmanr(train[&#39;Central_Air&#39;], train[&#39;Bonus&#39;]) ## SpearmanrResult(correlation=0.2109409146432549, pvalue=4.631215546188166e-22) ## ## C:\\PROGRA~3\\ANACON~1\\lib\\site-packages\\scipy\\stats\\_stats_py.py:112: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored. ## warnings.warn(&quot;The input array could not be properly &quot; 7.4 Introduction to Logistic Regression After exploring the categorical target variable, we can move on to modeling the categorical target variable. Logistic regression is a fundamental statistical analysis for data science and analytics. It part of a class of modeling techniques known as classification models since they are trying to predict categorical target variables. This target variable can be binary, ordinal, or even nominal in its structure. The primary focus will be binary logistic regression. It is the most common type of logistic regression, and sets up the foundation for both ordinal and nominal logistic regression. Ordinary least squares regression is not the best approach to modeling categorical target variables. Mathematically, it can be shown that with a binary target variable coded as 0 and 1, an OLS linear regression model will produce the linear probability model. 7.4.1 Linear Probability Model The linear probability model is not as widely used since probabilities do not tend to follow the properties of linearity in relation to their predictors. Also, the linear probability model possibly produces predictions outside of the bounds of 0 and 1 (where probabilities should be!). For completeness sake however, here is the linear probability model using the lm function to try and predict bonus eligibility. lp.model &lt;- lm(Bonus ~ Gr_Liv_Area, data = train) with(train, plot(x = Gr_Liv_Area, y = Bonus, main = &#39;OLS Regression?&#39;, xlab = &#39;Greater Living Area (Sqft)&#39;, ylab = &#39;Bonus Eligibility&#39;)) abline(lp.model) Even though it doesn’t appear to really look like our data, let’s fit this linear probability model anyway for completeness sake. lp.model &lt;- lm(Bonus ~ Gr_Liv_Area, data = train) summary(lp.model) ## ## Call: ## lm(formula = Bonus ~ Gr_Liv_Area, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.7306 -0.2993 -0.1011 0.4069 0.8567 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.059e-01 2.847e-02 -14.26 &lt;2e-16 *** ## Gr_Liv_Area 5.559e-04 1.794e-05 30.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4088 on 2049 degrees of freedom ## Multiple R-squared: 0.319, Adjusted R-squared: 0.3187 ## F-statistic: 959.9 on 1 and 2049 DF, p-value: &lt; 2.2e-16 qqnorm(rstandard(lp.model), ylab = &quot;Standardized Residuals&quot;, xlab = &quot;Normal Scores&quot;, main = &quot;QQ-Plot of Residuals&quot;) qqline(rstandard(lp.model)) plot(predict(lp.model), resid(lp.model), ylab=&quot;Residuals&quot;, xlab=&quot;Predicted Values&quot;, main=&quot;Residuals of Linear Probability Model&quot;) abline(0, 0) As we can see from the charts above, the assumptions of ordinary least squares don’t really hold in this situation. Therefore, we should be careful interpreting the results of the model. Maybe a better model won’t have these problems? 7.4.2 Binary Logistic Regression Due to the limitations of the linear probability model, people typically just use the binary logistic regression model. The logistic regression model does not have the limitations of the linear probability model. The outcome of the logistic regression model is the probability of getting a 1 in a binary variable. That probability is calculated as follows: \\[ p_i = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i})}} \\] This function has the desired properties for predicting probabilities. The predicted probability from the above equation will always be between 0 and 1. The parameter estimates do not enter the function linearly (this is a non-linear regression model), and the rate of change of the probability varies as the predictor variables vary as seen in Figure 7.1. Figure 7.1: Example of a Logistic Curve To create a linear model, a link function is applied to the probabilities. The specific link function for logistic regression is called the logit function. \\[ logit(p_i) = \\log(\\frac{p_i}{1-p_i}) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i} \\] The relationship between the predictor variables and the logits are linear in nature as the logits themselves are unbounded. This structure looks much more like our linear regression model structure. However, logistic regression does not use OLS to estimate the coefficients in our model. OLS requires residuals which the logistic regression model does not provide. The target variable is binary in nature, but the predictions are probabilities. Therefore, we cannot calculate a traditional residual. Instead, logistic regression uses maximum likelihood estimation. This is not covered here. There are two main assumptions for logistic regression: Independence of observations Linearity of the logit The first assumption of independence is the same as we had for linear regression. The second assumption implies that the logistic function transformation (the logit) actually makes a linear relationship with our predictor variables. This assumption can be tested, but will not be covered in this brief introduction to logistic regression. Let’s build a logistic regression model. We will use the glm function to do this. The glm function has a similar structure to the lm function. The main difference is the family = binomial(link = \"logit\") option to specify that we are uses a logistic regression model. Again, there are many different link functions, but only the logistic link function (the logit) is being used here. ames_logit &lt;- glm(Bonus ~ Gr_Liv_Area, data = train, family = binomial(link = &quot;logit&quot;)) summary(ames_logit) ## ## Call: ## glm(formula = Bonus ~ Gr_Liv_Area, family = binomial(link = &quot;logit&quot;), ## data = train) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.9299285 0.2711751 -21.87 &lt;2e-16 *** ## Gr_Liv_Area 0.0037615 0.0001778 21.16 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2803.6 on 2050 degrees of freedom ## Residual deviance: 1974.0 on 2049 degrees of freedom ## AIC: 1978 ## ## Number of Fisher Scoring iterations: 5 Let’s examine the above output. Scanning down the output, you can see the actual logistic regression equation for the variable Gr_Liv_Area. Here we can see that it appears to be a significant variable at predicting bonus eligibility. However, the coefficient reported does not have the same usable interpretation as in linear regression. An increase of one unit of greater living area square footage is linearly related to the logit not the probability of bonus eligibility. We can transform this coefficient to make it more interpretable. A single unit increase in greater living area square footage does have a \\(100 \\times (e^\\hat{\\beta}-1)\\%\\) increase in the average odds of bonus eligibility. We can use a combination of the exp and coef functions to obtain this number. 100*(exp(cbind(coef(ames_logit), confint(ames_logit)))-1) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -99.7341328 -99.8457566 -99.5532392 ## Gr_Liv_Area 0.3768579 0.3427238 0.4127002 In other words, every additional square foot in greater living area in the home leads to an average increase in odds of 0.385% to be bonus eligible. 7.4.3 Adding Categorical Variables Similar to linear regression as we learned in Chapter ??, logistic regression can have both continuous and categorical predictors for our categorical target variable. Let’s add both central air as well as number of fireplaces to our logistic regression model. ames_logit2 &lt;- glm(Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), data = train, family = binomial(link = &quot;logit&quot;)) summary(ames_logit2) ## ## Call: ## glm(formula = Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), ## family = binomial(link = &quot;logit&quot;), data = train) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.009e+01 6.845e-01 -14.746 &lt; 2e-16 *** ## Gr_Liv_Area 3.686e-03 2.011e-04 18.327 &lt; 2e-16 *** ## Central_AirY 3.841e+00 5.591e-01 6.870 6.4e-12 *** ## factor(Fireplaces)1 1.049e+00 1.241e-01 8.452 &lt; 2e-16 *** ## factor(Fireplaces)2 5.996e-01 2.298e-01 2.609 0.00908 ** ## factor(Fireplaces)3 -2.909e-02 1.151e+00 -0.025 0.97983 ## factor(Fireplaces)4 9.064e+00 3.247e+02 0.028 0.97773 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2803.6 on 2050 degrees of freedom ## Residual deviance: 1778.1 on 2044 degrees of freedom ## AIC: 1792.1 ## ## Number of Fisher Scoring iterations: 11 Just like with linear regression, categorical predictor variables are a comparison between two categories. Again, the coefficients from the logistic regression model need to be transformed to be interpreted. 100*(exp(cbind(coef(ames_logit2), confint(ames_logit2)))-1) ## 2.5 % 97.5 % ## (Intercept) -99.995864 -99.999020 -99.9854303 ## Gr_Liv_Area 0.369311 0.330681 0.4098519 ## Central_AirY 4558.752989 1606.907138 15562.8101471 ## factor(Fireplaces)1 185.461129 123.956983 264.3565589 ## factor(Fireplaces)2 82.134665 16.129048 186.2369073 ## factor(Fireplaces)3 -2.866930 -89.674170 965.2093862 ## factor(Fireplaces)4 863747.190478 -100.000000 NA Let’s use the first fireplace variable as an example. A home with one fireplace has, on average, 167.04% higher odds of being bonus eligible as compared to a home with zero fireplaces. 7.4.4 Model Assessment There are dozens of different ways to evaluate a logistic regression model. We will cover one popular way here - concordance. Counting the number of concordant, discordant, and tied pairs is a way to to assess how well the model fits the data. To find concordant, discordant, and tied pairs, we must compare all of the 0’s in the target variable to all of the 1’s. For our example, we will compare every pair of homes where one home is bonus eligible and one is not (every 0 and 1 pair). A concordant pair is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has a higher predicted probability than the non-bonus eligible home (the 0 in our model) - our model successfully ordered these two observations by probability. It does not matter what the actual predicted probability values are as long as the bonus eligible home has a higher predicted probability than the non-bonus eligible home. A discordant pair is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has a lower predicted probability than the non-bonus eligible home (the 0 in our model) - our model unsuccessfully ordered the homes. It does not matter what the actual predicted probability values are as long as the bonus eligible home has a lower predicted probability than the non-bonus eligible home. A tied pair is a 0 and 1 pair where the bonus eligible home has the same predicted probability as the non-bonus eligible home - the model is confused and sees these two different things as the same. In general, you want a high percentage of concordant pairs and low percentages of discordant and tied pairs. We can use the Concordance function to obtain these values on our predictions from the predict function. library(survival) survival::concordance(ames_logit) ## Call: ## concordance.lm(object = ames_logit) ## ## n= 2051 ## Concordance= 0.8584 se= 0.00791 ## concordant discordant tied.x tied.y tied.xy ## 884991 145814 539 1069022 1909 From the above output we have a concordance of 86.3% for our model. There is no good or bad value as this can only be compared with another model to see which is better. Let’s compare this to our model with the categorical variables. survival::concordance(ames_logit2) ## Call: ## concordance.lm(object = ames_logit2) ## ## n= 2051 ## Concordance= 0.8819 se= 0.007135 ## concordant discordant tied.x tied.y tied.xy ## 909389 121737 218 1069852 1079 We can see that the model with categorical predictors added to it has a higher concordance at 88.4%. That implies that our model is correctly able to rank our observations 88.4% of the time. This is NOT the same thing as saying our model is 88.4% accurate. Accuracy (which is not covered here) deals with a prediction being correct or incorrect. Concordance is only measuring how often we are able to predict 1’s with higher probability than 0’s - again, correctly ranking the observations. 7.4.5 Variable Selection and Regularized Regression As with linear regression in Chapters ?? and ??, logistic regression uses the same approaches to doing variable selection. In fact, the same function are used as well. Let’s use the step function to apply a forward and backward selection to the logistic regression model. train_sel_log &lt;- train %&gt;% dplyr::select(Bonus, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %&gt;% replace(is.na(.), 0) full.model &lt;- glm(Bonus ~ . , data = train_sel_log) empty.model &lt;- glm(Bonus ~ 1, data = train_sel_log) for.model &lt;- step(empty.model, scope = list(lower = formula(empty.model), upper = formula(full.model)), direction = &quot;forward&quot;, k = log(dim(train_sel_log)[1])) ## Start: AIC=2946.83 ## Bonus ~ 1 ## ## Df Deviance AIC ## + Overall_Qual 9 238.05 1481.7 ## + Full_Bath 1 337.19 2134.8 ## + Gr_Liv_Area 1 342.43 2166.4 ## + Garage_Area 1 372.19 2337.4 ## + First_Flr_SF 1 396.47 2467.0 ## + Fireplaces 1 423.45 2602.0 ## + TotRms_AbvGrd 1 433.21 2648.7 ## + House_Style 7 447.93 2763.0 ## + Half_Bath 1 465.65 2796.8 ## + Second_Flr_SF 1 470.16 2816.6 ## + Central_Air 1 480.47 2861.1 ## + Lot_Area 1 485.78 2883.6 ## + Bldg_Type 4 484.94 2902.9 ## &lt;none&gt; 502.85 2946.8 ## + Street 1 502.46 2952.9 ## + Roof_Style 5 497.52 2963.1 ## ## Step: AIC=1481.67 ## Bonus ~ Overall_Qual ## ## Df Deviance AIC ## + Gr_Liv_Area 1 213.77 1268.7 ## + Full_Bath 1 214.91 1279.6 ## + First_Flr_SF 1 220.34 1330.8 ## + Garage_Area 1 225.43 1377.6 ## + Fireplaces 1 226.35 1386.0 ## + TotRms_AbvGrd 1 228.02 1401.0 ## + Lot_Area 1 228.53 1405.6 ## + Half_Bath 1 234.29 1456.7 ## + Bldg_Type 4 232.70 1465.6 ## + Second_Flr_SF 1 235.35 1465.9 ## + House_Style 7 230.67 1470.5 ## + Central_Air 1 236.56 1476.5 ## &lt;none&gt; 238.05 1481.7 ## + Street 1 237.98 1488.7 ## + Roof_Style 5 237.87 1518.3 ## ## Step: AIC=1268.65 ## Bonus ~ Overall_Qual + Gr_Liv_Area ## ## Df Deviance AIC ## + House_Style 7 200.44 1190.0 ## + Full_Bath 1 206.15 1201.8 ## + First_Flr_SF 1 207.39 1214.1 ## + Garage_Area 1 207.62 1216.5 ## + Second_Flr_SF 1 208.20 1222.1 ## + Fireplaces 1 209.85 1238.3 ## + Lot_Area 1 210.18 1241.5 ## + Central_Air 1 211.83 1257.6 ## + Bldg_Type 4 209.52 1258.0 ## + TotRms_AbvGrd 1 212.29 1262.1 ## &lt;none&gt; 213.77 1268.7 ## + Street 1 213.61 1274.8 ## + Half_Bath 1 213.76 1276.2 ## + Roof_Style 5 212.90 1298.4 ## ## Step: AIC=1189.97 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style ## ## Df Deviance AIC ## + Full_Bath 1 194.27 1133.5 ## + Garage_Area 1 197.27 1164.9 ## + Fireplaces 1 197.66 1169.0 ## + Lot_Area 1 198.26 1175.2 ## + Bldg_Type 4 196.28 1177.5 ## + Central_Air 1 199.56 1188.7 ## + TotRms_AbvGrd 1 199.56 1188.7 ## + Half_Bath 1 199.59 1188.9 ## &lt;none&gt; 200.44 1190.0 ## + First_Flr_SF 1 200.01 1193.2 ## + Second_Flr_SF 1 200.25 1195.7 ## + Street 1 200.28 1196.0 ## + Roof_Style 5 198.31 1206.3 ## ## Step: AIC=1133.51 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style + Full_Bath ## ## Df Deviance AIC ## + Bldg_Type 4 187.53 1091.6 ## + Fireplaces 1 189.82 1093.6 ## + Garage_Area 1 191.38 1110.4 ## + Half_Bath 1 191.55 1112.2 ## + Lot_Area 1 191.67 1113.5 ## + TotRms_AbvGrd 1 192.67 1124.1 ## + Central_Air 1 193.37 1131.7 ## &lt;none&gt; 194.27 1133.5 ## + First_Flr_SF 1 193.73 1135.5 ## + Second_Flr_SF 1 194.00 1138.3 ## + Street 1 194.07 1139.0 ## + Roof_Style 5 192.70 1155.0 ## ## Step: AIC=1091.58 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style + Full_Bath + ## Bldg_Type ## ## Df Deviance AIC ## + Fireplaces 1 184.20 1062.4 ## + Half_Bath 1 184.45 1065.2 ## + Garage_Area 1 185.54 1077.4 ## + Lot_Area 1 185.82 1080.5 ## + TotRms_AbvGrd 1 186.63 1089.3 ## + First_Flr_SF 1 186.70 1090.1 ## &lt;none&gt; 187.53 1091.6 ## + Second_Flr_SF 1 187.06 1094.1 ## + Central_Air 1 187.12 1094.8 ## + Street 1 187.28 1096.5 ## + Roof_Style 5 185.91 1111.9 ## ## Step: AIC=1062.44 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style + Full_Bath + ## Bldg_Type + Fireplaces ## ## Df Deviance AIC ## + Half_Bath 1 181.47 1039.5 ## + Garage_Area 1 182.28 1048.6 ## + Lot_Area 1 183.10 1057.8 ## + TotRms_AbvGrd 1 183.42 1061.4 ## &lt;none&gt; 184.20 1062.4 ## + First_Flr_SF 1 183.65 1064.0 ## + Second_Flr_SF 1 183.91 1066.9 ## + Central_Air 1 183.92 1067.0 ## + Street 1 184.00 1067.9 ## + Roof_Style 5 182.52 1081.8 ## ## Step: AIC=1039.49 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style + Full_Bath + ## Bldg_Type + Fireplaces + Half_Bath ## ## Df Deviance AIC ## + Garage_Area 1 179.78 1027.9 ## + Lot_Area 1 180.32 1034.1 ## + First_Flr_SF 1 180.42 1035.2 ## + Second_Flr_SF 1 180.78 1039.3 ## + TotRms_AbvGrd 1 180.78 1039.3 ## &lt;none&gt; 181.47 1039.5 ## + Street 1 181.27 1044.8 ## + Central_Air 1 181.39 1046.2 ## + Roof_Style 5 179.91 1059.9 ## ## Step: AIC=1027.87 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style + Full_Bath + ## Bldg_Type + Fireplaces + Half_Bath + Garage_Area ## ## Df Deviance AIC ## + Lot_Area 1 178.81 1024.5 ## + First_Flr_SF 1 178.93 1025.8 ## &lt;none&gt; 179.78 1027.9 ## + Second_Flr_SF 1 179.23 1029.2 ## + TotRms_AbvGrd 1 179.24 1029.4 ## + Street 1 179.64 1033.9 ## + Central_Air 1 179.77 1035.5 ## + Roof_Style 5 178.26 1048.7 ## ## Step: AIC=1024.46 ## Bonus ~ Overall_Qual + Gr_Liv_Area + House_Style + Full_Bath + ## Bldg_Type + Fireplaces + Half_Bath + Garage_Area + Lot_Area ## ## Df Deviance AIC ## &lt;none&gt; 178.81 1024.5 ## + First_Flr_SF 1 178.16 1024.6 ## + TotRms_AbvGrd 1 178.37 1027.0 ## + Second_Flr_SF 1 178.41 1027.4 ## + Street 1 178.77 1031.6 ## + Central_Air 1 178.81 1032.1 ## + Roof_Style 5 177.29 1045.1 back.model &lt;- step(full.model, scope = list(lower = formula(empty.model), upper = formula(full.model)), direction = &quot;backward&quot;, k = log(dim(train_sel_log)[1])) ## Start: AIC=1068.82 ## Bonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + ## Roof_Style + Central_Air + First_Flr_SF + Second_Flr_SF + ## Full_Bath + Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + ## TotRms_AbvGrd ## ## Df Deviance AIC ## - Roof_Style 5 177.51 1047.6 ## - Central_Air 1 176.05 1061.2 ## - Gr_Liv_Area 1 176.07 1061.4 ## - Street 1 176.11 1061.9 ## - Second_Flr_SF 1 176.19 1062.8 ## - First_Flr_SF 1 176.42 1065.4 ## - TotRms_AbvGrd 1 176.48 1066.2 ## - Lot_Area 1 176.66 1068.2 ## &lt;none&gt; 176.05 1068.8 ## - Garage_Area 1 177.19 1074.4 ## - House_Style 7 181.36 1076.4 ## - Fireplaces 1 178.18 1085.8 ## - Bldg_Type 4 180.60 1090.7 ## - Half_Bath 1 178.71 1091.9 ## - Full_Bath 1 187.54 1190.9 ## - Overall_Qual 9 220.95 1466.1 ## ## Step: AIC=1047.6 ## Bonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + ## Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath + ## Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd ## ## Df Deviance AIC ## - Central_Air 1 177.51 1040.0 ## - Gr_Liv_Area 1 177.54 1040.3 ## - Street 1 177.56 1040.5 ## - Second_Flr_SF 1 177.65 1041.5 ## - First_Flr_SF 1 177.89 1044.4 ## - TotRms_AbvGrd 1 177.96 1045.2 ## - Lot_Area 1 178.10 1046.8 ## &lt;none&gt; 177.51 1047.6 ## - House_Style 7 182.54 1051.5 ## - Garage_Area 1 178.67 1053.3 ## - Fireplaces 1 179.60 1064.0 ## - Bldg_Type 4 182.11 1069.6 ## - Half_Bath 1 180.25 1071.4 ## - Full_Bath 1 189.71 1176.3 ## - Overall_Qual 9 222.70 1444.1 ## ## Step: AIC=1039.97 ## Bonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + ## First_Flr_SF + Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + ## Garage_Area + Gr_Liv_Area + TotRms_AbvGrd ## ## Df Deviance AIC ## - Gr_Liv_Area 1 177.54 1032.7 ## - Street 1 177.56 1032.9 ## - Second_Flr_SF 1 177.65 1033.9 ## - First_Flr_SF 1 177.89 1036.7 ## - TotRms_AbvGrd 1 177.96 1037.5 ## - Lot_Area 1 178.10 1039.2 ## &lt;none&gt; 177.51 1040.0 ## - House_Style 7 182.56 1044.1 ## - Garage_Area 1 178.71 1046.1 ## - Fireplaces 1 179.60 1056.4 ## - Bldg_Type 4 182.14 1062.3 ## - Half_Bath 1 180.31 1064.5 ## - Full_Bath 1 189.78 1169.4 ## - Overall_Qual 9 222.70 1436.5 ## ## Step: AIC=1032.67 ## Bonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + ## First_Flr_SF + Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + ## Garage_Area + TotRms_AbvGrd ## ## Df Deviance AIC ## - Street 1 177.58 1025.6 ## - Second_Flr_SF 1 178.00 1030.4 ## - TotRms_AbvGrd 1 178.03 1030.7 ## - Lot_Area 1 178.13 1031.9 ## &lt;none&gt; 177.54 1032.7 ## - House_Style 7 182.60 1036.9 ## - Garage_Area 1 178.73 1038.8 ## - Fireplaces 1 179.62 1049.0 ## - Bldg_Type 4 182.15 1054.8 ## - Half_Bath 1 180.33 1057.1 ## - First_Flr_SF 1 181.61 1071.5 ## - Full_Bath 1 189.78 1161.8 ## - Overall_Qual 9 222.76 1429.4 ## ## Step: AIC=1025.59 ## Bonus ~ Lot_Area + Bldg_Type + House_Style + Overall_Qual + First_Flr_SF + ## Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + ## TotRms_AbvGrd ## ## Df Deviance AIC ## - Second_Flr_SF 1 178.04 1023.3 ## - TotRms_AbvGrd 1 178.08 1023.7 ## &lt;none&gt; 177.58 1025.6 ## - Lot_Area 1 178.26 1025.8 ## - House_Style 7 182.65 1029.9 ## - Garage_Area 1 178.81 1032.0 ## - Fireplaces 1 179.67 1041.9 ## - Bldg_Type 4 182.16 1047.3 ## - Half_Bath 1 180.37 1049.9 ## - First_Flr_SF 1 181.62 1064.1 ## - Full_Bath 1 189.79 1154.3 ## - Overall_Qual 9 222.81 1422.3 ## ## Step: AIC=1023.26 ## Bonus ~ Lot_Area + Bldg_Type + House_Style + Overall_Qual + First_Flr_SF + ## Full_Bath + Half_Bath + Fireplaces + Garage_Area + TotRms_AbvGrd ## ## Df Deviance AIC ## - TotRms_AbvGrd 1 178.29 1018.5 ## &lt;none&gt; 178.04 1023.3 ## - Lot_Area 1 178.76 1023.8 ## - House_Style 7 182.81 1024.1 ## - Garage_Area 1 179.40 1031.2 ## - Fireplaces 1 180.40 1042.6 ## - Bldg_Type 4 182.92 1048.2 ## - Half_Bath 1 181.57 1055.9 ## - First_Flr_SF 1 181.64 1056.6 ## - Full_Bath 1 192.31 1173.8 ## - Overall_Qual 9 223.59 1421.8 ## ## Step: AIC=1018.47 ## Bonus ~ Lot_Area + Bldg_Type + House_Style + Overall_Qual + First_Flr_SF + ## Full_Bath + Half_Bath + Fireplaces + Garage_Area ## ## Df Deviance AIC ## &lt;none&gt; 178.29 1018.5 ## - Lot_Area 1 179.06 1019.7 ## - House_Style 7 183.22 1021.1 ## - Garage_Area 1 179.70 1027.0 ## - Fireplaces 1 180.62 1037.4 ## - Bldg_Type 4 183.34 1045.3 ## - Half_Bath 1 181.71 1049.8 ## - First_Flr_SF 1 181.88 1051.8 ## - Full_Bath 1 192.38 1166.8 ## - Overall_Qual 9 224.36 1421.2 In the above two approaches we used the BIC selection criteria. Here both forward and backward selection actually picked the same model. Let’s check the concordance of this model. #Concordance(train$Bonus, predict(back.model, type = &quot;response&quot;)) Not surprisingly, this model outperforms the previous model that we had with a concordance of 96.1%. Although not covered in detail here, regularized regression can also be applied to logistic regression to get a different view. This might be helpful with the multicollinearity present in these predictor variables. Again, we can use the glmnet function with the addition of a family = \"binomial\" option. 7.4.6 Python Code Linear Probability Model import statsmodels.formula.api as smf lp_model = smf.ols(&quot;Bonus ~ Gr_Liv_Area&quot;, data = train).fit() lp_model.summary() OLS Regression Results Dep. Variable: Bonus R-squared: 0.319 Model: OLS Adj. R-squared: 0.319 Method: Least Squares F-statistic: 959.9 Date: Tue, 06 Jun 2023 Prob (F-statistic): 3.42e-173 Time: 14:58:04 Log-Likelihood: -1074.6 No. Observations: 2051 AIC: 2153. Df Residuals: 2049 BIC: 2164. Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] Intercept -0.4059 0.028 -14.259 0.000 -0.462 -0.350 Gr_Liv_Area 0.0006 1.79e-05 30.983 0.000 0.001 0.001 Omnibus: 0.691 Durbin-Watson: 1.974 Prob(Omnibus): 0.708 Jarque-Bera (JB): 0.755 Skew: 0.030 Prob(JB): 0.685 Kurtosis: 2.928 Cond. No. 5.00e+03 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 5e+03. This might indicate that there arestrong multicollinearity or other numerical problems. sm.api.qqplot(lp_model.resid) plt.show() train[&#39;pred_lp&#39;] = lp_model.predict() train[&#39;resid_lp&#39;] = lp_model.resid train[[&#39;Bonus&#39;, &#39;pred_lp&#39;, &#39;resid_lp&#39;]].head(n = 10) ## Bonus pred_lp resid_lp ## 0 1 0.459673 0.540327 ## 1 0 0.347376 -0.347376 ## 2 1 0.580865 0.419135 ## 3 0 0.665921 -0.665921 ## 4 0 0.085536 -0.085536 ## 5 0 0.096655 -0.096655 ## 6 0 0.181155 -0.181155 ## 7 1 0.351268 0.648732 ## 8 1 0.866054 0.133946 ## 9 1 0.557516 0.442484 plt.cla() ax = sns.relplot(data = train, y = &quot;resid_lp&quot;, x = &quot;pred_lp&quot;) ax.set(ylabel = &#39;Residuals&#39;, xlabel = &#39;Predicted Probability of Bonus&#39;) plt.show() Binary Logistic Regression log_model = smf.logit(&quot;Bonus ~ Gr_Liv_Area&quot;, data = train).fit() ## Optimization terminated successfully. ## Current function value: 0.481217 ## Iterations 7 log_model.summary() Logit Regression Results Dep. Variable: Bonus No. Observations: 2051 Model: Logit Df Residuals: 2049 Method: MLE Df Model: 1 Date: Tue, 06 Jun 2023 Pseudo R-squ.: 0.2959 Time: 14:58:07 Log-Likelihood: -986.98 converged: True LL-Null: -1401.8 Covariance Type: nonrobust LLR p-value: 1.975e-182 coef std err z P>|z| [0.025 0.975] Intercept -5.9299 0.271 -21.867 0.000 -6.461 -5.398 Gr_Liv_Area 0.0038 0.000 21.159 0.000 0.003 0.004 odds_ratio = 100*(np.exp(log_model.params) - 1) print(odds_ratio) ## Intercept -99.734133 ## Gr_Liv_Area 0.376858 ## dtype: float64 Adding Categorical Variables log_model2 = smf.logit(&quot;Bonus ~ Gr_Liv_Area + C(Central_Air) + C(Fireplaces)&quot;, data = train).fit() ## Warning: Maximum number of iterations has been exceeded. ## Current function value: 0.433476 ## Iterations: 35 ## ## C:\\PROGRA~3\\ANACON~1\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals ## warnings.warn(&quot;Maximum Likelihood optimization failed to &quot; log_model2.summary() Logit Regression Results Dep. Variable: Bonus No. Observations: 2051 Model: Logit Df Residuals: 2044 Method: MLE Df Model: 6 Date: Tue, 06 Jun 2023 Pseudo R-squ.: 0.3658 Time: 14:58:08 Log-Likelihood: -889.06 converged: False LL-Null: -1401.8 Covariance Type: nonrobust LLR p-value: 2.811e-218 coef std err z P>|z| [0.025 0.975] Intercept -10.0932 0.684 -14.746 0.000 -11.435 -8.752 C(Central_Air)[T.Y] 3.8413 0.559 6.870 0.000 2.745 4.937 C(Fireplaces)[T.1] 1.0489 0.124 8.452 0.000 0.806 1.292 C(Fireplaces)[T.2] 0.5996 0.230 2.609 0.009 0.149 1.050 C(Fireplaces)[T.3] -0.0291 1.151 -0.025 0.980 -2.284 2.226 C(Fireplaces)[T.4] 17.6705 3.96e+04 0.000 1.000 -7.76e+04 7.76e+04 Gr_Liv_Area 0.0037 0.000 18.327 0.000 0.003 0.004 odds_ratio = 100*(np.exp(log_model2.params) - 1) print(odds_ratio) ## Intercept -9.999586e+01 ## C(Central_Air)[T.Y] 4.558753e+03 ## C(Fireplaces)[T.1] 1.854611e+02 ## C(Fireplaces)[T.2] 8.213467e+01 ## C(Fireplaces)[T.3] -2.866930e+00 ## C(Fireplaces)[T.4] 4.722795e+09 ## Gr_Liv_Area 3.693110e-01 ## dtype: float64 Model Assessment Python doesn’t have concordant / discordant pair calculations. We will learn in Fall semester other metrics to evaluate a logistic regression model that Python does have. Variable Selection and Regularized Regression Python does NOT have nice capabilities to do variable selection automatically in statsmodels, scikitlearn, or scipy. All resources I can find involve downloading and installing a package (mlxtend) that is not included by default in anaconda or writing your own function. Scikit learn has something similar but uses the model’s coefficients (!!!) to select, not p-values. Scikit learn can do this by evaluating a metric on cross-validation, but that is not covered until machine learning in Fall 3. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
