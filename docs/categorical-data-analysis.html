<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Categorical Data Analysis | Statistical Foundations</title>
  <meta name="description" content="Chapter 7 Categorical Data Analysis | Statistical Foundations" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Categorical Data Analysis | Statistical Foundations" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="IAA-Faculty/statistical_foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Categorical Data Analysis | Statistical Foundations" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="model-building-scoring-for-prediction.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a style="font-weight:bold, text-align:center" href="https://github.com/IAA-Faculty/statistical_foundations/">Statistical Foundations</a>
<img src="./img/iaaicon.png" alt="IAA"  class="center"</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-stat.html"><a href="intro-stat.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-stat.html"><a href="intro-stat.html#eda"><i class="fa fa-check"></i><b>1.1</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro-stat.html"><a href="intro-stat.html#vartypes"><i class="fa fa-check"></i><b>1.1.1</b> Types of Variables</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-stat.html"><a href="intro-stat.html#dist"><i class="fa fa-check"></i><b>1.1.2</b> Distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-stat.html"><a href="intro-stat.html#normal"><i class="fa fa-check"></i><b>1.1.3</b> The Normal Distribution</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-stat.html"><a href="intro-stat.html#skew"><i class="fa fa-check"></i><b>1.1.4</b> Skewness</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-stat.html"><a href="intro-stat.html#kurt"><i class="fa fa-check"></i><b>1.1.5</b> Kurtosis</a></li>
<li class="chapter" data-level="1.1.6" data-path="intro-stat.html"><a href="intro-stat.html#graphdist"><i class="fa fa-check"></i><b>1.1.6</b> Graphical Displays of Distributions</a></li>
<li class="chapter" data-level="1.1.7" data-path="intro-stat.html"><a href="intro-stat.html#python-code"><i class="fa fa-check"></i><b>1.1.7</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro-stat.html"><a href="intro-stat.html#pointest"><i class="fa fa-check"></i><b>1.2</b> Point Estimates</a></li>
<li class="chapter" data-level="1.3" data-path="intro-stat.html"><a href="intro-stat.html#ci"><i class="fa fa-check"></i><b>1.3</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-stat.html"><a href="intro-stat.html#python-code-1"><i class="fa fa-check"></i><b>1.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-stat.html"><a href="intro-stat.html#hypotest"><i class="fa fa-check"></i><b>1.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro-stat.html"><a href="intro-stat.html#onesample"><i class="fa fa-check"></i><b>1.4.1</b> One-Sample T-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro-stat.html"><a href="intro-stat.html#python-code-2"><i class="fa fa-check"></i><b>1.4.2</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-stat.html"><a href="intro-stat.html#two-sample-t-tests"><i class="fa fa-check"></i><b>1.5</b> Two-Sample t-tests</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-stat.html"><a href="intro-stat.html#testnorm"><i class="fa fa-check"></i><b>1.5.1</b> Testing Normality of Groups</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-stat.html"><a href="intro-stat.html#ftest"><i class="fa fa-check"></i><b>1.5.2</b> Testing Equality of Variances</a></li>
<li class="chapter" data-level="1.5.3" data-path="intro-stat.html"><a href="intro-stat.html#tsttest"><i class="fa fa-check"></i><b>1.5.3</b> Testing Equality of Means</a></li>
<li class="chapter" data-level="1.5.4" data-path="intro-stat.html"><a href="intro-stat.html#python-code-3"><i class="fa fa-check"></i><b>1.5.4</b> Python Code</a></li>
<li class="chapter" data-level="1.5.5" data-path="intro-stat.html"><a href="intro-stat.html#wilcoxon"><i class="fa fa-check"></i><b>1.5.5</b> Mann-Whitney-Wilcoxon Test</a></li>
<li class="chapter" data-level="1.5.6" data-path="intro-stat.html"><a href="intro-stat.html#python-code-4"><i class="fa fa-check"></i><b>1.5.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="slr.html"><a href="slr.html"><i class="fa fa-check"></i><b>2</b> Introduction to ANOVA and Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="slr.html"><a href="slr.html#evp"><i class="fa fa-check"></i><b>2.1</b> Predictive vs. Explanatory</a></li>
<li class="chapter" data-level="2.2" data-path="slr.html"><a href="slr.html#trainvalidtest"><i class="fa fa-check"></i><b>2.2</b> Honest Assessment</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="slr.html"><a href="slr.html#python-code-5"><i class="fa fa-check"></i><b>2.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="slr.html"><a href="slr.html#bivariate-eda"><i class="fa fa-check"></i><b>2.3</b> Bivariate EDA</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="slr.html"><a href="slr.html#continuous-continuous-associations"><i class="fa fa-check"></i><b>2.3.1</b> Continuous-Continuous Associations</a></li>
<li class="chapter" data-level="2.3.2" data-path="slr.html"><a href="slr.html#continuous-categorical-associations"><i class="fa fa-check"></i><b>2.3.2</b> Continuous-Categorical Associations</a></li>
<li class="chapter" data-level="2.3.3" data-path="slr.html"><a href="slr.html#python-code-6"><i class="fa fa-check"></i><b>2.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="slr.html"><a href="slr.html#oneway"><i class="fa fa-check"></i><b>2.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="slr.html"><a href="slr.html#python-code-7"><i class="fa fa-check"></i><b>2.4.1</b> Python Code</a></li>
<li class="chapter" data-level="2.4.2" data-path="slr.html"><a href="slr.html#testing-assumptions"><i class="fa fa-check"></i><b>2.4.2</b> Testing Assumptions</a></li>
<li class="chapter" data-level="2.4.3" data-path="slr.html"><a href="slr.html#python-code-8"><i class="fa fa-check"></i><b>2.4.3</b> Python Code</a></li>
<li class="chapter" data-level="2.4.4" data-path="slr.html"><a href="slr.html#kruskal"><i class="fa fa-check"></i><b>2.4.4</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="2.4.5" data-path="slr.html"><a href="slr.html#python-code-9"><i class="fa fa-check"></i><b>2.4.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="slr.html"><a href="slr.html#posthoc"><i class="fa fa-check"></i><b>2.5</b> ANOVA Post-hoc Testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="slr.html"><a href="slr.html#tukey"><i class="fa fa-check"></i><b>2.5.1</b> Tukey-Kramer</a></li>
<li class="chapter" data-level="2.5.2" data-path="slr.html"><a href="slr.html#dunnett"><i class="fa fa-check"></i><b>2.5.2</b> Dunnett’s Test</a></li>
<li class="chapter" data-level="2.5.3" data-path="slr.html"><a href="slr.html#python-code-10"><i class="fa fa-check"></i><b>2.5.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="slr.html"><a href="slr.html#cor"><i class="fa fa-check"></i><b>2.6</b> Pearson Correlation</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="slr.html"><a href="slr.html#testcor"><i class="fa fa-check"></i><b>2.6.1</b> Statistical Test</a></li>
<li class="chapter" data-level="2.6.2" data-path="slr.html"><a href="slr.html#effect-of-anomalous-observations"><i class="fa fa-check"></i><b>2.6.2</b> Effect of Anomalous Observations</a></li>
<li class="chapter" data-level="2.6.3" data-path="slr.html"><a href="slr.html#the-correlation-matrix"><i class="fa fa-check"></i><b>2.6.3</b> The Correlation Matrix</a></li>
<li class="chapter" data-level="2.6.4" data-path="slr.html"><a href="slr.html#python-code-11"><i class="fa fa-check"></i><b>2.6.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="slr.html"><a href="slr.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="slr.html"><a href="slr.html#slrassumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions of Linear Regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="slr.html"><a href="slr.html#testing-for-association"><i class="fa fa-check"></i><b>2.7.2</b> Testing for Association</a></li>
<li class="chapter" data-level="2.7.3" data-path="slr.html"><a href="slr.html#python-code-12"><i class="fa fa-check"></i><b>2.7.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Complex ANOVA and Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#two-way-anova"><i class="fa fa-check"></i><b>3.1</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mlr.html"><a href="mlr.html#exploration"><i class="fa fa-check"></i><b>3.1.1</b> Exploration</a></li>
<li class="chapter" data-level="3.1.2" data-path="mlr.html"><a href="mlr.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
<li class="chapter" data-level="3.1.3" data-path="mlr.html"><a href="mlr.html#post-hoc-testing"><i class="fa fa-check"></i><b>3.1.3</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.1.4" data-path="mlr.html"><a href="mlr.html#python-code-13"><i class="fa fa-check"></i><b>3.1.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#two-way-anova-with-interactions"><i class="fa fa-check"></i><b>3.2</b> Two-Way ANOVA with Interactions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#post-hoc-testing-1"><i class="fa fa-check"></i><b>3.2.1</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.2.2" data-path="mlr.html"><a href="mlr.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="mlr.html"><a href="mlr.html#python-code-14"><i class="fa fa-check"></i><b>3.2.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#randomized-block-design"><i class="fa fa-check"></i><b>3.3</b> Randomized Block Design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mlr.html"><a href="mlr.html#garlic-bulb-weight-example"><i class="fa fa-check"></i><b>3.3.1</b> Garlic Bulb Weight Example</a></li>
<li class="chapter" data-level="3.3.2" data-path="mlr.html"><a href="mlr.html#assumptions-1"><i class="fa fa-check"></i><b>3.3.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.3.3" data-path="mlr.html"><a href="mlr.html#python-code-15"><i class="fa fa-check"></i><b>3.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model Structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="mlr.html"><a href="mlr.html#global-local-inference"><i class="fa fa-check"></i><b>3.4.2</b> Global &amp; Local Inference</a></li>
<li class="chapter" data-level="3.4.3" data-path="mlr.html"><a href="mlr.html#assumptions-2"><i class="fa fa-check"></i><b>3.4.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.4.4" data-path="mlr.html"><a href="mlr.html#multiple-coefficients-of-determination"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Coefficients of Determination</a></li>
<li class="chapter" data-level="3.4.5" data-path="mlr.html"><a href="mlr.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>3.4.5</b> Categorical Predictor Variables</a></li>
<li class="chapter" data-level="3.4.6" data-path="mlr.html"><a href="mlr.html#python-code-16"><i class="fa fa-check"></i><b>3.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sel.html"><a href="sel.html"><i class="fa fa-check"></i><b>4</b> Model Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sel.html"><a href="sel.html#selection-criteria"><i class="fa fa-check"></i><b>4.1</b> Selection Criteria</a></li>
<li class="chapter" data-level="4.2" data-path="sel.html"><a href="sel.html#stepwise-selection"><i class="fa fa-check"></i><b>4.2</b> Stepwise Selection</a>
<ul>
<li class="chapter" data-level="" data-path="sel.html"><a href="sel.html#forward"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="sel.html"><a href="sel.html#backward"><i class="fa fa-check"></i>Backward</a></li>
<li class="chapter" data-level="" data-path="sel.html"><a href="sel.html#stepwise"><i class="fa fa-check"></i>Stepwise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="sel.html"><a href="sel.html#significance-levels"><i class="fa fa-check"></i><b>4.3</b> Significance Levels</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="sel.html"><a href="sel.html#python-code-17"><i class="fa fa-check"></i><b>4.3.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>5</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="diag.html"><a href="diag.html#python-code-18"><i class="fa fa-check"></i><b>5.0.1</b> Python Code</a></li>
<li class="chapter" data-level="5.1" data-path="diag.html"><a href="diag.html#examining-residuals"><i class="fa fa-check"></i><b>5.1</b> Examining Residuals</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="diag.html"><a href="diag.html#python-code-19"><i class="fa fa-check"></i><b>5.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="diag.html"><a href="diag.html#misspecified-model"><i class="fa fa-check"></i><b>5.2</b> Misspecified Model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="diag.html"><a href="diag.html#python-code-20"><i class="fa fa-check"></i><b>5.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="diag.html"><a href="diag.html#constant-variance"><i class="fa fa-check"></i><b>5.3</b> Constant Variance</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="diag.html"><a href="diag.html#python-code-21"><i class="fa fa-check"></i><b>5.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="diag.html"><a href="diag.html#normality"><i class="fa fa-check"></i><b>5.4</b> Normality</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="diag.html"><a href="diag.html#python-code-22"><i class="fa fa-check"></i><b>5.4.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="diag.html"><a href="diag.html#correlated-errors"><i class="fa fa-check"></i><b>5.5</b> Correlated Errors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="diag.html"><a href="diag.html#python-code-23"><i class="fa fa-check"></i><b>5.5.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="diag.html"><a href="diag.html#influential-observations-and-outliers"><i class="fa fa-check"></i><b>5.6</b> Influential Observations and Outliers</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="diag.html"><a href="diag.html#python-code-24"><i class="fa fa-check"></i><b>5.6.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="diag.html"><a href="diag.html#multicollinearity"><i class="fa fa-check"></i><b>5.7</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="diag.html"><a href="diag.html#python-code-25"><i class="fa fa-check"></i><b>5.7.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html"><i class="fa fa-check"></i><b>6</b> Model Building &amp; Scoring for Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#regularized-regression"><i class="fa fa-check"></i><b>6.1</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#penalties-in-models"><i class="fa fa-check"></i><b>6.1.1</b> Penalties in Models</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#lasso"><i class="fa fa-check"></i><b>6.1.3</b> LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="6.1.5" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#python-code-26"><i class="fa fa-check"></i><b>6.1.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#optimizing-penalties"><i class="fa fa-check"></i><b>6.2</b> Optimizing Penalties</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#cross-validation"><i class="fa fa-check"></i><b>6.2.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#cv-in-regularized-regression"><i class="fa fa-check"></i><b>6.2.2</b> CV in Regularized Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#model-comparisons"><i class="fa fa-check"></i><b>6.3</b> Model Comparisons</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#model-metrics"><i class="fa fa-check"></i><b>6.3.1</b> Model Metrics</a></li>
<li class="chapter" data-level="6.3.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#test-dataset-comparison"><i class="fa fa-check"></i><b>6.3.2</b> Test Dataset Comparison</a></li>
<li class="chapter" data-level="6.3.3" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#python-code-27"><i class="fa fa-check"></i><b>6.3.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#describing-categorical-data"><i class="fa fa-check"></i><b>7.1</b> Describing Categorical Data</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-28"><i class="fa fa-check"></i><b>7.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#tests-of-association"><i class="fa fa-check"></i><b>7.2</b> Tests of Association</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-29"><i class="fa fa-check"></i><b>7.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#measures-of-association"><i class="fa fa-check"></i><b>7.3</b> Measures of Association</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-30"><i class="fa fa-check"></i><b>7.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Introduction to Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#linear-probability-model"><i class="fa fa-check"></i><b>7.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.4.2</b> Binary Logistic Regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#adding-categorical-variables"><i class="fa fa-check"></i><b>7.4.3</b> Adding Categorical Variables</a></li>
<li class="chapter" data-level="7.4.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#model-assessment"><i class="fa fa-check"></i><b>7.4.4</b> Model Assessment</a></li>
<li class="chapter" data-level="7.4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#variable-selection-and-regularized-regression"><i class="fa fa-check"></i><b>7.4.5</b> Variable Selection and Regularized Regression</a></li>
<li class="chapter" data-level="7.4.6" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-31"><i class="fa fa-check"></i><b>7.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="categorical-data-analysis" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Categorical Data Analysis<a href="categorical-data-analysis.html#categorical-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Everything analysis covered so far has used a continuous variable as a target variable of interest. What if our target variable was categorical instead of continuous? Our analysis must change to adjust.</p>
<p>This Chapter aims to answer the following questions:</p>
<ul>
<li><p>How do you explore categorical variables?</p>
<ul>
<li>Nominal vs. Ordinal</li>
<li>Tests of Association</li>
<li>Measures of Association</li>
</ul></li>
<li><p>How do you model a categorical target variable?</p>
<ul>
<li>Logistic Regression</li>
<li>Interpreting Logistic Regression</li>
<li>Assessing Logistic Regression</li>
</ul></li>
</ul>
<div id="describing-categorical-data" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Describing Categorical Data<a href="categorical-data-analysis.html#describing-categorical-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We need to first explore our data before building any models to try and explain/predict our categorical target variable. With categorical variables, we can look at the distribution of the categories as well as see if this distribution has any association with other variables. For this analysis we are going to still use our Ames housing data. Imagine you worked for a real estate agency and got a bonus check if you sold a house above $175,000 in value. Let’s create this variable in our data:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="categorical-data-analysis.html#cb1-1" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span></span>
<span id="cb1-2"><a href="categorical-data-analysis.html#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Bonus =</span> <span class="fu">ifelse</span>(Sale_Price <span class="sc">&gt;</span> <span class="dv">175000</span>, <span class="dv">1</span>, <span class="dv">0</span>))</span></code></pre></div>
<p>You are interested in what variables might be associated with obtaining a higher chance of getting a bonus (selling a house above $175,000). An association exists between two categorical variables if the distribution of one variable changes when the value of the other categorical changes. If there is no association, the distribution of the first variable is the same regardless of the value of the other variable. For example, if we wanted to know if obtaining a bonus on selling a house in Ames, Iowa was associated with whether the house had central air we could look at the distribution of bonus eligible houses. If we observe that 42% of homes with central air are bonus eligible and 42% of homes without central air are bonus eligible, then it appears that central air has no bearing on whether the home is bonus eligible. However, if instead we observe that only 3% of homes without central air are bonus eligible, but 44% of home with central air are bonus eligible, then it appears that having central air might be related to a home being bonus eligible.</p>
<p>To understand the distribution of categorical variables we need to look at frequency tables. A frequency table shows the number of observations that occur in certain categories or intervals. A one way frequency table examines all the categories of one variable. These are easily visualized with bar charts.</p>
<p>Let’s look at the distribution of both bonus eligibility and central air using the <code>table</code> function. The <code>ggplot</code> function with the <code>geom_bar</code> function allows us to view our data in a bar chart.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="categorical-data-analysis.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(train<span class="sc">$</span>Bonus)</span></code></pre></div>
<pre><code>## 
##    0    1 
## 1211  840</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="categorical-data-analysis.html#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> train) <span class="sc">+</span></span>
<span id="cb4-2"><a href="categorical-data-analysis.html#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Bonus))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="categorical-data-analysis.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(train<span class="sc">$</span>Central_Air)</span></code></pre></div>
<pre><code>## 
##    N    Y 
##  147 1904</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="categorical-data-analysis.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> train) <span class="sc">+</span></span>
<span id="cb7-2"><a href="categorical-data-analysis.html#cb7-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Central_Air))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Frequency tables show single variables, but if we want to explore two variables together we look at <strong>cross-tabulation</strong> tables. A cross-tabulation table shows the number of observations for each combination of the row and column variables.</p>
<p>Let’s again examine bonus eligibility, but this time across levels of central air. Again, we can use the <code>table</code> function. The <code>prop.table</code> function allows us to compare two variables in terms of proportions instead of frequencies.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="categorical-data-analysis.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus)</span></code></pre></div>
<pre><code>##    
##        0    1
##   N  142    5
##   Y 1069  835</code></pre>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="categorical-data-analysis.html#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus))</span></code></pre></div>
<pre><code>##    
##               0           1
##   N 0.069234520 0.002437835
##   Y 0.521209166 0.407118479</code></pre>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="categorical-data-analysis.html#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> train) <span class="sc">+</span></span>
<span id="cb12-2"><a href="categorical-data-analysis.html#cb12-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Bonus, <span class="at">fill =</span> Central_Air))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>From the above output we can see that 147 homes have no central air with only 5 of them being bonus eligible. However, there are 1904 homes that have central air with 835 of them being bonus eligible. For an even more detailed breakdown we can use the <code>CrossTable</code> function.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="categorical-data-analysis.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gmodels)</span>
<span id="cb13-2"><a href="categorical-data-analysis.html#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="categorical-data-analysis.html#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="fu">CrossTable</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus)</span></code></pre></div>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## | Chi-square contribution |
## |           N / Row Total |
## |           N / Col Total |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  2051 
## 
##  
##                   | train$Bonus 
## train$Central_Air |         0 |         1 | Row Total | 
## ------------------|-----------|-----------|-----------|
##                 N |       142 |         5 |       147 | 
##                   |    35.112 |    50.620 |           | 
##                   |     0.966 |     0.034 |     0.072 | 
##                   |     0.117 |     0.006 |           | 
##                   |     0.069 |     0.002 |           | 
## ------------------|-----------|-----------|-----------|
##                 Y |      1069 |       835 |      1904 | 
##                   |     2.711 |     3.908 |           | 
##                   |     0.561 |     0.439 |     0.928 | 
##                   |     0.883 |     0.994 |           | 
##                   |     0.521 |     0.407 |           | 
## ------------------|-----------|-----------|-----------|
##      Column Total |      1211 |       840 |      2051 | 
##                   |     0.590 |     0.410 |           | 
## ------------------|-----------|-----------|-----------|
## 
## </code></pre>
<p>The advantage of the <code>CrossTable</code> function is that we can easily get not only the frequencies, but the cell, row, and column proportions. For example, the third number in each cell gives us the row proportion. For homes without central air, 96.6% of them are not bonus eligible, while 3.4% of them are. For homes with central air, 56.1% of the homes are not bonus eligible, while 43.9% of them are. This would appear that the distribution of bonus eligible homes changes across levels of central air - a relationship between the two variables. This expected relationship needs to be tested statistically for verification.</p>
<div id="python-code-28" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Python Code<a href="categorical-data-analysis.html#python-code-28" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="categorical-data-analysis.html#cb15-1" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;Bonus&#39;</span>] <span class="op">=</span> np.where(train[<span class="st">&#39;Sale_Price&#39;</span>] <span class="op">&gt;</span> <span class="dv">175000</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span></code></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="categorical-data-analysis.html#cb16-1" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;Bonus&#39;</span>].value_counts()</span></code></pre></div>
<pre><code>## 0    1211
## 1     840
## Name: Bonus, dtype: int64</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="categorical-data-analysis.html#cb18-1" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.countplot(x <span class="op">=</span> <span class="st">&quot;Bonus&quot;</span>, data <span class="op">=</span> train, color <span class="op">=</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb18-2"><a href="categorical-data-analysis.html#cb18-2" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">&#39;Bonus Eligible&#39;</span>,</span>
<span id="cb18-3"><a href="categorical-data-analysis.html#cb18-3" aria-hidden="true" tabindex="-1"></a>       ylabel <span class="op">=</span> <span class="st">&#39;Frequency&#39;</span>,</span>
<span id="cb18-4"><a href="categorical-data-analysis.html#cb18-4" aria-hidden="true" tabindex="-1"></a>       title <span class="op">=</span> <span class="st">&#39;Bar Graph of Bonus Eligibility&#39;</span>)</span>
<span id="cb18-5"><a href="categorical-data-analysis.html#cb18-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="categorical-data-analysis.html#cb19-1" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;Central_Air&#39;</span>].value_counts()</span></code></pre></div>
<pre><code>## Y    1904
## N     147
## Name: Central_Air, dtype: int64</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="categorical-data-analysis.html#cb21-1" aria-hidden="true" tabindex="-1"></a>plt.cla()</span>
<span id="cb21-2"><a href="categorical-data-analysis.html#cb21-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.countplot(x <span class="op">=</span> <span class="st">&quot;Central_Air&quot;</span>, data <span class="op">=</span> train, color <span class="op">=</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb21-3"><a href="categorical-data-analysis.html#cb21-3" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">&#39;Central Air&#39;</span>,</span>
<span id="cb21-4"><a href="categorical-data-analysis.html#cb21-4" aria-hidden="true" tabindex="-1"></a>       ylabel <span class="op">=</span> <span class="st">&#39;Frequency&#39;</span>,</span>
<span id="cb21-5"><a href="categorical-data-analysis.html#cb21-5" aria-hidden="true" tabindex="-1"></a>       title <span class="op">=</span> <span class="st">&#39;Bar Graph of Central Air Availability&#39;</span>)</span>
<span id="cb21-6"><a href="categorical-data-analysis.html#cb21-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-10-3.png" width="672" /></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="categorical-data-analysis.html#cb22-1" aria-hidden="true" tabindex="-1"></a>plt.cla()</span>
<span id="cb22-2"><a href="categorical-data-analysis.html#cb22-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.countplot(x <span class="op">=</span> <span class="st">&quot;Bonus&quot;</span>, data <span class="op">=</span> train, hue <span class="op">=</span> <span class="st">&quot;Central_Air&quot;</span>)</span>
<span id="cb22-3"><a href="categorical-data-analysis.html#cb22-3" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(xlabel <span class="op">=</span> <span class="st">&#39;Central Air&#39;</span>,</span>
<span id="cb22-4"><a href="categorical-data-analysis.html#cb22-4" aria-hidden="true" tabindex="-1"></a>       ylabel <span class="op">=</span> <span class="st">&#39;Frequency&#39;</span>,</span>
<span id="cb22-5"><a href="categorical-data-analysis.html#cb22-5" aria-hidden="true" tabindex="-1"></a>       title <span class="op">=</span> <span class="st">&#39;Bar Graph of Central Air Availability&#39;</span>)</span>
<span id="cb22-6"><a href="categorical-data-analysis.html#cb22-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-11-5.png" width="672" /></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="categorical-data-analysis.html#cb23-1" aria-hidden="true" tabindex="-1"></a>pd.crosstab(index <span class="op">=</span> train[<span class="st">&#39;Central_Air&#39;</span>], columns <span class="op">=</span> train[<span class="st">&#39;Bonus&#39;</span>])</span></code></pre></div>
<pre><code>## Bonus           0    1
## Central_Air           
## N             142    5
## Y            1069  835</code></pre>
</div>
</div>
<div id="tests-of-association" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Tests of Association<a href="categorical-data-analysis.html#tests-of-association" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Much like in Chapter <a href="slr.html#slr">2</a> we have statistical tests to evaluate relationships between two categorical variables. The null hypothesis for these statistical tests is that the two variables have no association - the distribution of one variable does not change across levels of another variable. The alternative hypothesis is an association between the two variables - the distribution of one variable changes across levels of another variable.</p>
<p>These statistical tests follow a <span class="math inline">\(\chi^2\)</span>-distribution. The <span class="math inline">\(\chi^2\)</span>-distribution is a distribution that has the following characteristics:</p>
<ul>
<li>Bounded below by 0</li>
<li>Right-skewed</li>
<li>One set of degrees of freedom</li>
</ul>
<p>A plot of a variety of <span class="math inline">\(\chi^2\)</span>-distributions is shown here:</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-13-7.png" width="672" /></p>
<p>Two common <span class="math inline">\(\chi^2\)</span> tests are the Pearson and Likelihood Ratio <span class="math inline">\(\chi^2\)</span> tests. They compare the observed count of observations in each cell of a cross-tabulation table between two variables to their expected count <strong>if</strong> there was no relationship. The expected cell count applies the overall distribution of one variable across all the levels of the other variable. For example, overall 59% of all homes are not bonus eligible. <strong>If</strong> that were to apply to every level of central air, then the 140 homes without central air would be expected to have 86.73 ( $ = 147 $ ) of them would be bonus eligible while 60.27 ( $ = 147 $ ) of them would not be bonus eligible. We actually observe 142 and 5 homes for each of these categories respectively. The further the observed data is from the expected data, the more evidence we have that there is a relationship between the two variables.</p>
<p>The test statistic for the Pearson <span class="math inline">\(\chi^2\)</span> test is the following:</p>
<p><span class="math display">\[
\chi^2_P = \sum_{i=1}^R \sum_{j=1}^C \frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}
\]</span>
From the equation above, the closer that the observed count of each cross-tabulation table cell to the expected count, the smaller the test statistic. As with all previous hypothesis tests, the smaller the test statistic, the larger the p-value, implying less evidence for the alternative hypothesis.</p>
<p>Let’s examine the relationship between central air and bonus eligibility using the <code>chisq.test</code> function.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="categorical-data-analysis.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(<span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus))</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  table(train$Central_Air, train$Bonus)
## X-squared = 90.686, df = 1, p-value &lt; 2.2e-16</code></pre>
<p>The above results shows an extremely small p-value that is below any reasonable significance level. This implies that we have statistical evidence for a relationship between having central air and bonus eligibility of homes. The p-value comes from a <span class="math inline">\(\chi^2\)</span>-distribution with degrees of freedom that equal the product of the number of rows minus one and the number of columns minus one.</p>
<p>Another common test is the Likelihood Ratio test. The test statistic for this is the following:</p>
<p><span class="math display">\[
\chi^2_L = 2 \times \sum_{i=1}^R \sum_{j=1}^C Obs_{i,j} \times \log(\frac{Obs_{i,j}}{Exp_{i,j}})
\]</span></p>
<p>The p-value comes from a <span class="math inline">\(\chi^2\)</span>-distribution with degrees of freedom that equal the product of the number of rows minus one and the number of columns minus one. Both of the above tests have a sample size requirement. The sample size requirement is 80% or more of the cells in the cross-tabulation table need <strong>expected</strong> count larger
than 5.</p>
<p>For smaller sample sizes, this might be hard to meet. In those situations, we can use a more computationally expensive test called Fisher’s exact test. This test calculates every possible permutation of the data being evaluated to calculate the p-value without any distributional assumptions. To perform this test we can use the <code>fisher.test</code> function.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="categorical-data-analysis.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fisher.test</span>(<span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus))</span></code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  table(train$Central_Air, train$Bonus)
## p-value &lt; 2.2e-16
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##   9.213525 69.646380
## sample estimates:
## odds ratio 
##   22.16545</code></pre>
<p>We see the same results as with the Pearson test because the assumptions were met for sample size.</p>
<p>Both the Pearson and Likelihood Ratio <span class="math inline">\(\chi^2\)</span> tests can handle any type of categorical variable either ordinal, nominal, or both. However, ordinal variables provide us extra information since the order of the categories actually matters compared to nominal. We can test for even more with ordinal variables against other ordinal variables whether two ordinal variables have a <strong>linear relationship</strong> as compared to just a general one. An ordinal test for association is the Mantel-Haenszel <span class="math inline">\(\chi^2\)</span> test. The test statistic for the Mantel-Haenszel <span class="math inline">\(\chi^2\)</span> test is the following:</p>
<p><span class="math display">\[
\chi^2_{MH} = (n-1)r^2
\]</span>
where <span class="math inline">\(r^2\)</span> is the Pearson correlation between the column and row variables. This test follows a <span class="math inline">\(\chi^2\)</span>-distribution with only one degree of freedom.</p>
<p>Since both the central air and bonus eligibility variables are binary, they are ordinal. Since they are both ordinal, we should use the Mantel-Haenszel <span class="math inline">\(\chi^2\)</span> test with the <code>CMHtest</code> function. In the main output table, the first row is the Mantel-Haenszel <span class="math inline">\(\chi^2\)</span> test.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="categorical-data-analysis.html#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vcdExtra)</span>
<span id="cb29-2"><a href="categorical-data-analysis.html#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="categorical-data-analysis.html#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="fu">CMHtest</span>(<span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus))<span class="sc">$</span>table[<span class="dv">1</span>,]</span></code></pre></div>
<pre><code>##        Chisq           Df         Prob 
## 9.230619e+01 1.000000e+00 7.425180e-22</code></pre>
<p>From here we can see another extremely small p-value as we saw in earlier, more general <span class="math inline">\(\chi^2\)</span> tests.</p>
<div id="python-code-29" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Python Code<a href="categorical-data-analysis.html#python-code-29" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="categorical-data-analysis.html#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> chi2_contingency</span>
<span id="cb31-2"><a href="categorical-data-analysis.html#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="categorical-data-analysis.html#cb31-3" aria-hidden="true" tabindex="-1"></a>chi2_contingency(pd.crosstab(index <span class="op">=</span> train[<span class="st">&#39;Central_Air&#39;</span>], columns <span class="op">=</span> train[<span class="st">&#39;Bonus&#39;</span>]), correction <span class="op">=</span> <span class="va">True</span>)</span></code></pre></div>
<pre><code>## (90.6859058522855, 1.683884397784894e-21, 1, array([[  86.79522184,   60.20477816],
##        [1124.20477816,  779.79522184]]))</code></pre>
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="categorical-data-analysis.html#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> fisher_exact</span>
<span id="cb33-2"><a href="categorical-data-analysis.html#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="categorical-data-analysis.html#cb33-3" aria-hidden="true" tabindex="-1"></a>fisher_exact(pd.crosstab(index <span class="op">=</span> train[<span class="st">&#39;Central_Air&#39;</span>], columns <span class="op">=</span> train[<span class="st">&#39;Bonus&#39;</span>]))</span></code></pre></div>
<pre><code>## (22.18334892422825, 1.0365949500671678e-27)</code></pre>
<p>No real Mantel-Haenszel options in Python that work for anything more than a 2x2 table so I wouldn’t trust them.</p>
</div>
</div>
<div id="measures-of-association" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Measures of Association<a href="categorical-data-analysis.html#measures-of-association" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Tests of association are best designed for just that, testing the existence of an association between two categorical variables. However, just like we saw in Chapter <a href="intro-stat.html#eda">1.1</a>, hypothesis tests are impacted by sample size. When we have the same sample size, tests of association can rank significance of variables with p-values. However, when sample sizes are not the same between two tests, the tests of association are not best for comparing the strength of an association. In those scenarios, we have measures of strength of association that can be compared across any sample size.</p>
<p>Measures of association were not designed to test if an association exists, as that is what statistical testing is for. They are designed to measure the strength of association. There are dozens of these measures. Three of the most common are the following:</p>
<ul>
<li>Odds Ratios (only for comparing two binary variables)</li>
<li>Cramer’s V (able to compare nominal variables with any number of categories)</li>
<li>Spearman’s Correlation (able to compare ordinal variables with any number of categories)</li>
</ul>
<p>An <strong>odds ratio</strong> indicates how much more likely, with respect to <strong>odds</strong>, a certain event occurs in one group relative to its occurrence in another group. The odds of an event occurring is <em>not</em> the same as the probability that an event occurs. The odds of an event occurring is the probability the event occurs divided by the probability that event does not occur.</p>
<p><span class="math display">\[
Odds = \frac{p}{1-p}
\]</span></p>
<p>Let’s again examine the cross-tabulation table between central air and bonus eligibility.</p>
<pre><code>## 
##  
##    Cell Contents
## |-------------------------|
## |                       N |
## | Chi-square contribution |
## |           N / Row Total |
## |           N / Col Total |
## |         N / Table Total |
## |-------------------------|
## 
##  
## Total Observations in Table:  2051 
## 
##  
##                   | train$Bonus 
## train$Central_Air |         0 |         1 | Row Total | 
## ------------------|-----------|-----------|-----------|
##                 N |       142 |         5 |       147 | 
##                   |    35.112 |    50.620 |           | 
##                   |     0.966 |     0.034 |     0.072 | 
##                   |     0.117 |     0.006 |           | 
##                   |     0.069 |     0.002 |           | 
## ------------------|-----------|-----------|-----------|
##                 Y |      1069 |       835 |      1904 | 
##                   |     2.711 |     3.908 |           | 
##                   |     0.561 |     0.439 |     0.928 | 
##                   |     0.883 |     0.994 |           | 
##                   |     0.521 |     0.407 |           | 
## ------------------|-----------|-----------|-----------|
##      Column Total |      1211 |       840 |      2051 | 
##                   |     0.590 |     0.410 |           | 
## ------------------|-----------|-----------|-----------|
## 
## </code></pre>
<p>Let’s look at the row without central air. The probability that a home without central air is not bonus eligible is 96.6%. That implies that the odds of not being bonus eligible in homes without central air is 28.41 (= 0.966/0.034). For homes with central air, the odds of not being bonus eligible are 1.28 (= 0.561/0.439). The odds ratio between these two would be approximately 22.2 (= 28.41/1.28). In other words, homes without central air are 22.2 times more likely (in terms of odds) to not be bonus eligible as compared to homes with central air. This relationship is intuitive based on the numbers we have seen. Without going into details, it can also be shown that homes with central air are 22.2 times as likely (in terms of odds) to be bonus eligible.</p>
<p>We can use the <code>OddsRatio</code> function to get these same results.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="categorical-data-analysis.html#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb36-2"><a href="categorical-data-analysis.html#cb36-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-3"><a href="categorical-data-analysis.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="fu">OddsRatio</span>(<span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus))</span></code></pre></div>
<pre><code>## [1] 22.18335</code></pre>
<p><strong>Cramer’s V</strong> is another measure of strength of association. Cramer’s V is calculated as follows:</p>
<p><span class="math display">\[
V = \sqrt{\frac{\chi^2_P/n}{\min(Rows-1, Columns-1)}}
\]</span></p>
<p>Cramer’s V is bounded between 0 and 1 for every comparison other than two binary variables. For two binary variables being compared the bounds are -1 to 1. The idea is still the same for both. The further the value is from 0, the stronger the relationship. Unfortunately, unlike <span class="math inline">\(R^2\)</span>, Cramer’s V has no interpretative value. It can only be used for comparison.</p>
<p>We use the <code>assocstats</code> function to get the Cramer’s V value. This function also provides the Pearson and Likelihood Ratio <span class="math inline">\(\chi^2\)</span> tests as well.</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb38-1"><a href="categorical-data-analysis.html#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="fu">assocstats</span>(<span class="fu">table</span>(train<span class="sc">$</span>Central_Air, train<span class="sc">$</span>Bonus))</span></code></pre></div>
<pre><code>##                      X^2 df P(&gt; X^2)
## Likelihood Ratio 121.499  1        0
## Pearson           92.351  1        0
## 
## Phi-Coefficient   : 0.212 
## Contingency Coeff.: 0.208 
## Cramer&#39;s V        : 0.212</code></pre>
<p>Lastly, we have Spearman’s correlation. Much like the Mantel-Haenszel test of association was specifically designed for comparing two ordinal variables, Spearman correlation measures the strength of association between two ordinal variables. Spearman is not limited to only categorical data analysis as it was also seen back in Chapter <a href="diag.html#diag">5</a> with detecting heteroskedasticity. Remember, Spearman correlation is a correlation on the ranks of the observations as compared to the actual values of the observations.</p>
<p>The <code>cor.test</code> function that gave us Pearson’s correlation also provides Spearman’s correlation.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="categorical-data-analysis.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cor.test</span>(<span class="at">x =</span> <span class="fu">as.numeric</span>(<span class="fu">ordered</span>(train<span class="sc">$</span>Central_Air)), </span>
<span id="cb40-2"><a href="categorical-data-analysis.html#cb40-2" aria-hidden="true" tabindex="-1"></a>         <span class="at">y =</span> <span class="fu">as.numeric</span>(<span class="fu">ordered</span>(train<span class="sc">$</span>Bonus)), </span>
<span id="cb40-3"><a href="categorical-data-analysis.html#cb40-3" aria-hidden="true" tabindex="-1"></a>         <span class="at">method =</span> <span class="st">&quot;spearman&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  x and y
## S = 1132826666, p-value &lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.2121966</code></pre>
<p>As previously mentioned, these are only a few of the dozens of different measures of association that exist. However, they are the most used ones.</p>
<div id="python-code-30" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Python Code<a href="categorical-data-analysis.html#python-code-30" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Odds Ratios are the statistic calculated from the Fisher’s Exact test from the previous code:</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="categorical-data-analysis.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> fisher_exact</span>
<span id="cb42-2"><a href="categorical-data-analysis.html#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="categorical-data-analysis.html#cb42-3" aria-hidden="true" tabindex="-1"></a>fisher_exact(pd.crosstab(index <span class="op">=</span> train[<span class="st">&#39;Central_Air&#39;</span>], columns <span class="op">=</span> train[<span class="st">&#39;Bonus&#39;</span>]))</span></code></pre></div>
<pre><code>## (22.18334892422825, 1.0365949500671678e-27)</code></pre>
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="categorical-data-analysis.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> fisher_exact</span>
<span id="cb44-2"><a href="categorical-data-analysis.html#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="categorical-data-analysis.html#cb44-3" aria-hidden="true" tabindex="-1"></a>fisher_exact(pd.crosstab(index <span class="op">=</span> train[<span class="st">&#39;Central_Air&#39;</span>], columns <span class="op">=</span> train[<span class="st">&#39;Bonus&#39;</span>]))</span></code></pre></div>
<pre><code>## (22.18334892422825, 1.0365949500671678e-27)</code></pre>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="categorical-data-analysis.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats.contingency <span class="im">import</span> association</span>
<span id="cb46-2"><a href="categorical-data-analysis.html#cb46-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-3"><a href="categorical-data-analysis.html#cb46-3" aria-hidden="true" tabindex="-1"></a>association(pd.crosstab(index <span class="op">=</span> train[<span class="st">&#39;Central_Air&#39;</span>], columns <span class="op">=</span> train[<span class="st">&#39;Bonus&#39;</span>]), method <span class="op">=</span> <span class="st">&quot;cramer&quot;</span>)</span></code></pre></div>
<pre><code>## 0.21219662657018892</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="categorical-data-analysis.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> spearmanr</span>
<span id="cb48-2"><a href="categorical-data-analysis.html#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="categorical-data-analysis.html#cb48-3" aria-hidden="true" tabindex="-1"></a>spearmanr(train[<span class="st">&#39;Central_Air&#39;</span>], train[<span class="st">&#39;Bonus&#39;</span>])</span></code></pre></div>
<pre><code>## SpearmanrResult(correlation=0.21219662657018892, pvalue=2.604243816433701e-22)
## 
## C:\PROGRA~3\ANACON~1\lib\site-packages\scipy\stats\_stats_py.py:112: RuntimeWarning: The input array could not be properly checked for nan values. nan values will be ignored.
##   warnings.warn(&quot;The input array could not be properly &quot;</code></pre>
</div>
</div>
<div id="introduction-to-logistic-regression" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Introduction to Logistic Regression<a href="categorical-data-analysis.html#introduction-to-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After exploring the categorical target variable, we can move on to modeling the categorical target variable. Logistic regression is a fundamental statistical analysis for data science and analytics. It part of a class of modeling techniques known as classification models since they are trying to predict categorical target variables. This target variable can be binary, ordinal, or even nominal in its structure. The primary focus will be binary logistic regression. It is the most common type of logistic regression, and sets up the foundation for both ordinal and nominal logistic regression.</p>
<p>Ordinary least squares regression is not the best approach to modeling categorical target variables. Mathematically, it can be shown that with a binary target variable coded as 0 and 1, an OLS linear regression model will produce the <strong>linear probability model</strong>.</p>
<div id="linear-probability-model" class="section level3 hasAnchor" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Linear Probability Model<a href="categorical-data-analysis.html#linear-probability-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The linear probability model is not as widely used since probabilities do not tend to follow the properties of linearity in relation to their predictors. Also, the linear probability model possibly produces predictions outside of the bounds of 0 and 1 (where probabilities should be!). For completeness sake however, here is the linear probability model using the <code>lm</code> function to try and predict bonus eligibility.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="categorical-data-analysis.html#cb50-1" aria-hidden="true" tabindex="-1"></a>lp.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Bonus <span class="sc">~</span> Gr_Liv_Area, <span class="at">data =</span> train)</span>
<span id="cb50-2"><a href="categorical-data-analysis.html#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="categorical-data-analysis.html#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(train, <span class="fu">plot</span>(<span class="at">x =</span> Gr_Liv_Area, <span class="at">y =</span> Bonus,</span>
<span id="cb50-4"><a href="categorical-data-analysis.html#cb50-4" aria-hidden="true" tabindex="-1"></a>               <span class="at">main =</span> <span class="st">&#39;OLS Regression?&#39;</span>,</span>
<span id="cb50-5"><a href="categorical-data-analysis.html#cb50-5" aria-hidden="true" tabindex="-1"></a>               <span class="at">xlab =</span> <span class="st">&#39;Greater Living Area (Sqft)&#39;</span>,</span>
<span id="cb50-6"><a href="categorical-data-analysis.html#cb50-6" aria-hidden="true" tabindex="-1"></a>               <span class="at">ylab =</span> <span class="st">&#39;Bonus Eligibility&#39;</span>))</span>
<span id="cb50-7"><a href="categorical-data-analysis.html#cb50-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(lp.model)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>Even though it doesn’t appear to really look like our data, let’s fit this linear probability model anyway for completeness sake.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="categorical-data-analysis.html#cb51-1" aria-hidden="true" tabindex="-1"></a>lp.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(Bonus <span class="sc">~</span> Gr_Liv_Area, <span class="at">data =</span> train)</span>
<span id="cb51-2"><a href="categorical-data-analysis.html#cb51-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-3"><a href="categorical-data-analysis.html#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(lp.model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Bonus ~ Gr_Liv_Area, data = train)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.70766 -0.29160 -0.09983  0.39432  0.86198 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -4.149e-01  2.776e-02  -14.94   &lt;2e-16 ***
## Gr_Liv_Area  5.534e-04  1.765e-05   31.36   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4044 on 2049 degrees of freedom
## Multiple R-squared:  0.3243, Adjusted R-squared:  0.324 
## F-statistic: 983.6 on 1 and 2049 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="categorical-data-analysis.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(<span class="fu">rstandard</span>(lp.model),</span>
<span id="cb53-2"><a href="categorical-data-analysis.html#cb53-2" aria-hidden="true" tabindex="-1"></a>       <span class="at">ylab =</span> <span class="st">&quot;Standardized Residuals&quot;</span>,</span>
<span id="cb53-3"><a href="categorical-data-analysis.html#cb53-3" aria-hidden="true" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Normal Scores&quot;</span>,</span>
<span id="cb53-4"><a href="categorical-data-analysis.html#cb53-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;QQ-Plot of Residuals&quot;</span>)</span>
<span id="cb53-5"><a href="categorical-data-analysis.html#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(<span class="fu">rstandard</span>(lp.model))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb54-1"><a href="categorical-data-analysis.html#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(lp.model), <span class="fu">resid</span>(lp.model), </span>
<span id="cb54-2"><a href="categorical-data-analysis.html#cb54-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">ylab=</span><span class="st">&quot;Residuals&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Predicted Values&quot;</span>, </span>
<span id="cb54-3"><a href="categorical-data-analysis.html#cb54-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">main=</span><span class="st">&quot;Residuals of Linear Probability Model&quot;</span>) </span>
<span id="cb54-4"><a href="categorical-data-analysis.html#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">0</span>) </span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<p>As we can see from the charts above, the assumptions of ordinary least squares don’t really hold in this situation. Therefore, we should be careful interpreting the results of the model. Maybe a better model won’t have these problems?</p>
</div>
<div id="binary-logistic-regression" class="section level3 hasAnchor" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Binary Logistic Regression<a href="categorical-data-analysis.html#binary-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Due to the limitations of the linear probability model, people typically just use the binary logistic regression model. The logistic regression model does not have the limitations of the linear probability model. The outcome of the logistic regression model is the probability of getting a 1 in a binary variable. That probability is calculated as follows:</p>
<p><span class="math display">\[
p_i = \frac{1}{1+e^{-(\beta_0 + \beta_1x_{1,i} + \cdots + \beta_k x_{k,i})}}
\]</span></p>
<p>This function has the desired properties for predicting probabilities. The predicted probability from the above equation will always be between 0 and 1. The parameter estimates do not enter the function linearly (this is a non-linear regression model), and the rate of change of the probability varies as the predictor variables vary as seen in Figure <a href="categorical-data-analysis.html#fig:logistic">7.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logistic"></span>
<img src="img/logistic.png" alt="Example of a Logistic Curve" width="50%" />
<p class="caption">
Figure 7.1: Example of a Logistic Curve
</p>
</div>
<p>To create a linear model, a <strong>link function</strong> is applied to the probabilities. The specific link function for logistic regression is called the <strong>logit</strong> function.</p>
<p><span class="math display">\[
logit(p_i) = \log(\frac{p_i}{1-p_i}) = \beta_0 + \beta_1x_{1,i} + \cdots + \beta_k x_{k,i}
\]</span></p>
<p>The relationship between the predictor variables and the logits are linear in nature as the logits themselves are unbounded. This structure looks much more like our linear regression model structure. However, logistic regression does not use OLS to estimate the coefficients in our model. OLS requires residuals which the logistic regression model does not provide. The target variable is binary in nature, but the predictions are probabilities. Therefore, we cannot calculate a traditional residual. Instead, logistic regression uses maximum likelihood estimation. This is not covered here.</p>
<p>There are two main assumptions for logistic regression:</p>
<ol style="list-style-type: decimal">
<li>Independence of observations</li>
<li>Linearity of the logit</li>
</ol>
<p>The first assumption of independence is the same as we had for linear regression. The second assumption implies that the logistic function transformation (the logit) actually makes a linear relationship with our predictor variables. This assumption can be tested, but will not be covered in this brief introduction to logistic regression.</p>
<p>Let’s build a logistic regression model. We will use the <code>glm</code> function to do this. The <code>glm</code> function has a similar structure to the <code>lm</code> function. The main difference is the <code>family = binomial(link = "logit")</code> option to specify that we are uses a logistic regression model. Again, there are many different link functions, but only the logistic link function (the logit) is being used here.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="categorical-data-analysis.html#cb55-1" aria-hidden="true" tabindex="-1"></a>ames_logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Bonus <span class="sc">~</span> Gr_Liv_Area, </span>
<span id="cb55-2"><a href="categorical-data-analysis.html#cb55-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> train, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb55-3"><a href="categorical-data-analysis.html#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_logit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Bonus ~ Gr_Liv_Area, family = binomial(link = &quot;logit&quot;), 
##     data = train)
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -6.1348858  0.2757473  -22.25   &lt;2e-16 ***
## Gr_Liv_Area  0.0038463  0.0001799   21.38   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2775.8  on 2050  degrees of freedom
## Residual deviance: 1926.4  on 2049  degrees of freedom
## AIC: 1930.4
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Let’s examine the above output. Scanning down the output, you can see the actual logistic regression equation for the variable <code>Gr_Liv_Area</code>. Here we can see that it appears to be a significant variable at predicting bonus eligibility. However, the coefficient reported does not have the same usable interpretation as in linear regression. An increase of one unit of greater living area square footage is linearly related to the logit <strong>not</strong> the probability of bonus eligibility. We can transform this coefficient to make it more interpretable. A single unit increase in greater living area square footage <strong>does</strong> have a <span class="math inline">\(100 \times (e^\hat{\beta}-1)\%\)</span> increase in the average <strong>odds</strong> of bonus eligibility. We can use a combination of the <code>exp</code> and <code>coef</code> functions to obtain this number.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="categorical-data-analysis.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="dv">100</span><span class="sc">*</span>(<span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="fu">coef</span>(ames_logit), <span class="fu">confint</span>(ames_logit)))<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                               2.5 %      97.5 %
## (Intercept) -99.7834027 -99.8755103 -99.6328865
## Gr_Liv_Area   0.3853699   0.3508132   0.4216532</code></pre>
<p>In other words, every additional square foot in greater living area in the home leads to an average increase in odds of 0.385% to be bonus eligible.</p>
</div>
<div id="adding-categorical-variables" class="section level3 hasAnchor" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Adding Categorical Variables<a href="categorical-data-analysis.html#adding-categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to linear regression as we learned in Chapter <a href="mlr.html#mlr">3</a>, logistic regression can have both continuous and categorical predictors for our categorical target variable. Let’s add both central air as well as number of fireplaces to our logistic regression model.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="categorical-data-analysis.html#cb60-1" aria-hidden="true" tabindex="-1"></a>ames_logit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Bonus <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> Central_Air <span class="sc">+</span> <span class="fu">factor</span>(Fireplaces), </span>
<span id="cb60-2"><a href="categorical-data-analysis.html#cb60-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> train, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>))</span>
<span id="cb60-3"><a href="categorical-data-analysis.html#cb60-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_logit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), 
##     family = binomial(link = &quot;logit&quot;), data = train)
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         -9.970e+00  6.549e-01 -15.223  &lt; 2e-16 ***
## Gr_Liv_Area          3.759e-03  2.031e-04  18.506  &lt; 2e-16 ***
## Central_AirY         3.564e+00  5.310e-01   6.711 1.93e-11 ***
## factor(Fireplaces)1  9.822e-01  1.253e-01   7.837 4.60e-15 ***
## factor(Fireplaces)2  6.734e-01  2.406e-01   2.799  0.00513 ** 
## factor(Fireplaces)3 -3.993e-02  8.711e-01  -0.046  0.96344    
## factor(Fireplaces)4  9.025e+00  3.247e+02   0.028  0.97783    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2775.8  on 2050  degrees of freedom
## Residual deviance: 1746.2  on 2044  degrees of freedom
## AIC: 1760.2
## 
## Number of Fisher Scoring iterations: 11</code></pre>
<p>Just like with linear regression, categorical predictor variables are a comparison between two categories. Again, the coefficients from the logistic regression model need to be transformed to be interpreted.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="categorical-data-analysis.html#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="dv">100</span><span class="sc">*</span>(<span class="fu">exp</span>(<span class="fu">cbind</span>(<span class="fu">coef</span>(ames_logit2), <span class="fu">confint</span>(ames_logit2)))<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>##                                          2.5 %        97.5 %
## (Intercept)         -9.999532e+01  -99.9988238   -99.9843742
## Gr_Liv_Area          3.766413e-01    0.3376191     0.4175879
## Central_AirY         3.429115e+03 1264.8396217 11177.2021481
## factor(Fireplaces)1  1.670410e+02  108.9856565   241.6360910
## factor(Fireplaces)2  9.607987e+01   22.4764491   214.9571145
## factor(Fireplaces)3 -3.914247e+00  -81.9507819   497.4825425
## factor(Fireplaces)4  8.308750e+05 -100.0000000            NA</code></pre>
<p>Let’s use the first fireplace variable as an example. A home with one fireplace has, on average, 167.04% higher odds of being bonus eligible as compared to a home with zero fireplaces.</p>
</div>
<div id="model-assessment" class="section level3 hasAnchor" number="7.4.4">
<h3><span class="header-section-number">7.4.4</span> Model Assessment<a href="categorical-data-analysis.html#model-assessment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are dozens of different ways to evaluate a logistic regression model. We will cover one popular way here - concordance. Counting the number of <strong>concordant</strong>, <strong>discordant</strong>, and <strong>tied</strong> pairs is a way to to assess how well the model fits the data.</p>
<p>To find concordant, discordant, and tied pairs, we must compare all of the 0’s in the target variable to all of the 1’s. For our example, we will compare every pair of homes where one home is bonus eligible and one is not (every 0 and 1 pair). A <strong>concordant</strong> pair is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has a higher predicted probability than the non-bonus eligible home (the 0 in our model) - our model successfully ordered these two observations by probability. It does not matter what the actual predicted probability values are as long as the bonus eligible home has a higher predicted probability than the non-bonus eligible home. A <strong>discordant</strong> pair is a 0 and 1 pair where the bonus eligible home (the 1 in our model) has a lower predicted probability than the non-bonus eligible home (the 0 in our model) - our model unsuccessfully ordered the homes. It does not matter what the actual predicted probability values are as long as the bonus eligible home has a lower predicted probability than the non-bonus eligible home. A <strong>tied</strong> pair is a 0 and 1 pair where the bonus eligible home has the same predicted probability as the non-bonus eligible home - the model is confused and sees these two different things as the same. In general, you want a high percentage of concordant pairs and low percentages of discordant and tied pairs.</p>
<p>We can use the <code>Concordance</code> function to obtain these values on our predictions from the <code>predict</code> function.</p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="categorical-data-analysis.html#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co">#library(InformationValue)</span></span>
<span id="cb64-2"><a href="categorical-data-analysis.html#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="categorical-data-analysis.html#cb64-3" aria-hidden="true" tabindex="-1"></a><span class="co">#Concordance(train$Bonus, predict(ames_logit, type = &quot;response&quot;))</span></span></code></pre></div>
<p>From the above output we have a concordance of 86.3% for our model. There is no good or bad value as this can only be compared with another model to see which is better. Let’s compare this to our model with the categorical variables.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="categorical-data-analysis.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Concordance(train$Bonus, predict(ames_logit2, type = &quot;response&quot;))</span></span></code></pre></div>
<p>We can see that the model with categorical predictors added to it has a higher concordance at 88.4%. That implies that our model is correctly able to rank our observations 88.4% of the time. This is <strong>NOT</strong> the same thing as saying our model is 88.4% accurate. Accuracy (which is not covered here) deals with a prediction being correct or incorrect. Concordance is only measuring how often we are able to predict 1’s with higher probability than 0’s - again, correctly ranking the observations.</p>
</div>
<div id="variable-selection-and-regularized-regression" class="section level3 hasAnchor" number="7.4.5">
<h3><span class="header-section-number">7.4.5</span> Variable Selection and Regularized Regression<a href="categorical-data-analysis.html#variable-selection-and-regularized-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As with linear regression in Chapters <a href="#reg"><strong>??</strong></a> and <a href="sel.html#sel">4</a>, logistic regression uses the same approaches to doing variable selection. In fact, the same function are used as well. Let’s use the <code>step</code> function to apply a forward and backward selection to the logistic regression model.</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="categorical-data-analysis.html#cb66-1" aria-hidden="true" tabindex="-1"></a>train_sel_log <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span> </span>
<span id="cb66-2"><a href="categorical-data-analysis.html#cb66-2" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(Bonus, </span>
<span id="cb66-3"><a href="categorical-data-analysis.html#cb66-3" aria-hidden="true" tabindex="-1"></a>         Lot_Area,</span>
<span id="cb66-4"><a href="categorical-data-analysis.html#cb66-4" aria-hidden="true" tabindex="-1"></a>         Street,</span>
<span id="cb66-5"><a href="categorical-data-analysis.html#cb66-5" aria-hidden="true" tabindex="-1"></a>         Bldg_Type,</span>
<span id="cb66-6"><a href="categorical-data-analysis.html#cb66-6" aria-hidden="true" tabindex="-1"></a>         House_Style,</span>
<span id="cb66-7"><a href="categorical-data-analysis.html#cb66-7" aria-hidden="true" tabindex="-1"></a>         Overall_Qual,</span>
<span id="cb66-8"><a href="categorical-data-analysis.html#cb66-8" aria-hidden="true" tabindex="-1"></a>         Roof_Style,</span>
<span id="cb66-9"><a href="categorical-data-analysis.html#cb66-9" aria-hidden="true" tabindex="-1"></a>         Central_Air,</span>
<span id="cb66-10"><a href="categorical-data-analysis.html#cb66-10" aria-hidden="true" tabindex="-1"></a>         First_Flr_SF,</span>
<span id="cb66-11"><a href="categorical-data-analysis.html#cb66-11" aria-hidden="true" tabindex="-1"></a>         Second_Flr_SF,</span>
<span id="cb66-12"><a href="categorical-data-analysis.html#cb66-12" aria-hidden="true" tabindex="-1"></a>         Full_Bath,</span>
<span id="cb66-13"><a href="categorical-data-analysis.html#cb66-13" aria-hidden="true" tabindex="-1"></a>         Half_Bath,</span>
<span id="cb66-14"><a href="categorical-data-analysis.html#cb66-14" aria-hidden="true" tabindex="-1"></a>         Fireplaces,</span>
<span id="cb66-15"><a href="categorical-data-analysis.html#cb66-15" aria-hidden="true" tabindex="-1"></a>         Garage_Area,</span>
<span id="cb66-16"><a href="categorical-data-analysis.html#cb66-16" aria-hidden="true" tabindex="-1"></a>         Gr_Liv_Area, </span>
<span id="cb66-17"><a href="categorical-data-analysis.html#cb66-17" aria-hidden="true" tabindex="-1"></a>         TotRms_AbvGrd) <span class="sc">%&gt;%</span></span>
<span id="cb66-18"><a href="categorical-data-analysis.html#cb66-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">replace</span>(<span class="fu">is.na</span>(.), <span class="dv">0</span>)</span>
<span id="cb66-19"><a href="categorical-data-analysis.html#cb66-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-20"><a href="categorical-data-analysis.html#cb66-20" aria-hidden="true" tabindex="-1"></a>full.model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Bonus <span class="sc">~</span> . , <span class="at">data =</span> train_sel_log)</span>
<span id="cb66-21"><a href="categorical-data-analysis.html#cb66-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb66-22"><a href="categorical-data-analysis.html#cb66-22" aria-hidden="true" tabindex="-1"></a>empty.model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Bonus <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data =</span> train_sel_log)</span></code></pre></div>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="categorical-data-analysis.html#cb67-1" aria-hidden="true" tabindex="-1"></a>for.model <span class="ot">&lt;-</span> <span class="fu">step</span>(empty.model,</span>
<span id="cb67-2"><a href="categorical-data-analysis.html#cb67-2" aria-hidden="true" tabindex="-1"></a>                  <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> <span class="fu">formula</span>(empty.model),</span>
<span id="cb67-3"><a href="categorical-data-analysis.html#cb67-3" aria-hidden="true" tabindex="-1"></a>                               <span class="at">upper =</span> <span class="fu">formula</span>(full.model)),</span>
<span id="cb67-4"><a href="categorical-data-analysis.html#cb67-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">direction =</span> <span class="st">&quot;forward&quot;</span>, <span class="at">k =</span> <span class="fu">log</span>(<span class="fu">dim</span>(train_sel_log)[<span class="dv">1</span>]))</span></code></pre></div>
<pre><code>## Start:  AIC=2918.59
## Bonus ~ 1
## 
##                 Df Deviance    AIC
## + Overall_Qual   9   234.74 1453.0
## + Full_Bath      1   320.51 2030.8
## + Gr_Liv_Area    1   335.11 2122.1
## + Garage_Area    1   367.62 2312.0
## + First_Flr_SF   1   396.15 2465.3
## + Fireplaces     1   415.26 2561.9
## + TotRms_AbvGrd  1   422.53 2597.6
## + House_Style    7   444.22 2745.9
## + Half_Bath      1   459.40 2769.1
## + Second_Flr_SF  1   461.26 2777.4
## + Central_Air    1   473.64 2831.7
## + Lot_Area       1   478.42 2852.3
## + Bldg_Type      4   477.21 2870.0
## &lt;none&gt;               495.97 2918.6
## + Street         1   495.86 2925.8
## + Roof_Style     5   491.40 2937.7
## 
## Step:  AIC=1452.99
## Bonus ~ Overall_Qual
## 
##                 Df Deviance    AIC
## + Full_Bath      1   203.87 1171.4
## + Gr_Liv_Area    1   208.62 1218.7
## + First_Flr_SF   1   218.70 1315.5
## + Garage_Area    1   223.10 1356.3
## + TotRms_AbvGrd  1   223.12 1356.5
## + Fireplaces     1   223.33 1358.5
## + Lot_Area       1   225.07 1374.3
## + Bldg_Type      4   227.31 1417.6
## + Second_Flr_SF  1   230.98 1427.5
## + Half_Bath      1   231.81 1434.8
## + Central_Air    1   233.55 1450.2
## &lt;none&gt;               234.74 1453.0
## + Street         1   234.30 1456.8
## + House_Style    7   230.28 1467.1
## + Roof_Style     5   234.18 1486.2
## 
## Step:  AIC=1171.42
## Bonus ~ Overall_Qual + Full_Bath
## 
##                 Df Deviance    AIC
## + Fireplaces     1   193.70 1074.1
## + First_Flr_SF   1   195.11 1089.0
## + Gr_Liv_Area    1   196.26 1101.0
## + Lot_Area       1   197.22 1111.0
## + Bldg_Type      4   195.24 1113.2
## + Garage_Area    1   197.63 1115.3
## + Half_Bath      1   201.31 1153.1
## + TotRms_AbvGrd  1   202.51 1165.3
## + Central_Air    1   202.61 1166.4
## &lt;none&gt;               203.87 1171.4
## + Street         1   203.38 1174.2
## + Second_Flr_SF  1   203.85 1178.9
## + House_Style    7   201.34 1199.2
## + Roof_Style     5   203.40 1204.8
## 
## Step:  AIC=1074.14
## Bonus ~ Overall_Qual + Full_Bath + Fireplaces
## 
##                 Df Deviance    AIC
## + Garage_Area    1   188.96 1030.9
## + First_Flr_SF   1   189.34 1035.0
## + Bldg_Type      4   187.45 1037.3
## + Lot_Area       1   190.10 1043.3
## + Gr_Liv_Area    1   190.64 1049.1
## + Half_Bath      1   192.04 1064.0
## &lt;none&gt;               193.70 1074.1
## + Central_Air    1   193.02 1074.5
## + Street         1   193.35 1078.0
## + TotRms_AbvGrd  1   193.42 1078.7
## + Second_Flr_SF  1   193.69 1081.7
## + House_Style    7   190.38 1092.0
## + Roof_Style     5   193.16 1106.5
## 
## Step:  AIC=1030.9
## Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area
## 
##                 Df Deviance    AIC
## + Bldg_Type      4   183.72 1003.8
## + First_Flr_SF   1   186.24 1008.8
## + Lot_Area       1   186.46 1011.2
## + Gr_Liv_Area    1   186.87 1015.7
## + Half_Bath      1   187.55 1023.2
## &lt;none&gt;               188.96 1030.9
## + Street         1   188.68 1035.5
## + Central_Air    1   188.76 1036.3
## + TotRms_AbvGrd  1   188.78 1036.6
## + Second_Flr_SF  1   188.96 1038.5
## + House_Style    7   186.42 1056.5
## + Roof_Style     5   188.20 1060.8
## 
## Step:  AIC=1003.78
## Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
##     Bldg_Type
## 
##                 Df Deviance     AIC
## + First_Flr_SF   1   181.17  982.68
## + Half_Bath      1   181.99  991.99
## + Gr_Liv_Area    1   182.00  992.09
## + Lot_Area       1   182.09  993.12
## &lt;none&gt;               183.72 1003.78
## + Street         1   183.36 1007.35
## + Central_Air    1   183.60 1009.98
## + TotRms_AbvGrd  1   183.66 1010.71
## + Second_Flr_SF  1   183.72 1011.35
## + House_Style    7   180.41 1019.87
## + Roof_Style     5   182.90 1032.69
## 
## Step:  AIC=982.68
## Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
##     Bldg_Type + First_Flr_SF
## 
##                 Df Deviance     AIC
## + Half_Bath      1   177.46  947.94
## + Second_Flr_SF  1   180.10  978.18
## + Lot_Area       1   180.24  979.73
## + Gr_Liv_Area    1   180.28  980.18
## &lt;none&gt;               181.17  982.68
## + Street         1   180.77  985.80
## + House_Style    7   176.95  987.73
## + Central_Air    1   181.06  989.10
## + TotRms_AbvGrd  1   181.16  990.19
## + Roof_Style     5   179.76 1004.77
## 
## Step:  AIC=947.94
## Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
##     Bldg_Type + First_Flr_SF + Half_Bath
## 
##                 Df Deviance    AIC
## + Lot_Area       1   176.75 947.31
## &lt;none&gt;               177.46 947.94
## + TotRms_AbvGrd  1   177.03 950.49
## + Street         1   177.09 951.18
## + Central_Air    1   177.43 955.17
## + Gr_Liv_Area    1   177.46 955.45
## + Second_Flr_SF  1   177.46 955.56
## + Roof_Style     5   175.95 968.54
## + House_Style    7   174.78 970.01
## 
## Step:  AIC=947.31
## Bonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + 
##     Bldg_Type + First_Flr_SF + Half_Bath + Lot_Area
## 
##                 Df Deviance    AIC
## &lt;none&gt;               176.75 947.31
## + TotRms_AbvGrd  1   176.32 949.96
## + Street         1   176.54 952.49
## + Central_Air    1   176.72 954.56
## + Gr_Liv_Area    1   176.73 954.71
## + Second_Flr_SF  1   176.75 954.89
## + Roof_Style     5   175.25 968.01
## + House_Style    7   174.09 969.62</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="categorical-data-analysis.html#cb69-1" aria-hidden="true" tabindex="-1"></a>back.model <span class="ot">&lt;-</span> <span class="fu">step</span>(full.model,</span>
<span id="cb69-2"><a href="categorical-data-analysis.html#cb69-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">scope =</span> <span class="fu">list</span>(<span class="at">lower =</span> <span class="fu">formula</span>(empty.model),</span>
<span id="cb69-3"><a href="categorical-data-analysis.html#cb69-3" aria-hidden="true" tabindex="-1"></a>                                <span class="at">upper =</span> <span class="fu">formula</span>(full.model)),</span>
<span id="cb69-4"><a href="categorical-data-analysis.html#cb69-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">direction =</span> <span class="st">&quot;backward&quot;</span>, <span class="at">k =</span> <span class="fu">log</span>(<span class="fu">dim</span>(train_sel_log)[<span class="dv">1</span>]))</span></code></pre></div>
<pre><code>## Start:  AIC=1016.59
## Bonus ~ Lot_Area + Street + Bldg_Type + House_Style + Overall_Qual + 
##     Roof_Style + Central_Air + First_Flr_SF + Second_Flr_SF + 
##     Full_Bath + Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + 
##     TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - House_Style    7   174.31  995.05
## - Roof_Style     5   173.24  997.67
## - Gr_Liv_Area    1   171.63 1009.07
## - Central_Air    1   171.64 1009.12
## - Second_Flr_SF  1   171.71 1009.95
## - First_Flr_SF   1   171.84 1011.48
## - Street         1   171.89 1012.09
## - TotRms_AbvGrd  1   171.94 1012.72
## - Lot_Area       1   172.08 1014.43
## &lt;none&gt;               171.62 1016.59
## - Garage_Area    1   172.50 1019.38
## - Half_Bath      1   173.37 1029.70
## - Fireplaces     1   173.90 1035.95
## - Bldg_Type      4   177.07 1050.13
## - Full_Bath      1   187.25 1187.64
## - Overall_Qual   9   217.74 1436.04
## 
## Step:  AIC=995.05
## Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + Roof_Style + 
##     Central_Air + First_Flr_SF + Second_Flr_SF + Full_Bath + 
##     Half_Bath + Fireplaces + Garage_Area + Gr_Liv_Area + TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - Roof_Style     5   175.81  974.50
## - Central_Air    1   174.35  987.91
## - Gr_Liv_Area    1   174.43  988.84
## - Second_Flr_SF  1   174.49  989.53
## - Street         1   174.53  989.99
## - TotRms_AbvGrd  1   174.74  992.45
## - Lot_Area       1   174.80  993.19
## - First_Flr_SF   1   174.89  994.22
## &lt;none&gt;               174.31  995.05
## - Garage_Area    1   175.57 1002.23
## - Fireplaces     1   176.38 1011.68
## - Half_Bath      1   176.86 1017.24
## - Bldg_Type      4   179.33 1022.77
## - Full_Bath      1   191.76 1183.08
## - Overall_Qual   9   222.96 1431.28
## 
## Step:  AIC=974.5
## Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + Central_Air + 
##     First_Flr_SF + Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + 
##     Garage_Area + Gr_Liv_Area + TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - Central_Air    1   175.85  967.29
## - Gr_Liv_Area    1   175.91  968.02
## - Second_Flr_SF  1   175.97  968.79
## - Street         1   176.01  969.19
## - TotRms_AbvGrd  1   176.30  972.55
## - Lot_Area       1   176.31  972.65
## - First_Flr_SF   1   176.31  972.66
## &lt;none&gt;               175.81  974.50
## - Garage_Area    1   177.04  981.15
## - Fireplaces     1   177.86  990.64
## - Half_Bath      1   178.19  994.42
## - Bldg_Type      4   180.75 1000.81
## - Full_Bath      1   193.59 1164.44
## - Overall_Qual   9   224.35 1405.92
## 
## Step:  AIC=967.29
## Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + First_Flr_SF + 
##     Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + 
##     Gr_Liv_Area + TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - Gr_Liv_Area    1   175.95  960.84
## - Second_Flr_SF  1   176.01  961.59
## - Street         1   176.03  961.83
## - TotRms_AbvGrd  1   176.34  965.38
## - First_Flr_SF   1   176.35  965.52
## - Lot_Area       1   176.35  965.52
## &lt;none&gt;               175.85  967.29
## - Garage_Area    1   177.19  975.25
## - Fireplaces     1   177.95  984.00
## - Half_Bath      1   178.34  988.55
## - Bldg_Type      4   180.88  994.63
## - Full_Bath      1   193.82 1159.28
## - Overall_Qual   9   224.36 1398.33
## 
## Step:  AIC=960.84
## Bonus ~ Lot_Area + Street + Bldg_Type + Overall_Qual + First_Flr_SF + 
##     Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + 
##     TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - Street         1   176.13  955.40
## - Second_Flr_SF  1   176.14  955.45
## - Lot_Area       1   176.44  959.00
## - TotRms_AbvGrd  1   176.54  960.09
## &lt;none&gt;               175.95  960.84
## - Garage_Area    1   177.32  969.16
## - Fireplaces     1   178.04  977.46
## - Half_Bath      1   178.50  982.75
## - Bldg_Type      4   180.94  987.72
## - First_Flr_SF   1   179.15  990.17
## - Full_Bath      1   193.95 1153.02
## - Overall_Qual   9   224.57 1392.65
## 
## Step:  AIC=955.4
## Bonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + 
##     Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + 
##     TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - Second_Flr_SF  1   176.32  949.96
## - TotRms_AbvGrd  1   176.75  954.89
## - Lot_Area       1   176.78  955.30
## &lt;none&gt;               176.13  955.40
## - Garage_Area    1   177.53  963.91
## - Fireplaces     1   178.25  972.22
## - Half_Bath      1   178.71  977.53
## - Bldg_Type      4   181.03  981.12
## - First_Flr_SF   1   179.28  984.03
## - Full_Bath      1   194.12 1147.16
## - Overall_Qual   9   224.74 1386.53
## 
## Step:  AIC=949.96
## Bonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + 
##     Full_Bath + Half_Bath + Fireplaces + Garage_Area + TotRms_AbvGrd
## 
##                 Df Deviance     AIC
## - TotRms_AbvGrd  1   176.75  947.31
## &lt;none&gt;               176.32  949.96
## - Lot_Area       1   177.03  950.49
## - Garage_Area    1   177.78  959.22
## - Fireplaces     1   178.72  970.00
## - Bldg_Type      4   181.35  977.15
## - Half_Bath      1   180.23  987.27
## - First_Flr_SF   1   180.28  987.88
## - Full_Bath      1   198.37 1183.99
## - Overall_Qual   9   225.34 1384.40
## 
## Step:  AIC=947.31
## Bonus ~ Lot_Area + Bldg_Type + Overall_Qual + First_Flr_SF + 
##     Full_Bath + Half_Bath + Fireplaces + Garage_Area
## 
##                Df Deviance     AIC
## &lt;none&gt;              176.75  947.31
## - Lot_Area      1   177.46  947.94
## - Garage_Area   1   178.30  957.58
## - Fireplaces    1   178.97  965.27
## - Bldg_Type     4   181.72  973.64
## - Half_Bath     1   180.24  979.73
## - First_Flr_SF  1   180.34  980.88
## - Full_Bath     1   199.82 1191.34
## - Overall_Qual  9   225.73 1380.31</code></pre>
<p>In the above two approaches we used the BIC selection criteria. Here both forward and backward selection actually picked the same model. Let’s check the concordance of this model.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="categorical-data-analysis.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Concordance(train$Bonus, predict(back.model, type = &quot;response&quot;))</span></span></code></pre></div>
<p>Not surprisingly, this model outperforms the previous model that we had with a concordance of 96.1%.</p>
<p>Although not covered in detail here, regularized regression can also be applied to logistic regression to get a different view. This might be helpful with the multicollinearity present in these predictor variables. Again, we can use the <code>glmnet</code> function with the addition of a <code>family = "binomial"</code> option.</p>
</div>
<div id="python-code-31" class="section level3 hasAnchor" number="7.4.6">
<h3><span class="header-section-number">7.4.6</span> Python Code<a href="categorical-data-analysis.html#python-code-31" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Linear Probability Model</em></p>
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="categorical-data-analysis.html#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb72-2"><a href="categorical-data-analysis.html#cb72-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-3"><a href="categorical-data-analysis.html#cb72-3" aria-hidden="true" tabindex="-1"></a>lp_model <span class="op">=</span> smf.ols(<span class="st">&quot;Bonus ~ Gr_Liv_Area&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb72-4"><a href="categorical-data-analysis.html#cb72-4" aria-hidden="true" tabindex="-1"></a>lp_model.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>Bonus</td>      <th>  R-squared:         </th> <td>   0.324</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.324</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   983.6</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 04 Jun 2023</td> <th>  Prob (F-statistic):</th> <td>1.11e-176</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:43:13</td>     <th>  Log-Likelihood:    </th> <td> -1052.4</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>   2109.</td> 
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2049</td>      <th>  BIC:               </th> <td>   2120.</td> 
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>   <td>   -0.4149</td> <td>    0.028</td> <td>  -14.944</td> <td> 0.000</td> <td>   -0.469</td> <td>   -0.360</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th> <td>    0.0006</td> <td> 1.76e-05</td> <td>   31.362</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.621</td> <th>  Durbin-Watson:     </th> <td>   2.012</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.445</td> <th>  Jarque-Bera (JB):  </th> <td>   1.587</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.068</td> <th>  Prob(JB):          </th> <td>   0.452</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.013</td> <th>  Cond. No.          </th> <td>4.89e+03</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.89e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
<div class="sourceCode" id="cb73"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb73-1"><a href="categorical-data-analysis.html#cb73-1" aria-hidden="true" tabindex="-1"></a>sm.api.qqplot(lp_model.resid)</span>
<span id="cb73-2"><a href="categorical-data-analysis.html#cb73-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="categorical-data-analysis.html#cb74-1" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;pred_lp&#39;</span>] <span class="op">=</span> lp_model.predict()</span>
<span id="cb74-2"><a href="categorical-data-analysis.html#cb74-2" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;resid_lp&#39;</span>] <span class="op">=</span> lp_model.resid</span>
<span id="cb74-3"><a href="categorical-data-analysis.html#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="categorical-data-analysis.html#cb74-4" aria-hidden="true" tabindex="-1"></a>train[[<span class="st">&#39;Bonus&#39;</span>, <span class="st">&#39;pred_lp&#39;</span>, <span class="st">&#39;resid_lp&#39;</span>]].head(n <span class="op">=</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##    Bonus   pred_lp  resid_lp
## 0      1  0.409204  0.590796
## 1      0  0.428574 -0.428574
## 2      0  0.514912 -0.514912
## 3      1  0.666002  0.333998
## 4      0  0.211624 -0.211624
## 5      0  0.276931 -0.276931
## 6      1  0.514358  0.485642
## 7      0  0.258113 -0.258113
## 8      0  0.069942 -0.069942
## 9      0  0.063301 -0.063301</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="categorical-data-analysis.html#cb76-1" aria-hidden="true" tabindex="-1"></a>plt.cla()</span>
<span id="cb76-2"><a href="categorical-data-analysis.html#cb76-2" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.relplot(data <span class="op">=</span> train, y <span class="op">=</span> <span class="st">&quot;resid_lp&quot;</span>, x <span class="op">=</span> <span class="st">&quot;pred_lp&quot;</span>)</span>
<span id="cb76-3"><a href="categorical-data-analysis.html#cb76-3" aria-hidden="true" tabindex="-1"></a>ax.<span class="bu">set</span>(ylabel <span class="op">=</span> <span class="st">&#39;Residuals&#39;</span>,</span>
<span id="cb76-4"><a href="categorical-data-analysis.html#cb76-4" aria-hidden="true" tabindex="-1"></a>       xlabel <span class="op">=</span> <span class="st">&#39;Predicted Probability of Bonus&#39;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-42-3.png" width="245" /></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="categorical-data-analysis.html#cb77-1" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-42-4.png" width="480" /></p>
<p><em>Binary Logistic Regression</em></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="categorical-data-analysis.html#cb78-1" aria-hidden="true" tabindex="-1"></a>log_model <span class="op">=</span> smf.logit(<span class="st">&quot;Bonus ~ Gr_Liv_Area&quot;</span>, data <span class="op">=</span> train).fit()</span></code></pre></div>
<pre><code>## Optimization terminated successfully.
##          Current function value: 0.469614
##          Iterations 7</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="categorical-data-analysis.html#cb80-1" aria-hidden="true" tabindex="-1"></a>log_model.summary()</span></code></pre></div>
<table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>Bonus</td>      <th>  No. Observations:  </th>   <td>  2051</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  2049</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 04 Jun 2023</td> <th>  Pseudo R-squ.:     </th>   <td>0.3060</td>  
</tr>
<tr>
  <th>Time:</th>                <td>13:43:15</td>     <th>  Log-Likelihood:    </th>  <td> -963.18</td> 
</tr>
<tr>
  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1387.9</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>9.553e-187</td>
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>   <td>   -6.1349</td> <td>    0.276</td> <td>  -22.248</td> <td> 0.000</td> <td>   -6.675</td> <td>   -5.594</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th> <td>    0.0038</td> <td>    0.000</td> <td>   21.375</td> <td> 0.000</td> <td>    0.003</td> <td>    0.004</td>
</tr>
</table>
<div class="sourceCode" id="cb81"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb81-1"><a href="categorical-data-analysis.html#cb81-1" aria-hidden="true" tabindex="-1"></a>odds_ratio <span class="op">=</span> <span class="dv">100</span><span class="op">*</span>(np.exp(log_model.params) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb81-2"><a href="categorical-data-analysis.html#cb81-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-3"><a href="categorical-data-analysis.html#cb81-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(odds_ratio)</span></code></pre></div>
<pre><code>## Intercept     -99.783403
## Gr_Liv_Area     0.385370
## dtype: float64</code></pre>
<p><em>Adding Categorical Variables</em></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb83-1"><a href="categorical-data-analysis.html#cb83-1" aria-hidden="true" tabindex="-1"></a>log_model2 <span class="op">=</span> smf.logit(<span class="st">&quot;Bonus ~ Gr_Liv_Area + C(Central_Air) + C(Fireplaces)&quot;</span>, data <span class="op">=</span> train).fit()</span></code></pre></div>
<pre><code>## Warning: Maximum number of iterations has been exceeded.
##          Current function value: 0.425703
##          Iterations: 35
## 
## C:\PROGRA~3\ANACON~1\lib\site-packages\statsmodels\base\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals
##   warnings.warn(&quot;Maximum Likelihood optimization failed to &quot;</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="categorical-data-analysis.html#cb85-1" aria-hidden="true" tabindex="-1"></a>log_model2.summary()</span></code></pre></div>
<table class="simpletable">
<caption>Logit Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>Bonus</td>      <th>  No. Observations:  </th>   <td>  2051</td>  
</tr>
<tr>
  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  2044</td>  
</tr>
<tr>
  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     6</td>  
</tr>
<tr>
  <th>Date:</th>            <td>Sun, 04 Jun 2023</td> <th>  Pseudo R-squ.:     </th>   <td>0.3709</td>  
</tr>
<tr>
  <th>Time:</th>                <td>13:43:16</td>     <th>  Log-Likelihood:    </th>  <td> -873.12</td> 
</tr>
<tr>
  <th>converged:</th>             <td>False</td>      <th>  LL-Null:           </th>  <td> -1387.9</td> 
</tr>
<tr>
  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>3.585e-219</td>
</tr>
</table>
<table class="simpletable">
<tr>
           <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>           <td>   -9.9700</td> <td>    0.655</td> <td>  -15.223</td> <td> 0.000</td> <td>  -11.254</td> <td>   -8.686</td>
</tr>
<tr>
  <th>C(Central_Air)[T.Y]</th> <td>    3.5636</td> <td>    0.531</td> <td>    6.711</td> <td> 0.000</td> <td>    2.523</td> <td>    4.604</td>
</tr>
<tr>
  <th>C(Fireplaces)[T.1]</th>  <td>    0.9822</td> <td>    0.125</td> <td>    7.837</td> <td> 0.000</td> <td>    0.737</td> <td>    1.228</td>
</tr>
<tr>
  <th>C(Fireplaces)[T.2]</th>  <td>    0.6734</td> <td>    0.241</td> <td>    2.799</td> <td> 0.005</td> <td>    0.202</td> <td>    1.145</td>
</tr>
<tr>
  <th>C(Fireplaces)[T.3]</th>  <td>   -0.0399</td> <td>    0.871</td> <td>   -0.046</td> <td> 0.963</td> <td>   -1.747</td> <td>    1.667</td>
</tr>
<tr>
  <th>C(Fireplaces)[T.4]</th>  <td>   17.6306</td> <td> 3.96e+04</td> <td>    0.000</td> <td> 1.000</td> <td>-7.75e+04</td> <td> 7.76e+04</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th>         <td>    0.0038</td> <td>    0.000</td> <td>   18.506</td> <td> 0.000</td> <td>    0.003</td> <td>    0.004</td>
</tr>
</table>
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="categorical-data-analysis.html#cb86-1" aria-hidden="true" tabindex="-1"></a>odds_ratio <span class="op">=</span> <span class="dv">100</span><span class="op">*</span>(np.exp(log_model2.params) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb86-2"><a href="categorical-data-analysis.html#cb86-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb86-3"><a href="categorical-data-analysis.html#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(odds_ratio)</span></code></pre></div>
<pre><code>## Intercept             -9.999532e+01
## C(Central_Air)[T.Y]    3.429115e+03
## C(Fireplaces)[T.1]     1.670410e+02
## C(Fireplaces)[T.2]     9.607987e+01
## C(Fireplaces)[T.3]    -3.914247e+00
## C(Fireplaces)[T.4]     4.537869e+09
## Gr_Liv_Area            3.766413e-01
## dtype: float64</code></pre>
<p><em>Model Assessment</em></p>
<p>Python doesn’t have concordant / discordant pair calculations. We will learn in Fall semester other metrics to evaluate a logistic regression model that Python does have.</p>
<p><em>Variable Selection and Regularized Regression</em></p>
<p>Python does NOT have nice capabilities to do variable selection automatically in statsmodels, scikitlearn, or scipy. All resources I can find involve downloading and installing a package (mlxtend) that is not included by default in anaconda or writing your own function. Scikit learn has something similar but uses the model’s coefficients (!!!) to select, not p-values. Scikit learn can do this by evaluating a metric on cross-validation, but that is not covered until machine learning in Fall 3.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="model-building-scoring-for-prediction.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/edit/master/07-categorical.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/blob/master/07-categorical.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"toc": null
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
