<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations</title>
  <meta name="description" content="Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="IAA-Faculty/statistical_foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="slr.html"/>
<link rel="next" href="model-selection.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a style="font-weight:bold, text-align:center" href="https://github.com/IAA-Faculty/statistical_foundations/">Statistical Foundations</a>
<img src="./img/iaaicon.png" alt="IAA"  class="center"</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-stat.html"><a href="intro-stat.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-stat.html"><a href="intro-stat.html#eda"><i class="fa fa-check"></i><b>1.1</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro-stat.html"><a href="intro-stat.html#vartypes"><i class="fa fa-check"></i><b>1.1.1</b> Types of Variables</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-stat.html"><a href="intro-stat.html#nominal-variables"><i class="fa fa-check"></i><b>1.1.2</b> Nominal Variables</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-stat.html"><a href="intro-stat.html#interval-variables"><i class="fa fa-check"></i><b>1.1.3</b> Interval Variables</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-stat.html"><a href="intro-stat.html#ordinal-variables"><i class="fa fa-check"></i><b>1.1.4</b> Ordinal Variables</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-stat.html"><a href="intro-stat.html#distributions"><i class="fa fa-check"></i><b>1.1.5</b> Distributions</a></li>
<li class="chapter" data-level="1.1.6" data-path="intro-stat.html"><a href="intro-stat.html#location"><i class="fa fa-check"></i><b>1.1.6</b> Location</a></li>
<li class="chapter" data-level="1.1.7" data-path="intro-stat.html"><a href="intro-stat.html#spread"><i class="fa fa-check"></i><b>1.1.7</b> Spread</a></li>
<li class="chapter" data-level="1.1.8" data-path="intro-stat.html"><a href="intro-stat.html#shape"><i class="fa fa-check"></i><b>1.1.8</b> Shape</a></li>
<li class="chapter" data-level="1.1.9" data-path="intro-stat.html"><a href="intro-stat.html#normal"><i class="fa fa-check"></i><b>1.1.9</b> The Normal Distribution</a></li>
<li class="chapter" data-level="1.1.10" data-path="intro-stat.html"><a href="intro-stat.html#skew"><i class="fa fa-check"></i><b>1.1.10</b> Skewness</a></li>
<li class="chapter" data-level="1.1.11" data-path="intro-stat.html"><a href="intro-stat.html#kurt"><i class="fa fa-check"></i><b>1.1.11</b> Kurtosis</a></li>
<li class="chapter" data-level="1.1.12" data-path="intro-stat.html"><a href="intro-stat.html#graphdist"><i class="fa fa-check"></i><b>1.1.12</b> Graphical Displays of Distributions</a></li>
<li class="chapter" data-level="1.1.13" data-path="intro-stat.html"><a href="intro-stat.html#histograms"><i class="fa fa-check"></i><b>1.1.13</b> Histograms</a></li>
<li class="chapter" data-level="1.1.14" data-path="intro-stat.html"><a href="intro-stat.html#normal-probability-plots-qq-plots"><i class="fa fa-check"></i><b>1.1.14</b> Normal probability plots (QQ Plots)</a></li>
<li class="chapter" data-level="1.1.15" data-path="intro-stat.html"><a href="intro-stat.html#box-plots"><i class="fa fa-check"></i><b>1.1.15</b> Box Plots</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro-stat.html"><a href="intro-stat.html#pointest"><i class="fa fa-check"></i><b>1.2</b> Point Estimates</a></li>
<li class="chapter" data-level="1.3" data-path="intro-stat.html"><a href="intro-stat.html#ci"><i class="fa fa-check"></i><b>1.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="1.4" data-path="intro-stat.html"><a href="intro-stat.html#hypotest"><i class="fa fa-check"></i><b>1.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro-stat.html"><a href="intro-stat.html#onesample"><i class="fa fa-check"></i><b>1.4.1</b> One-Sample T-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-stat.html"><a href="intro-stat.html#two-sample-t-tests"><i class="fa fa-check"></i><b>1.5</b> Two-Sample t-tests</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-stat.html"><a href="intro-stat.html#testnorm"><i class="fa fa-check"></i><b>1.5.1</b> Testing Normality of Groups</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-stat.html"><a href="intro-stat.html#ftest"><i class="fa fa-check"></i><b>1.5.2</b> Testing Equality of Variances</a></li>
<li class="chapter" data-level="1.5.3" data-path="intro-stat.html"><a href="intro-stat.html#tsttest"><i class="fa fa-check"></i><b>1.5.3</b> Testing Equality of Means</a></li>
<li class="chapter" data-level="1.5.4" data-path="intro-stat.html"><a href="intro-stat.html#wilcoxon"><i class="fa fa-check"></i><b>1.5.4</b> Mann-Whitney-Wilcoxon Test</a></li>
<li class="chapter" data-level="1.5.5" data-path="intro-stat.html"><a href="intro-stat.html#bootstrap"><i class="fa fa-check"></i><b>1.5.5</b> Bootstrap</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="slr.html"><a href="slr.html"><i class="fa fa-check"></i><b>2</b> Introduction to ANOVA and Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="slr.html"><a href="slr.html#evp"><i class="fa fa-check"></i><b>2.1</b> Predictive vs. Explanatory</a></li>
<li class="chapter" data-level="2.2" data-path="slr.html"><a href="slr.html#trainvalidtest"><i class="fa fa-check"></i><b>2.2</b> Honest Assessment</a></li>
<li class="chapter" data-level="2.3" data-path="slr.html"><a href="slr.html#bivariate-eda"><i class="fa fa-check"></i><b>2.3</b> Bivariate EDA</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="slr.html"><a href="slr.html#continuous-continuous-associations"><i class="fa fa-check"></i><b>2.3.1</b> Continuous-Continuous Associations</a></li>
<li class="chapter" data-level="2.3.2" data-path="slr.html"><a href="slr.html#continuous-categorical-associations"><i class="fa fa-check"></i><b>2.3.2</b> Continuous-Categorical Associations</a></li>
<li class="chapter" data-level="2.3.3" data-path="slr.html"><a href="slr.html#python-code-12"><i class="fa fa-check"></i><b>2.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="slr.html"><a href="slr.html#oneway"><i class="fa fa-check"></i><b>2.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="slr.html"><a href="slr.html#kruskal"><i class="fa fa-check"></i><b>2.4.1</b> Kruskal-Wallis</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="slr.html"><a href="slr.html#posthoc"><i class="fa fa-check"></i><b>2.5</b> ANOVA Post-hoc Testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="slr.html"><a href="slr.html#tukey"><i class="fa fa-check"></i><b>2.5.1</b> Tukey-Kramer</a></li>
<li class="chapter" data-level="2.5.2" data-path="slr.html"><a href="slr.html#dunnett"><i class="fa fa-check"></i><b>2.5.2</b> Dunnett’s Test</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="slr.html"><a href="slr.html#cor"><i class="fa fa-check"></i><b>2.6</b> Pearson Correlation</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="slr.html"><a href="slr.html#testcor"><i class="fa fa-check"></i><b>2.6.1</b> Statistical Test</a></li>
<li class="chapter" data-level="2.6.2" data-path="slr.html"><a href="slr.html#effect-of-anomalous-observations"><i class="fa fa-check"></i><b>2.6.2</b> Effect of Anomalous Observations</a></li>
<li class="chapter" data-level="2.6.3" data-path="slr.html"><a href="slr.html#the-correlation-matrix"><i class="fa fa-check"></i><b>2.6.3</b> The Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="slr.html"><a href="slr.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="slr.html"><a href="slr.html#slrassumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions of Linear Regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="slr.html"><a href="slr.html#testing-for-association"><i class="fa fa-check"></i><b>2.7.2</b> Testing for Association</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Complex ANOVA and Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#two-way-anova"><i class="fa fa-check"></i><b>3.1</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mlr.html"><a href="mlr.html#exploration"><i class="fa fa-check"></i><b>3.1.1</b> Exploration</a></li>
<li class="chapter" data-level="3.1.2" data-path="mlr.html"><a href="mlr.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
<li class="chapter" data-level="3.1.3" data-path="mlr.html"><a href="mlr.html#post-hoc-testing"><i class="fa fa-check"></i><b>3.1.3</b> Post-Hoc Testing</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#two-way-anova-with-interactions"><i class="fa fa-check"></i><b>3.2</b> Two-Way ANOVA with Interactions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#post-hoc-testing-1"><i class="fa fa-check"></i><b>3.2.1</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.2.2" data-path="mlr.html"><a href="mlr.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#randomized-block-design"><i class="fa fa-check"></i><b>3.3</b> Randomized Block Design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mlr.html"><a href="mlr.html#garlic-bulb-weight-example"><i class="fa fa-check"></i><b>3.3.1</b> Garlic Bulb Weight Example</a></li>
<li class="chapter" data-level="3.3.2" data-path="mlr.html"><a href="mlr.html#assumptions-1"><i class="fa fa-check"></i><b>3.3.2</b> Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model Structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="mlr.html"><a href="mlr.html#global-local-inference"><i class="fa fa-check"></i><b>3.4.2</b> Global &amp; Local Inference</a></li>
<li class="chapter" data-level="3.4.3" data-path="mlr.html"><a href="mlr.html#assumptions-2"><i class="fa fa-check"></i><b>3.4.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.4.4" data-path="mlr.html"><a href="mlr.html#multiple-coefficients-of-determination"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Coefficients of Determination</a></li>
<li class="chapter" data-level="3.4.5" data-path="mlr.html"><a href="mlr.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>3.4.5</b> Categorical Predictor Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>4</b> Model Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-selection.html"><a href="model-selection.html#selection-criteria"><i class="fa fa-check"></i><b>4.1</b> Selection Criteria</a></li>
<li class="chapter" data-level="4.2" data-path="model-selection.html"><a href="model-selection.html#stepwise-selection"><i class="fa fa-check"></i><b>4.2</b> Stepwise Selection</a>
<ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward"><i class="fa fa-check"></i>Backward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#stepwise"><i class="fa fa-check"></i>Stepwise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-selection.html"><a href="model-selection.html#significance-levels"><i class="fa fa-check"></i><b>4.3</b> Significance Levels</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-selection.html"><a href="model-selection.html#python-code-22"><i class="fa fa-check"></i><b>4.3.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>5</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="diag.html"><a href="diag.html#python-code-23"><i class="fa fa-check"></i><b>5.0.1</b> Python Code</a></li>
<li class="chapter" data-level="5.1" data-path="diag.html"><a href="diag.html#examining-residuals"><i class="fa fa-check"></i><b>5.1</b> Examining Residuals</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="diag.html"><a href="diag.html#python-code-24"><i class="fa fa-check"></i><b>5.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="diag.html"><a href="diag.html#misspecified-model"><i class="fa fa-check"></i><b>5.2</b> Misspecified Model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="diag.html"><a href="diag.html#python-code-25"><i class="fa fa-check"></i><b>5.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="diag.html"><a href="diag.html#constant-variance"><i class="fa fa-check"></i><b>5.3</b> Constant Variance</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="diag.html"><a href="diag.html#python-code-26"><i class="fa fa-check"></i><b>5.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="diag.html"><a href="diag.html#normality"><i class="fa fa-check"></i><b>5.4</b> Normality</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="diag.html"><a href="diag.html#python-code-27"><i class="fa fa-check"></i><b>5.4.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="diag.html"><a href="diag.html#correlated-errors"><i class="fa fa-check"></i><b>5.5</b> Correlated Errors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="diag.html"><a href="diag.html#python-code-28"><i class="fa fa-check"></i><b>5.5.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="diag.html"><a href="diag.html#influential-observations-and-outliers"><i class="fa fa-check"></i><b>5.6</b> Influential Observations and Outliers</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="diag.html"><a href="diag.html#python-code-29"><i class="fa fa-check"></i><b>5.6.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="diag.html"><a href="diag.html#multicollinearity"><i class="fa fa-check"></i><b>5.7</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="diag.html"><a href="diag.html#python-code-30"><i class="fa fa-check"></i><b>5.7.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html"><i class="fa fa-check"></i><b>6</b> Model Building and Scoring for Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#regularized-regression"><i class="fa fa-check"></i><b>6.1</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#penalties-in-models"><i class="fa fa-check"></i><b>6.1.1</b> Penalties in Models</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#lasso"><i class="fa fa-check"></i><b>6.1.3</b> LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="6.1.5" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-31"><i class="fa fa-check"></i><b>6.1.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#optimizing-penalties"><i class="fa fa-check"></i><b>6.2</b> Optimizing Penalties</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cross-validation"><i class="fa fa-check"></i><b>6.2.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cv-in-regularized-regression"><i class="fa fa-check"></i><b>6.2.2</b> CV in Regularized Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-comparisons"><i class="fa fa-check"></i><b>6.3</b> Model Comparisons</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-metrics"><i class="fa fa-check"></i><b>6.3.1</b> Model Metrics</a></li>
<li class="chapter" data-level="6.3.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#test-dataset-comparison"><i class="fa fa-check"></i><b>6.3.2</b> Test Dataset Comparison</a></li>
<li class="chapter" data-level="6.3.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-32"><i class="fa fa-check"></i><b>6.3.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#describing-categorical-data"><i class="fa fa-check"></i><b>7.1</b> Describing Categorical Data</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-33"><i class="fa fa-check"></i><b>7.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#tests-of-association"><i class="fa fa-check"></i><b>7.2</b> Tests of Association</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-34"><i class="fa fa-check"></i><b>7.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#measures-of-association"><i class="fa fa-check"></i><b>7.3</b> Measures of Association</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-35"><i class="fa fa-check"></i><b>7.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Introduction to Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#linear-probability-model"><i class="fa fa-check"></i><b>7.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.4.2</b> Binary Logistic Regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#adding-categorical-variables"><i class="fa fa-check"></i><b>7.4.3</b> Adding Categorical Variables</a></li>
<li class="chapter" data-level="7.4.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#model-assessment"><i class="fa fa-check"></i><b>7.4.4</b> Model Assessment</a></li>
<li class="chapter" data-level="7.4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#variable-selection-and-regularized-regression"><i class="fa fa-check"></i><b>7.4.5</b> Variable Selection and Regularized Regression</a></li>
<li class="chapter" data-level="7.4.6" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-36"><i class="fa fa-check"></i><b>7.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mlr" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Complex ANOVA and Multiple Linear Regression<a href="mlr.html#mlr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the One-Way ANOVA and simple linear regression models, there was only one variable - categorical for ANOVA and continuous for simple linear regression - to explain and predict our target variable. Rarely do we believe that only a single variable will suffice in predicting a variable of interest. Here in this Chapter we will generalize these models to the <span class="math inline">\(n\)</span>-Way ANOVA and multiple linear regression models. These models contain multiple sets of variables to explain and predict our target variable.</p>
<p>This Chapter aims to answer the following questions:</p>
<ul>
<li>How do we include multiple variables in ANOVA?
<ul>
<li>Exploration</li>
<li>Assumptions</li>
<li>Predictions</li>
</ul></li>
<li>What is an interaction between two predictor variables?
<ul>
<li>Interpretation</li>
<li>Evaluation</li>
<li>Within Category Effects</li>
</ul></li>
<li>What is blocking in ANOVA?
<ul>
<li>Nuisance Factors</li>
<li>Differences Between Blocking and Two-Way ANOVA</li>
</ul></li>
<li>How do we include multiple variables in regression?
<ul>
<li>Model Structure</li>
<li>Global &amp; Local Inference</li>
<li>Assumptions</li>
<li>Adjusted <span class="math inline">\(R^2\)</span></li>
<li>Categorical Variables in Regression</li>
</ul></li>
</ul>
<div id="two-way-anova" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Two-Way ANOVA<a href="mlr.html#two-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the One-Way ANOVA model, we used a single categorical predictor variable with <span class="math inline">\(k\)</span> levels to predict our continuous target variable. Now we will generalize this model to include <span class="math inline">\(n\)</span> categorical variables that each have different numbers of levels (<span class="math inline">\(k_1, k_2, ..., k_n\)</span>).</p>
<div id="exploration" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Exploration<a href="mlr.html#exploration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s use the basic example of two categorical predictor variables in a Two-Way ANOVA. Previously, we talked about using heating quality as a factor to explain and predict sale price of homes in Ames, Iowa. Now, we also consider whether the home has central air. Although similar in nature, these two factors potentially provide important, unique pieces of information about the home. Similar to previous data science problems, let us first explore our variables and their potential relationships.</p>
<p>Now that we have two variables that we will use to explain and predict sale price, here are some summary statistics (mean, standard deviation, minimum, and maximum) for each combination of category. We will use the <code>group_by</code> function on both predictor variables of interest to split the data and then the <code>summarise</code> function to calculate the metrics we are interested in.</p>
<div id="r-code-18" class="section level4 hasAnchor" number="3.1.1.1">
<h4><span class="header-section-number">3.1.1.1</span> R code:<a href="mlr.html#r-code-18" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="mlr.html#cb1-1" tabindex="-1"></a>train <span class="sc">%&gt;%</span> </span>
<span id="cb1-2"><a href="mlr.html#cb1-2" tabindex="-1"></a>  <span class="fu">group_by</span>(Heating_QC, Central_Air) <span class="sc">%&gt;%</span></span>
<span id="cb1-3"><a href="mlr.html#cb1-3" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price), </span>
<span id="cb1-4"><a href="mlr.html#cb1-4" tabindex="-1"></a>            <span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price), </span>
<span id="cb1-5"><a href="mlr.html#cb1-5" tabindex="-1"></a>            <span class="at">max =</span> <span class="fu">max</span>(Sale_Price), </span>
<span id="cb1-6"><a href="mlr.html#cb1-6" tabindex="-1"></a>            <span class="at">min =</span> <span class="fu">min</span>(Sale_Price),</span>
<span id="cb1-7"><a href="mlr.html#cb1-7" tabindex="-1"></a>            <span class="at">n =</span> <span class="fu">n</span>())</span></code></pre></div>
<pre><code>## `summarise()` has grouped output by &#39;Heating_QC&#39;. You can override using the
## `.groups` argument.</code></pre>
<pre><code>## # A tibble: 10 × 7
## # Groups:   Heating_QC [5]
##    Heating_QC Central_Air    mean     sd    max    min     n
##    &lt;ord&gt;      &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt;
##  1 Poor       N            50050  52255.  87000  13100     2
##  2 Poor       Y           107000     NA  107000 107000     1
##  3 Fair       N            84748. 28267. 158000  37900    29
##  4 Fair       Y           145165. 38624. 230000  50000    36
##  5 Typical    N           103469. 34663. 209500  12789    82
##  6 Typical    Y           142003. 39657. 375000  60000   527
##  7 Good       N           110811. 38455. 214500  59000    23
##  8 Good       Y           160113. 54158. 415000  52000   318
##  9 Excellent  N           115062. 33271. 184900  64000    11
## 10 Excellent  Y           216401. 88518. 745000  58500  1022</code></pre>
<p>We can already see above that there appears to be some differences in average sale price across the categories overall. Within each grouping of heating quality, homes with central air appear to have a higher sale price than homes without. Also, similar to before, homes with higher heating quality appear to have higher sale prices compared to homes with lower heating quality. Careful about samepl size in these situations though!</p>
<p>We also see these relationships in the bar chart in Figure <a href="mlr.html#fig:twomeans">3.1</a>.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="mlr.html#cb4-1" tabindex="-1"></a>CA_heat <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span> </span>
<span id="cb4-2"><a href="mlr.html#cb4-2" tabindex="-1"></a>  <span class="fu">group_by</span>(Heating_QC, Central_Air) <span class="sc">%&gt;%</span></span>
<span id="cb4-3"><a href="mlr.html#cb4-3" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price), </span>
<span id="cb4-4"><a href="mlr.html#cb4-4" tabindex="-1"></a>            <span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price), </span>
<span id="cb4-5"><a href="mlr.html#cb4-5" tabindex="-1"></a>            <span class="at">max =</span> <span class="fu">max</span>(Sale_Price), </span>
<span id="cb4-6"><a href="mlr.html#cb4-6" tabindex="-1"></a>            <span class="at">min =</span> <span class="fu">min</span>(Sale_Price), </span>
<span id="cb4-7"><a href="mlr.html#cb4-7" tabindex="-1"></a>            <span class="at">n =</span> <span class="fu">n</span>())</span></code></pre></div>
<pre><code>## `summarise()` has grouped output by &#39;Heating_QC&#39;. You can override using the
## `.groups` argument.</code></pre>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="mlr.html#cb6-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> CA_heat, <span class="fu">aes</span>(<span class="at">x =</span> Heating_QC, <span class="at">y =</span> mean<span class="sc">/</span><span class="dv">1000</span>, <span class="at">fill =</span> Central_Air)) <span class="sc">+</span></span>
<span id="cb6-2"><a href="mlr.html#cb6-2" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">position =</span> <span class="fu">position_dodge</span>()) <span class="sc">+</span></span>
<span id="cb6-3"><a href="mlr.html#cb6-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Heating Quality Category&quot;</span>) <span class="sc">+</span></span>
<span id="cb6-4"><a href="mlr.html#cb6-4" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Paired&quot;</span>) <span class="sc">+</span></span>
<span id="cb6-5"><a href="mlr.html#cb6-5" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twomeans"></span>
<img src="bookdownproj_files/figure-html/twomeans-1.png" alt="Distribution of Variables Heating_QC and Central_Air" width="672" />
<p class="caption">
Figure 3.1: Distribution of Variables Heating_QC and Central_Air
</p>
</div>
<p>As before, visually looking at bar charts and mean calculations only goes so far. We need to statistically be sure of any differences that exist between average sale price in categories.</p>
</div>
</div>
<div id="model" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Model<a href="mlr.html#model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are going to do this with the same approach as in the One-Way ANOVA, just with more variables as shown in the following equation:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + \varepsilon_{ijk}
\]</span>
where <span class="math inline">\(\mu\)</span> is the average baseline sales price of a home in Ames, Iowa, <span class="math inline">\(\alpha_i\)</span> is the variable representing the impacts of the levels of heating quality, and <span class="math inline">\(\beta_j\)</span> is the variable representing the impacts of the levels of central air. As mentioned previously, the unexplained error in this model is represented as <span class="math inline">\(\varepsilon_{ijk}\)</span>.</p>
<p>The same F test approach is also used, just for each one of the variables. Each variable’s test has a null hypothesis assuming all categories have the same mean. The alternative for each test is that at least one category’s mean is different.</p>
<p>Let’s view the results of the <code>aov</code> function.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="mlr.html#cb7-1" tabindex="-1"></a>ames_aov2 <span class="ot">&lt;-</span> <span class="fu">aov</span>(Sale_Price <span class="sc">~</span> Heating_QC <span class="sc">+</span> Central_Air, <span class="at">data =</span> train)</span>
<span id="cb7-2"><a href="mlr.html#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="mlr.html#cb7-3" tabindex="-1"></a><span class="fu">summary</span>(ames_aov2)</span></code></pre></div>
<pre><code>##               Df    Sum Sq   Mean Sq F value   Pr(&gt;F)    
## Heating_QC     4 2.891e+12 7.228e+11  147.60  &lt; 2e-16 ***
## Central_Air    1 2.903e+11 2.903e+11   59.28 2.11e-14 ***
## Residuals   2045 1.002e+13 4.897e+09                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From the above results, we have low p-values for each of the variables’ F test. Heating quality had 4 degrees of freedom, derived from the 5 categories <span class="math inline">\((4 = 5-1)\)</span>. Similarly, central air’s 2 categories produce 1 <span class="math inline">\((= 2-1)\)</span> degree of freedom. The F values are calculated the exact same way as described before with the mean square for each variable divided by the mean square error. Based on these tests, at least one category in each variable is statistically different than the rest.</p>
</div>
<div id="post-hoc-testing" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Post-Hoc Testing<a href="mlr.html#post-hoc-testing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As with the One-Way ANOVA, the next logical question is which of these categories is different. We will use the same post-hoc tests as before with the <code>TukeyHSD</code> function.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="mlr.html#cb9-1" tabindex="-1"></a>tukey.ames2 <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(ames_aov2)</span>
<span id="cb9-2"><a href="mlr.html#cb9-2" tabindex="-1"></a><span class="fu">print</span>(tukey.ames2)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Sale_Price ~ Heating_QC + Central_Air, data = train)
## 
## $Heating_QC
##                        diff        lwr       upr     p adj
## Fair-Poor          49176.42 -63650.448 162003.29 0.7571980
## Typical-Poor       67781.01 -42800.320 178362.35 0.4506761
## Good-Poor          87753.89 -23040.253 198548.03 0.1945181
## Excellent-Poor    146288.89  35818.859 256758.92 0.0028361
## Typical-Fair       18604.59  -6326.425  43535.61 0.2484556
## Good-Fair          38577.47  12718.894  64436.04 0.0004622
## Excellent-Fair     97112.47  72679.867 121545.07 0.0000000
## Good-Typical       19972.87   7050.230  32895.52 0.0002470
## Excellent-Typical  78507.88  68746.678  88269.07 0.0000000
## Excellent-Good     58535.00  46602.229  70467.78 0.0000000
## 
## $Central_Air
##         diff      lwr      upr p adj
## Y-N 43256.57 31508.27 55004.87     0</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="mlr.html#cb11-1" tabindex="-1"></a><span class="fu">plot</span>(tukey.ames2, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-5-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<p>Starting with the variable for central air, we can see there is a statistical difference between the two categories. This is the exact same result as the overall F test for the variable since there are only two categories. For the heating quality variable, we can see some categories are different from each other, while others are not. Noticeably, the combination of poor with fair, good, and typical categories are <strong>not</strong> statistically different. Notice also the different widths of these confidence intervals do to the different combinations of sample sizes for the categories being tested.</p>
<div id="python-code-18" class="section level4 hasAnchor" number="3.1.3.1">
<h4><span class="header-section-number">3.1.3.1</span> Python Code<a href="mlr.html#python-code-18" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Exploring the data:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="mlr.html#cb12-1" tabindex="-1"></a>summary <span class="op">=</span> (</span>
<span id="cb12-2"><a href="mlr.html#cb12-2" tabindex="-1"></a>    train</span>
<span id="cb12-3"><a href="mlr.html#cb12-3" tabindex="-1"></a>    .groupby([<span class="st">&#39;Heating_QC&#39;</span>, <span class="st">&#39;Central_Air&#39;</span>])</span>
<span id="cb12-4"><a href="mlr.html#cb12-4" tabindex="-1"></a>    .agg(</span>
<span id="cb12-5"><a href="mlr.html#cb12-5" tabindex="-1"></a>        mean<span class="op">=</span>(<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;mean&#39;</span>),</span>
<span id="cb12-6"><a href="mlr.html#cb12-6" tabindex="-1"></a>        sd<span class="op">=</span>(<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;std&#39;</span>),</span>
<span id="cb12-7"><a href="mlr.html#cb12-7" tabindex="-1"></a>        <span class="bu">max</span><span class="op">=</span>(<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;max&#39;</span>),</span>
<span id="cb12-8"><a href="mlr.html#cb12-8" tabindex="-1"></a>        <span class="bu">min</span><span class="op">=</span>(<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;min&#39;</span>),</span>
<span id="cb12-9"><a href="mlr.html#cb12-9" tabindex="-1"></a>        n<span class="op">=</span>(<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;count&#39;</span>)</span>
<span id="cb12-10"><a href="mlr.html#cb12-10" tabindex="-1"></a>    )</span>
<span id="cb12-11"><a href="mlr.html#cb12-11" tabindex="-1"></a>    .reset_index()</span>
<span id="cb12-12"><a href="mlr.html#cb12-12" tabindex="-1"></a>)</span>
<span id="cb12-13"><a href="mlr.html#cb12-13" tabindex="-1"></a></span>
<span id="cb12-14"><a href="mlr.html#cb12-14" tabindex="-1"></a><span class="bu">print</span>(summary)</span></code></pre></div>
<pre><code>##   Heating_QC Central_Air           mean            sd     max     min     n
## 0  Excellent           N  112975.000000  36635.766336  184900   64000    13
## 1  Excellent           Y  218809.105960  87247.973506  755000   65000  1057
## 2       Fair           N   85862.068966  25543.917557  158000   39300    29
## 3       Fair           Y  150940.257143  39913.408794  235000   50000    35
## 4       Good           N  107186.111111  33810.126422  214500   59000    18
## 5       Good           Y  157455.063291  54425.088131  415000   50138   316
## 6       Poor           N   50050.000000  52255.191130   87000   13100     2
## 7       Poor           Y  107000.000000           NaN  107000  107000     1
## 8    Typical           N  102143.342105  43069.612095  265979   12789    76
## 9    Typical           Y  145896.166667  42814.810603  375000   70000   504</code></pre>
<p>Graph:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="mlr.html#cb14-1" tabindex="-1"></a>p <span class="op">=</span> (</span>
<span id="cb14-2"><a href="mlr.html#cb14-2" tabindex="-1"></a>  ggplot(summary, aes(x <span class="op">=</span> <span class="st">&quot;Heating_QC&quot;</span>, y <span class="op">=</span> <span class="st">&quot;mean&quot;</span>,  fill<span class="op">=</span><span class="st">&quot;Central_Air&quot;</span>)) <span class="op">+</span></span>
<span id="cb14-3"><a href="mlr.html#cb14-3" tabindex="-1"></a>  geom_bar(stat <span class="op">=</span> <span class="st">&quot;identity&quot;</span>, position <span class="op">=</span> position_dodge()) <span class="op">+</span></span>
<span id="cb14-4"><a href="mlr.html#cb14-4" tabindex="-1"></a>  labs(y <span class="op">=</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>, x <span class="op">=</span> <span class="st">&quot;Heating Quality Category&quot;</span>)  <span class="op">+</span></span>
<span id="cb14-5"><a href="mlr.html#cb14-5" tabindex="-1"></a>  theme_minimal()</span>
<span id="cb14-6"><a href="mlr.html#cb14-6" tabindex="-1"></a>    )</span>
<span id="cb14-7"><a href="mlr.html#cb14-7" tabindex="-1"></a></span>
<span id="cb14-8"><a href="mlr.html#cb14-8" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-7-1.png" width="614" /></p>
<p>Two-way ANOVA model</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="mlr.html#cb15-1" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb15-2"><a href="mlr.html#cb15-2" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb15-3"><a href="mlr.html#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a href="mlr.html#cb15-4" tabindex="-1"></a></span>
<span id="cb15-5"><a href="mlr.html#cb15-5" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC) + C(Central_Air)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb15-6"><a href="mlr.html#cb15-6" tabindex="-1"></a>model.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.240</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.239</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   129.5</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th> <td>2.07e-119</td>
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:08</td>     <th>  Log-Likelihood:    </th> <td> -25806.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.162e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2045</td>      <th>  BIC:               </th> <td>5.166e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
              <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                <td> 1.633e+05</td> <td> 6920.153</td> <td>   23.594</td> <td> 0.000</td> <td>  1.5e+05</td> <td> 1.77e+05</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Fair]</th>    <td>-7.185e+04</td> <td> 9544.896</td> <td>   -7.528</td> <td> 0.000</td> <td>-9.06e+04</td> <td>-5.31e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Good]</th>    <td>-6.048e+04</td> <td> 4432.503</td> <td>  -13.646</td> <td> 0.000</td> <td>-6.92e+04</td> <td>-5.18e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Poor]</th>    <td>-1.125e+05</td> <td>  4.1e+04</td> <td>   -2.743</td> <td> 0.006</td> <td>-1.93e+05</td> <td>-3.21e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Typical]</th> <td>-7.083e+04</td> <td> 3724.285</td> <td>  -19.019</td> <td> 0.000</td> <td>-7.81e+04</td> <td>-6.35e+04</td>
</tr>
<tr>
  <th>C(Central_Air)[T.Y]</th>      <td> 5.492e+04</td> <td> 6656.049</td> <td>    8.251</td> <td> 0.000</td> <td> 4.19e+04</td> <td>  6.8e+04</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>816.668</td> <th>  Durbin-Watson:     </th> <td>   2.013</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4816.450</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.775</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 9.615</td>  <th>  Cond. No.          </th> <td>    37.3</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="mlr.html#cb16-1" tabindex="-1"></a>sm.stats.anova_lm(model, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                       sum_sq      df           F        PR(&gt;F)
## C(Heating_QC)   2.216871e+12     4.0  111.257391  6.745663e-86
## C(Central_Air)  3.390960e+11     1.0   68.072407  2.788148e-16
## Residual        1.018697e+13  2045.0         NaN           NaN</code></pre>
<p>Post-hoc testing</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="mlr.html#cb18-1" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multicomp <span class="im">import</span> pairwise_tukeyhsd</span>
<span id="cb18-2"><a href="mlr.html#cb18-2" tabindex="-1"></a></span>
<span id="cb18-3"><a href="mlr.html#cb18-3" tabindex="-1"></a><span class="co"># Tukey&#39;s HSD for Heating_QC</span></span>
<span id="cb18-4"><a href="mlr.html#cb18-4" tabindex="-1"></a>tukey_heating <span class="op">=</span> pairwise_tukeyhsd(endog<span class="op">=</span>train[<span class="st">&#39;Sale_Price&#39;</span>],</span>
<span id="cb18-5"><a href="mlr.html#cb18-5" tabindex="-1"></a>                                  groups<span class="op">=</span>train[<span class="st">&#39;Heating_QC&#39;</span>],</span>
<span id="cb18-6"><a href="mlr.html#cb18-6" tabindex="-1"></a>                                  alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb18-7"><a href="mlr.html#cb18-7" tabindex="-1"></a><span class="bu">print</span>(tukey_heating)</span></code></pre></div>
<pre><code>##          Multiple Comparison of Means - Tukey HSD, FWER=0.05         
## =====================================================================
##   group1   group2   meandiff   p-adj     lower        upper    reject
## ---------------------------------------------------------------------
## Excellent    Fair  -96071.5679    0.0 -121271.5851 -70871.5507   True
## Excellent    Good  -62777.3129    0.0  -75051.5531 -50503.0728   True
## Excellent    Poor -148489.9377 0.0032 -261710.0367 -35269.8387   True
## Excellent Typical  -77360.2331    0.0  -87457.6961 -67262.7701   True
##      Fair    Good    33294.255 0.0061    6573.1446  60015.3654   True
##      Fair    Poor  -52418.3698 0.7296 -168099.6197  63262.8802  False
##      Fair Typical   18711.3348 0.2758   -7082.4538  44505.1234  False
##      Good    Poor  -85712.6248 0.2378 -199280.9641  27855.7146  False
##      Good Typical  -14582.9202 0.0258  -28034.1518  -1131.6885   True
##      Poor Typical   71129.7046 0.4259  -42224.0315 184483.4407  False
## ---------------------------------------------------------------------</code></pre>
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="mlr.html#cb20-1" tabindex="-1"></a><span class="co"># Tukey&#39;s HSD for Central_Air</span></span>
<span id="cb20-2"><a href="mlr.html#cb20-2" tabindex="-1"></a>tukey_air <span class="op">=</span> pairwise_tukeyhsd(endog<span class="op">=</span>train[<span class="st">&#39;Sale_Price&#39;</span>],</span>
<span id="cb20-3"><a href="mlr.html#cb20-3" tabindex="-1"></a>                               groups<span class="op">=</span>train[<span class="st">&#39;Central_Air&#39;</span>],</span>
<span id="cb20-4"><a href="mlr.html#cb20-4" tabindex="-1"></a>                               alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb20-5"><a href="mlr.html#cb20-5" tabindex="-1"></a><span class="bu">print</span>(tukey_air)</span></code></pre></div>
<pre><code>##     Multiple Comparison of Means - Tukey HSD, FWER=0.05     
## ============================================================
## group1 group2  meandiff  p-adj   lower       upper    reject
## ------------------------------------------------------------
##      N      Y 88519.3896   0.0 75070.1553 101968.6238   True
## ------------------------------------------------------------</code></pre>
</div>
</div>
</div>
<div id="two-way-anova-with-interactions" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Two-Way ANOVA with Interactions<a href="mlr.html#two-way-anova-with-interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What if the relationship between a predictor and target variable changed depending on the value of another predictor variable? For our example, we would say that the average difference in sales price between having central air and not having central changed depending on what level of heating quality the home had. In the bar chart in Figure <a href="mlr.html#fig:twomeans">3.1</a>, a potential interaction effect is displayed when the differences between the two bars within each heating category is different across heating category. If the difference, was the same, then there is no interaction present. In other words, no matter the heating quality rating of the home, the average difference in sales price between having central air and not having central air is the same.</p>
<p>This interaction model is represented as follows:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \varepsilon_{ijk}
\]</span></p>
<p>with the interaction effect, <span class="math inline">\((\alpha \beta)_{ij}\)</span>, as the multiplication of the two variables involved in the interaction. Interactions can occur between more than two variables as well. Interactions are good to evaluate as they can mask the effects of individual variables. For example, imagine a hypothetical example as shown in Figure <a href="mlr.html#fig:twomeansint">3.2</a> where the impact of having central air is opposite depending on which category of heating quality you have.</p>
<div id="r-code-19" class="section level4 hasAnchor" number="3.2.0.1">
<h4><span class="header-section-number">3.2.0.1</span> R code:<a href="mlr.html#r-code-19" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="mlr.html#cb22-1" tabindex="-1"></a>fake_HQ <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Poor&quot;</span>, <span class="st">&quot;Poor&quot;</span>, <span class="st">&quot;Excellent&quot;</span>, <span class="st">&quot;Excellent&quot;</span>)</span>
<span id="cb22-2"><a href="mlr.html#cb22-2" tabindex="-1"></a>fake_CA <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;N&quot;</span>, <span class="st">&quot;Y&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb22-3"><a href="mlr.html#cb22-3" tabindex="-1"></a>fake_mean <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">100</span>)</span>
<span id="cb22-4"><a href="mlr.html#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="mlr.html#cb22-5" tabindex="-1"></a>fake_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(fake_HQ, fake_CA, fake_mean))</span></code></pre></div>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="mlr.html#cb23-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> fake_df, <span class="fu">aes</span>(<span class="at">x =</span> fake_HQ, <span class="at">y =</span> <span class="fu">as.numeric</span>(fake_mean), <span class="at">fill =</span> fake_CA)) <span class="sc">+</span></span>
<span id="cb23-2"><a href="mlr.html#cb23-2" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">position =</span> <span class="fu">position_dodge</span>()) <span class="sc">+</span></span>
<span id="cb23-3"><a href="mlr.html#cb23-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Fake Sales Price (Thousands $)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Fake Heating Quality Category&quot;</span>) <span class="sc">+</span></span>
<span id="cb23-4"><a href="mlr.html#cb23-4" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Paired&quot;</span>) <span class="sc">+</span></span>
<span id="cb23-5"><a href="mlr.html#cb23-5" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twomeansint"></span>
<img src="bookdownproj_files/figure-html/twomeansint-1.png" alt="Distribution of Variables Heating_QC and Central_Air" width="672" />
<p class="caption">
Figure 3.2: Distribution of Variables Heating_QC and Central_Air
</p>
</div>
<p>If you were to only look at the average sales price across heating quality, you would notice no difference between the two groups (average for both heating categories is 125). However, when the interaction is accounted for, you can clearly see in the bar heights that sales price is different depending on the value of central air.</p>
<p>Let’s evaluate the interaction term in our actual data. To do so, we just incorporate the multiplication of the two variables in the model statement by using the formula <code>Sale_Price ~ Heating_QC + Central_Air + Heating_QC:Central_Air</code>. You could also use the shorthand version of this by using the formula <code>Sale_Price ~ Heating_QC*Central_Air</code>. The <code>*</code> will include both main effects (the individual variables) and the interaction between them.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="mlr.html#cb24-1" tabindex="-1"></a>ames_aov_int <span class="ot">&lt;-</span> <span class="fu">aov</span>(Sale_Price <span class="sc">~</span> Heating_QC<span class="sc">*</span>Central_Air, <span class="at">data =</span> train)</span>
<span id="cb24-2"><a href="mlr.html#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="mlr.html#cb24-3" tabindex="-1"></a><span class="fu">summary</span>(ames_aov_int)</span></code></pre></div>
<pre><code>##                          Df    Sum Sq   Mean Sq F value   Pr(&gt;F)    
## Heating_QC                4 2.891e+12 7.228e+11 147.897  &lt; 2e-16 ***
## Central_Air               1 2.903e+11 2.903e+11  59.403 1.99e-14 ***
## Heating_QC:Central_Air    4 3.972e+10 9.930e+09   2.032   0.0875 .  
## Residuals              2041 9.975e+12 4.887e+09                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As seen by the output above, the interaction effect between heating quality and central air is <strong>not</strong> significant at the 0.05 level. Again, this implies that the average difference in sale price of the home between having central air and not does not differ depending on which category of heating quality the home belongs to. If our interaction was significant (say a 0.02 p-value instead) then we would keep it in our model, but here we would remove the interaction term from our model and rerun the analysis.</p>
</div>
<div id="post-hoc-testing-1" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Post-Hoc Testing<a href="mlr.html#post-hoc-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Post-hoc tests are also available for interaction models as well to determine where the statistical differences exist in all the combinations of possible categories. We evaluate these post-hoc tests using the same <code>TukeyHSD</code> function and its corresponding <code>plot</code> element.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="mlr.html#cb26-1" tabindex="-1"></a>tukey.ames_int <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(ames_aov_int)</span>
<span id="cb26-2"><a href="mlr.html#cb26-2" tabindex="-1"></a><span class="fu">print</span>(tukey.ames_int)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Sale_Price ~ Heating_QC * Central_Air, data = train)
## 
## $Heating_QC
##                        diff        lwr       upr     p adj
## Fair-Poor          49176.42 -63536.973 161889.81 0.7565086
## Typical-Poor       67781.01 -42689.104 178251.13 0.4496163
## Good-Poor          87753.89 -22928.822 198436.60 0.1936558
## Excellent-Poor    146288.89  35929.963 256647.82 0.0027979
## Typical-Fair       18604.59  -6301.351  43510.54 0.2474998
## Good-Fair          38577.47  12744.901  64410.03 0.0004543
## Excellent-Fair     97112.47  72704.440 121520.50 0.0000000
## Good-Typical       19972.87   7063.227  32882.52 0.0002425
## Excellent-Typical  78507.88  68756.495  88259.26 0.0000000
## Excellent-Good     58535.00  46614.230  70455.77 0.0000000
## 
## $Central_Air
##         diff      lwr      upr p adj
## Y-N 43256.57 31520.09 54993.05     0
## 
## $`Heating_QC:Central_Air`
##                               diff         lwr       upr     p adj
## Fair:N-Poor:N            34698.276 -127178.615 196575.17 0.9996249
## Typical:N-Poor:N         53419.220 -105046.645 211885.08 0.9876643
## Good:N-Poor:N            60760.870 -102472.555 223994.29 0.9755371
## Excellent:N-Poor:N       65011.727 -105195.635 235219.09 0.9709555
## Poor:Y-Poor:N            56950.000 -214233.733 328133.73 0.9996829
## Fair:Y-Poor:N            95114.833  -65743.496 255973.16 0.6876708
## Typical:Y-Poor:N         91952.772  -64912.040 248817.58 0.6985070
## Good:Y-Poor:N           110062.553  -46997.028 267122.13 0.4435353
## Excellent:Y-Poor:N      166351.347    9630.224 323072.47 0.0271785
## Typical:N-Fair:N         18720.944  -29117.117  66559.00 0.9659507
## Good:N-Fair:N            26062.594  -35761.358  87886.55 0.9454988
## Excellent:N-Fair:N       30313.451  -48093.155 108720.06 0.9685428
## Poor:Y-Fair:N            22251.724 -202954.108 247457.56 0.9999995
## Fair:Y-Fair:N            60416.557    5167.556 115665.56 0.0193847
## Typical:Y-Fair:N         57254.496   15021.578  99487.41 0.0007697
## Good:Y-Fair:N            75364.278   32413.584 118314.97 0.0000014
## Excellent:Y-Fair:N      131653.071   89957.021 173349.12 0.0000000
## Good:N-Typical:N          7341.650  -44902.998  59586.30 0.9999894
## Excellent:N-Typical:N    11592.508  -59505.300  82690.32 0.9999620
## Poor:Y-Typical:N          3530.780 -219235.844 226297.41 1.0000000
## Fair:Y-Typical:N         41695.614   -2573.500  85964.73 0.0848888
## Typical:Y-Typical:N      38533.553   12248.163  64818.94 0.0001576
## Good:Y-Typical:N         56643.334   29219.541  84067.13 0.0000000
## Excellent:Y-Typical:N   112932.128   87518.295 138345.96 0.0000000
## Excellent:N-Good:N        4250.858  -76919.452  85421.17 1.0000000
## Poor:Y-Good:N            -3810.870 -229993.738 222372.00 1.0000000
## Fair:Y-Good:N            34353.964  -24751.665  93459.59 0.7089196
## Typical:Y-Good:N         31191.903  -15974.214  78358.02 0.5315494
## Good:Y-Good:N            49301.684    1491.797  97111.57 0.0369201
## Excellent:Y-Good:N      105590.478   58904.465 152276.49 0.0000000
## Poor:Y-Excellent:N       -8061.727 -239327.991 223204.54 1.0000000
## Fair:Y-Excellent:N       30103.106  -46178.414 106384.63 0.9640522
## Typical:Y-Excellent:N    26941.045  -40512.921  94395.01 0.9611660
## Good:Y-Excellent:N       45050.826  -22854.846 112956.50 0.5267698
## Excellent:Y-Excellent:N 101339.620   34220.481 168458.76 0.0000809
## Fair:Y-Poor:Y            38164.833 -186309.978 262639.65 0.9999458
## Typical:Y-Poor:Y         35002.772 -186627.795 256633.34 0.9999711
## Good:Y-Poor:Y            53112.553 -168655.909 274881.02 0.9990789
## Excellent:Y-Poor:Y      109401.347 -112127.544 330930.24 0.8652766
## Typical:Y-Fair:Y         -3162.061  -41305.131  34981.01 0.9999999
## Good:Y-Fair:Y            14947.720  -23988.593  53884.03 0.9699661
## Excellent:Y-Fair:Y       71236.514   33688.745 108784.28 0.0000001
## Good:Y-Typical:Y         18109.781    2387.068  33832.49 0.0101482
## Excellent:Y-Typical:Y    74398.575   62524.140  86273.01 0.0000000
## Excellent:Y-Good:Y       56288.794   42071.027  70506.56 0.0000000</code></pre>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="mlr.html#cb28-1" tabindex="-1"></a><span class="fu">plot</span>(tukey.ames_int, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-12-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-12-2.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-12-3.png" width="672" /></p>
<p>In the giant table above as well as the confidence interval plots, you are able to inspect which combination of categories are statistically different.</p>
<p>With interactions present in ANOVA models, post-hoc tests might get overwhelming in trying to find where differences exist. To help guide the exploration of post-hoc tests with interactions, we can do <strong>slicing</strong>. Slicing is when you perform One-Way ANOVA on subsets of data within categories of other variables. Even though the interaction in our model was not significant, let’s imagine that it was for the sake of example. For example, to help discover differences in the interaction between central air and heating quality, we could subset the data into two groups - homes with central air and homes without. Within these two groups we perform One-Way ANOVA across heating quality to find where differences might exist within subgroups.</p>
<p>This can easily be done with the <code>group_by</code> function to subset the data. The <code>nest</code> and <code>mutate</code> functions are also used to applied the <code>aov</code> function to each subgroup. Here we run a One-Way ANOVA for heating quality within each subset of central air being present or not.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="mlr.html#cb29-1" tabindex="-1"></a>CA_aov <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span> </span>
<span id="cb29-2"><a href="mlr.html#cb29-2" tabindex="-1"></a>  <span class="fu">group_by</span>(Central_Air) <span class="sc">%&gt;%</span></span>
<span id="cb29-3"><a href="mlr.html#cb29-3" tabindex="-1"></a>  <span class="fu">nest</span>() <span class="sc">%&gt;%</span></span>
<span id="cb29-4"><a href="mlr.html#cb29-4" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">aov =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">summary</span>(<span class="fu">aov</span>(Sale_Price <span class="sc">~</span> Heating_QC, <span class="at">data =</span> .x))))</span>
<span id="cb29-5"><a href="mlr.html#cb29-5" tabindex="-1"></a></span>
<span id="cb29-6"><a href="mlr.html#cb29-6" tabindex="-1"></a>CA_aov</span></code></pre></div>
<pre><code>## # A tibble: 2 × 3
## # Groups:   Central_Air [2]
##   Central_Air data                  aov       
##   &lt;fct&gt;       &lt;list&gt;                &lt;list&gt;    
## 1 Y           &lt;tibble [1,904 × 81]&gt; &lt;summry.v&gt;
## 2 N           &lt;tibble [147 × 81]&gt;   &lt;summry.v&gt;</code></pre>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="mlr.html#cb31-1" tabindex="-1"></a><span class="fu">print</span>(CA_aov<span class="sc">$</span>aov)</span></code></pre></div>
<pre><code>## [[1]]
##               Df    Sum Sq   Mean Sq F value Pr(&gt;F)    
## Heating_QC     4 2.242e+12 5.606e+11   108.5 &lt;2e-16 ***
## Residuals   1899 9.809e+12 5.165e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## [[2]]
##              Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   
## Heating_QC    4 1.774e+10 4.435e+09   3.793 0.00582 **
## Residuals   142 1.660e+11 1.169e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see that both of these results are significant at the 0.05 level. This implies that there are statistical differences in average sale price across heating quality within homes that have central air as well as those that don’t have central air.</p>
</div>
<div id="assumptions" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Assumptions<a href="mlr.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumptions for the <span class="math inline">\(n\)</span>-Way ANOVA are the same as with the One-Way ANOVA - independence of observations, normality for each category of variable, and equal variances. With the inclusion of two or more variables (<span class="math inline">\(n\)</span>-Way ANOVA with <span class="math inline">\(n &gt; 1\)</span>), these assumptions can be harder to evaluate and test. These assumptions are then applied to the residuals of the model.</p>
<p>For equal variances, we can still apply the Levene Test for equal variances using the same <code>leveneTest</code> function as in Chapter <a href="slr.html#slr">2</a>.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb33-1"><a href="mlr.html#cb33-1" tabindex="-1"></a><span class="fu">leveneTest</span>(Sale_Price <span class="sc">~</span> Heating_QC<span class="sc">*</span>Central_Air, <span class="at">data =</span> train)</span></code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##         Df F value    Pr(&gt;F)    
## group    9   24.17 &lt; 2.2e-16 ***
##       2041                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From this test, we can see that we <strong>do not</strong> meet our assumption of equal variance.</p>
<p>Let’s explore the normality assumption. Again, we will assume this normality on the random error component, <span class="math inline">\(\varepsilon_{ijk}\)</span>, of the ANOVA model. More details are found for diagnostic testing using the error component in Diagnostic chapter. We can check normality using the same approaches of the QQ-plot or statistical testing as in the section on EDA. Here we will use the <code>plot</code> function on the <code>aov</code> object. Specifically, we want the second plot which is why we have a <code>2</code> in the <code>plot</code> function option. We then use the <code>shapiro.test</code> function on the error component. The estimate of the error component is calculated using the <code>residuals</code> function.</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="mlr.html#cb35-1" tabindex="-1"></a><span class="fu">plot</span>(ames_aov_int, <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="mlr.html#cb36-1" tabindex="-1"></a>ames_res <span class="ot">&lt;-</span> <span class="fu">residuals</span>(ames_aov_int)</span>
<span id="cb36-2"><a href="mlr.html#cb36-2" tabindex="-1"></a></span>
<span id="cb36-3"><a href="mlr.html#cb36-3" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="at">x =</span> ames_res)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  ames_res
## W = 0.8838, p-value &lt; 2.2e-16</code></pre>
<p>Neither of the normality or equal variance assumptions appear to be met here. This would be a good scenario to have a non-parametric approach. Unfortunately, the Kruskal-Wallis approach is not applicable to <span class="math inline">\(n\)</span>-Way ANOVA where <span class="math inline">\(n &gt; 1\)</span>. These approaches would need more non-parametric versions of multiple regression models to account for them.</p>
<div id="python-code-19" class="section level4 hasAnchor" number="3.2.2.1">
<h4><span class="header-section-number">3.2.2.1</span> Python Code<a href="mlr.html#python-code-19" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Two-way ANOVA with interactions</p>
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="mlr.html#cb38-1" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb38-2"><a href="mlr.html#cb38-2" tabindex="-1"></a><span class="im">import</span> scipy <span class="im">as</span> sp</span>
<span id="cb38-3"><a href="mlr.html#cb38-3" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb38-4"><a href="mlr.html#cb38-4" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb38-5"><a href="mlr.html#cb38-5" tabindex="-1"></a></span>
<span id="cb38-6"><a href="mlr.html#cb38-6" tabindex="-1"></a></span>
<span id="cb38-7"><a href="mlr.html#cb38-7" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC)*C(Central_Air)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb38-8"><a href="mlr.html#cb38-8" tabindex="-1"></a>model.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.244</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.240</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   73.08</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th> <td>3.30e-117</td>
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:10</td>     <th>  Log-Likelihood:    </th> <td> -25801.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.162e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2041</td>      <th>  BIC:               </th> <td>5.168e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
                        <td></td>                          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                                    <td>  1.13e+05</td> <td> 1.96e+04</td> <td>    5.778</td> <td> 0.000</td> <td> 7.46e+04</td> <td> 1.51e+05</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Fair]</th>                        <td>-2.711e+04</td> <td> 2.35e+04</td> <td>   -1.152</td> <td> 0.249</td> <td>-7.33e+04</td> <td>  1.9e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Good]</th>                        <td>-5788.8889</td> <td> 2.57e+04</td> <td>   -0.226</td> <td> 0.822</td> <td>-5.61e+04</td> <td> 4.45e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Poor]</th>                        <td>-6.293e+04</td> <td> 5.35e+04</td> <td>   -1.175</td> <td> 0.240</td> <td>-1.68e+05</td> <td> 4.21e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Typical]</th>                     <td>-1.083e+04</td> <td> 2.12e+04</td> <td>   -0.512</td> <td> 0.609</td> <td>-5.23e+04</td> <td> 3.07e+04</td>
</tr>
<tr>
  <th>C(Central_Air)[T.Y]</th>                          <td> 1.058e+05</td> <td> 1.97e+04</td> <td>    5.380</td> <td> 0.000</td> <td> 6.73e+04</td> <td> 1.44e+05</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Fair]:C(Central_Air)[T.Y]</th>    <td>-4.076e+04</td> <td> 2.65e+04</td> <td>   -1.540</td> <td> 0.124</td> <td>-9.27e+04</td> <td> 1.11e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Good]:C(Central_Air)[T.Y]</th>    <td>-5.557e+04</td> <td> 2.61e+04</td> <td>   -2.133</td> <td> 0.033</td> <td>-1.07e+05</td> <td>-4469.381</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Poor]:C(Central_Air)[T.Y]</th>    <td>-4.888e+04</td> <td> 8.86e+04</td> <td>   -0.552</td> <td> 0.581</td> <td>-2.23e+05</td> <td> 1.25e+05</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Typical]:C(Central_Air)[T.Y]</th> <td>-6.208e+04</td> <td> 2.15e+04</td> <td>   -2.888</td> <td> 0.004</td> <td>-1.04e+05</td> <td>-1.99e+04</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>815.967</td> <th>  Durbin-Watson:     </th> <td>   2.014</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4837.979</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.771</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 9.638</td>  <th>  Cond. No.          </th> <td>    91.1</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="mlr.html#cb39-1" tabindex="-1"></a>sm.stats.anova_lm(model, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                                     sum_sq      df           F        PR(&gt;F)
## C(Heating_QC)                 2.216871e+12     4.0  111.516329  4.580351e-86
## C(Central_Air)                3.390960e+11     1.0   68.230837  2.582411e-16
## C(Heating_QC):C(Central_Air)  4.353317e+10     4.0    2.189870  6.779864e-02
## Residual                      1.014343e+13  2041.0         NaN           NaN</code></pre>
<div class="sourceCode" id="cb41"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="mlr.html#cb41-1" tabindex="-1"></a></span>
<span id="cb41-2"><a href="mlr.html#cb41-2" tabindex="-1"></a>train[<span class="st">&#39;resid_anova_2way&#39;</span>] <span class="op">=</span> model.resid</span></code></pre></div>
<p>Post-hoc testing</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="mlr.html#cb42-1" tabindex="-1"></a>model_slice1 <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC)&quot;</span>, data <span class="op">=</span> train[train[<span class="st">&quot;Central_Air&quot;</span>] <span class="op">==</span> <span class="st">&#39;Y&#39;</span>]).fit()</span>
<span id="cb42-2"><a href="mlr.html#cb42-2" tabindex="-1"></a>model_slice1.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.184</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.182</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   107.7</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th> <td>7.88e-83</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:10</td>     <th>  Log-Likelihood:    </th> <td> -24113.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  1913</td>      <th>  AIC:               </th> <td>4.824e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  1908</td>      <th>  BIC:               </th> <td>4.826e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
              <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                <td> 2.188e+05</td> <td> 2220.937</td> <td>   98.521</td> <td> 0.000</td> <td> 2.14e+05</td> <td> 2.23e+05</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Fair]</th>    <td>-6.787e+04</td> <td> 1.24e+04</td> <td>   -5.471</td> <td> 0.000</td> <td>-9.22e+04</td> <td>-4.35e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Good]</th>    <td>-6.135e+04</td> <td> 4629.434</td> <td>  -13.253</td> <td> 0.000</td> <td>-7.04e+04</td> <td>-5.23e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Poor]</th>    <td>-1.118e+05</td> <td> 7.22e+04</td> <td>   -1.548</td> <td> 0.122</td> <td>-2.53e+05</td> <td> 2.99e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Typical]</th> <td>-7.291e+04</td> <td> 3908.610</td> <td>  -18.654</td> <td> 0.000</td> <td>-8.06e+04</td> <td>-6.52e+04</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>745.622</td> <th>  Durbin-Watson:     </th> <td>   2.043</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4152.079</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.749</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 9.313</td>  <th>  Cond. No.          </th> <td>    46.1</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="mlr.html#cb43-1" tabindex="-1"></a>sm.stats.anova_lm(model_slice1, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                      sum_sq      df           F        PR(&gt;F)
## C(Heating_QC)  2.246168e+12     4.0  107.704754  7.875980e-83
## Residual       9.947769e+12  1908.0         NaN           NaN</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="mlr.html#cb45-1" tabindex="-1"></a>model_slice2 <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC)&quot;</span>, data <span class="op">=</span> train[train[<span class="st">&quot;Central_Air&quot;</span>] <span class="op">==</span> <span class="st">&#39;N&#39;</span>]).fit()</span>
<span id="cb45-2"><a href="mlr.html#cb45-2" tabindex="-1"></a>model_slice2.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.068</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.040</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.419</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th>  <td>0.0516</td> 
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:10</td>     <th>  Log-Likelihood:    </th> <td> -1649.8</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   138</td>      <th>  AIC:               </th> <td>   3310.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   133</td>      <th>  BIC:               </th> <td>   3324.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
              <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                <td>  1.13e+05</td> <td> 1.06e+04</td> <td>   10.620</td> <td> 0.000</td> <td> 9.19e+04</td> <td> 1.34e+05</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Fair]</th>    <td>-2.711e+04</td> <td> 1.28e+04</td> <td>   -2.118</td> <td> 0.036</td> <td>-5.24e+04</td> <td>-1790.733</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Good]</th>    <td>-5788.8889</td> <td>  1.4e+04</td> <td>   -0.415</td> <td> 0.679</td> <td>-3.34e+04</td> <td> 2.18e+04</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Poor]</th>    <td>-6.292e+04</td> <td> 2.91e+04</td> <td>   -2.160</td> <td> 0.033</td> <td>-1.21e+05</td> <td>-5300.604</td>
</tr>
<tr>
  <th>C(Heating_QC)[T.Typical]</th> <td>-1.083e+04</td> <td> 1.15e+04</td> <td>   -0.941</td> <td> 0.348</td> <td>-3.36e+04</td> <td> 1.19e+04</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>40.204</td> <th>  Durbin-Watson:     </th> <td>   2.083</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  94.171</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.190</td> <th>  Prob(JB):          </th> <td>3.56e-21</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 6.274</td> <th>  Cond. No.          </th> <td>    11.2</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="mlr.html#cb46-1" tabindex="-1"></a>sm.stats.anova_lm(model_slice2, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                      sum_sq     df         F    PR(&gt;F)
## C(Heating_QC)  1.423639e+10    4.0  2.419249  0.051615
## Residual       1.956640e+11  133.0       NaN       NaN</code></pre>
<p>Assumptions</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="mlr.html#cb48-1" tabindex="-1"></a>sm.qqplot(train[<span class="st">&#39;resid_anova_2way&#39;</span>])</span>
<span id="cb48-2"><a href="mlr.html#cb48-2" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="mlr.html#cb49-1" tabindex="-1"></a>sp.stats.shapiro(model.resid)</span></code></pre></div>
<pre><code>## ShapiroResult(statistic=0.8877905358408816, pvalue=3.2424049575361566e-36)</code></pre>
</div>
</div>
</div>
<div id="randomized-block-design" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Randomized Block Design<a href="mlr.html#randomized-block-design" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are two typical groups of data analysis studies that are conducted. The first are observational/retrospective studies which are the typical data problems people try to solve. The primary characteristic of these analysis are looking at what already happened (retrospective) and potentially inferring those results to further data. These studies have little control over other factors contributing to the target of interest because data is collected after the fact.</p>
<p>The other type of data analysis study are controlled experiments. In these situations, you often want to look at the outcome measure prospectively. The focus of the controlled experiment is to control for other factors that might contribute to the target variable. By manipulating these factors of interest, one can more reasonably claim causation. We can more reasonably claim causation when random assignment is used to eliminate potential <strong>nuisance factors</strong>. Nuisance factors are variables that can potentially impact the target variable, but are not of interest in the analysis directly.</p>
<div id="garlic-bulb-weight-example" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Garlic Bulb Weight Example<a href="mlr.html#garlic-bulb-weight-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this analysis we will use a new dataset. This dataset contains the average garlic bulb weight from different plots of land. We want to compare the effects of fertilizer on average bulb weight. However, different plots of land could have different levels of sun exposure, pH for the soil, and rain amounts. Since we cannot alter the pH of the soil easily, or control the sun and rain, we can use blocking to account for these nuisance factors. Each fertilizer was randomly applied in quadrants of 8 plots of land. These 8 plots have different values for sun exposure, pH, and rain amount. Therefore, if we only put one fertilizer on each plot, we would not know if the fertilizer was the reason the garlic crop grew or if it was one of the nuisance factors. Since we <strong>blocked</strong> these 8 plots and applied all four fertilizers in each we have essentially accounted for (or removed the effect of) the nuisance factors.</p>
<p>Let’s briefly explore this new dataset by looking at all 32 values using the <code>print</code> function.</p>
<div id="r-code-20" class="section level4 hasAnchor" number="3.3.1.1">
<h4><span class="header-section-number">3.3.1.1</span> R code:<a href="mlr.html#r-code-20" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="mlr.html#cb51-1" tabindex="-1"></a>block <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">&quot;garlic_block.csv&quot;</span>)</span></code></pre></div>
<pre><code>## Rows: 32 Columns: 6
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (6): Sector, Position, Fertilizer, BulbWt, Cloves, BedId
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="mlr.html#cb53-1" tabindex="-1"></a><span class="fu">head</span>(block, <span class="at">n =</span> <span class="dv">32</span>)</span></code></pre></div>
<pre><code>## # A tibble: 32 × 6
##    Sector Position Fertilizer BulbWt Cloves BedId
##     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1      1        1          3  0.259   11.6 22961
##  2      1        2          4  0.207   12.6 23884
##  3      1        3          1  0.275   12.1 19642
##  4      1        4          2  0.245   12.1 20384
##  5      2        1          3  0.215   11.6 20303
##  6      2        2          4  0.170   12.7 21004
##  7      2        3          1  0.225   12.0 16117
##  8      2        4          2  0.168   11.9 19686
##  9      3        1          4  0.217   12.4 26527
## 10      3        2          3  0.226   11.7 23574
## # ℹ 22 more rows</code></pre>
<p>How do we account for this blocking in an ANOVA model context? This blocking ANOVA model is the exact same as the Two-Way ANOVA model. The variable that identifies which sector (block) an observation is in serves as another variable in the model. Think about this variable as the variable that accounts for all the nuisance factors in your ANOVA. That means we have two variables in this ANOVA model - fertilizer and sector (the block that accounts for sun exposure, pH level of soil, rain amount, etc.).</p>
<p>For this we can use the same <code>aov</code> function we described above.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="mlr.html#cb55-1" tabindex="-1"></a>block_aov <span class="ot">&lt;-</span> <span class="fu">aov</span>(BulbWt <span class="sc">~</span> <span class="fu">factor</span>(Fertilizer) <span class="sc">+</span> <span class="fu">factor</span>(Sector), <span class="at">data =</span> block)</span>
<span id="cb55-2"><a href="mlr.html#cb55-2" tabindex="-1"></a></span>
<span id="cb55-3"><a href="mlr.html#cb55-3" tabindex="-1"></a><span class="fu">summary</span>(block_aov)</span></code></pre></div>
<pre><code>##                    Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    
## factor(Fertilizer)  3 0.005086 0.0016954   4.307 0.016222 *  
## factor(Sector)      7 0.017986 0.0025695   6.527 0.000364 ***
## Residuals          21 0.008267 0.0003937                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Using the <code>summary</code> function we can see that both the sector (block) and fertilizer variables are significant in the model at the 0.05 level. What are the interpretations of this?
First, let’s address the blocking variable. Whether it is significant or not, it should <strong>always</strong> be included in the model. This is due to the fact that the data is structured in that way. It is a construct of the data that should be accounted for regardless of the significance. However, since the blocking variable (sector) was significant, that implies that different plots of land have different impacts of the average bulb weight of garlic. Again, this is most likely due to the differences between the plots of land - namely sun exposure, pH of soil, rain fall, etc.</p>
<p>Second, the variable of interest is the fertilizer variable. It is significant, implying that there is a difference in the average bulb weight of garlic for different fertilizers. To examine which fertilizer pairs are statistically difference we can use post-hos testing as described in the previous parts of this chapter using the <code>TukeyHSD</code> function.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="mlr.html#cb57-1" tabindex="-1"></a>tukey.block <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(block_aov)</span>
<span id="cb57-2"><a href="mlr.html#cb57-2" tabindex="-1"></a><span class="fu">print</span>(tukey.block)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = BulbWt ~ factor(Fertilizer) + factor(Sector), data = block)
## 
## $`factor(Fertilizer)`
##            diff         lwr          upr     p adj
## 2-1 -0.02509875 -0.05275125  0.002553751 0.0840024
## 3-1 -0.01294875 -0.04060125  0.014703751 0.5698678
## 4-1 -0.03336125 -0.06101375 -0.005708749 0.0144260
## 3-2  0.01215000 -0.01550250  0.039802501 0.6186232
## 4-2 -0.00826250 -0.03591500  0.019390001 0.8382800
## 4-3 -0.02041250 -0.04806500  0.007240001 0.1995492
## 
## $`factor(Sector)`
##           diff          lwr           upr     p adj
## 2-1 -0.0520675 -0.099126544 -5.008456e-03 0.0234315
## 3-1 -0.0145075 -0.061566544  3.255154e-02 0.9634255
## 4-1 -0.0450550 -0.092114044  2.004044e-03 0.0669646
## 5-1 -0.0616250 -0.108684044 -1.456596e-02 0.0051483
## 6-1 -0.0196650 -0.066724044  2.739404e-02 0.8466335
## 7-1  0.0084950 -0.038564044  5.555404e-02 0.9984089
## 8-1 -0.0393325 -0.086391544  7.726544e-03 0.1469768
## 3-2  0.0375600 -0.009499044  8.461904e-02 0.1841786
## 4-2  0.0070125 -0.040046544  5.407154e-02 0.9995370
## 5-2 -0.0095575 -0.056616544  3.750154e-02 0.9966777
## 6-2  0.0324025 -0.014656544  7.946154e-02 0.3337758
## 7-2  0.0605625  0.013503456  1.076215e-01 0.0061094
## 8-2  0.0127350 -0.034324044  5.979404e-02 0.9819446
## 4-3 -0.0305475 -0.077606544  1.651154e-02 0.4025951
## 5-3 -0.0471175 -0.094176544 -5.845586e-05 0.0495704
## 6-3 -0.0051575 -0.052216544  4.190154e-02 0.9999400
## 7-3  0.0230025 -0.024056544  7.006154e-02 0.7227812
## 8-3 -0.0248250 -0.071884044  2.223404e-02 0.6454690
## 5-4 -0.0165700 -0.063629044  3.048904e-02 0.9286987
## 6-4  0.0253900 -0.021669044  7.244904e-02 0.6208608
## 7-4  0.0535500  0.006490956  1.006090e-01 0.0186102
## 8-4  0.0057225 -0.041336544  5.278154e-02 0.9998793
## 6-5  0.0419600 -0.005099044  8.901904e-02 0.1034664
## 7-5  0.0701200  0.023060956  1.171790e-01 0.0012997
## 8-5  0.0222925 -0.024766544  6.935154e-02 0.7514897
## 7-6  0.0281600 -0.018899044  7.521904e-02 0.5004099
## 8-6 -0.0196675 -0.066726544  2.739154e-02 0.8465530
## 8-7 -0.0478275 -0.094886544 -7.684559e-04 0.0446174</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="mlr.html#cb59-1" tabindex="-1"></a><span class="fu">plot</span>(tukey.block, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-21-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
</div>
</div>
<div id="assumptions-1" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Assumptions<a href="mlr.html#assumptions-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Outside of the typical assumptions for ANOVA that still hold here, there are two additional assumptions to be met:</p>
<ul>
<li>Treatments are randomly assigned within each block</li>
<li>The effects of the treatment variable are constant across the levels of the blocking variable</li>
</ul>
<p>The first, new assumption of randomness deals with the reliability of the analysis. Randomness is key to removing the impact of the nuisance factors. The second, new assumption implies there is <strong>no interaction</strong> between the treatment variable and the blocking variable. For example, we are implying that the fertilizers’ impacts ob garlic bulb weight are not changed depending on what block you are on. In other words, fertilizers have the same impact regardless of sun exposure, pH levels, rain fall, etc. We are <strong>not</strong> saying these nuisance factors do not impact the target variable or bulb weight of garlic, just that they do not change the effect of the fertilizer on bulb weight.</p>
<div id="python-code-20" class="section level4 hasAnchor" number="3.3.2.1">
<h4><span class="header-section-number">3.3.2.1</span> Python Code<a href="mlr.html#python-code-20" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb60"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb60-1"><a href="mlr.html#cb60-1" tabindex="-1"></a>block<span class="op">=</span>pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/garlic_block.csv&#39;</span>)</span>
<span id="cb60-2"><a href="mlr.html#cb60-2" tabindex="-1"></a>block.head(n <span class="op">=</span> <span class="dv">32</span>)</span></code></pre></div>
<pre><code>##     Sector  Position  Fertilizer   BulbWt   Cloves  BedId
## 0        1         1           3  0.25881  11.6322  22961
## 1        1         2           4  0.20746  12.5837  23884
## 2        1         3           1  0.27453  12.0597  19642
## 3        1         4           2  0.24467  12.1001  20384
## 4        2         1           3  0.21454  11.5863  20303
## 5        2         2           4  0.16953  12.7132  21004
## 6        2         3           1  0.22504  12.0470  16117
## 7        2         4           2  0.16809  11.9071  19686
## 8        3         1           4  0.21720  12.3655  26527
## 9        3         2           3  0.22551  11.6864  23574
## 10       3         3           2  0.23536  12.0258  17499
## 11       3         4           1  0.24937  11.6668  16636
## 12       4         1           4  0.20811  12.5996  24834
## 13       4         2           1  0.21138  12.1393  19946
## 14       4         3           2  0.19320  12.0792  21504
## 15       4         4           3  0.19256  11.6464  23181
## 16       5         1           4  0.19851  12.5355  24533
## 17       5         2           1  0.18603  12.3307  15009
## 18       5         3           3  0.19698  11.5608  23845
## 19       5         4           2  0.15745  11.8735  18948
## 20       6         1           2  0.20058  11.9077  20019
## 21       6         2           3  0.25346  11.7294  22228
## 22       6         3           4  0.19838  12.7670  24424
## 23       6         4           1  0.25439  12.0139  13755
## 24       7         1           3  0.26578  11.7448  21087
## 25       7         2           4  0.21678  12.8531  25751
## 26       7         3           2  0.26183  12.3990  20296
## 27       7         4           1  0.27506  11.9383  20038
## 28       8         1           1  0.21420  12.1034  17843
## 29       8         2           3  0.17877  11.5682  21394
## 30       8         3           4  0.20714  12.5213  27191
## 31       8         4           2  0.22803  12.2317  20202</code></pre>
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="mlr.html#cb62-1" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb62-2"><a href="mlr.html#cb62-2" tabindex="-1"></a></span>
<span id="cb62-3"><a href="mlr.html#cb62-3" tabindex="-1"></a>model_b <span class="op">=</span> smf.ols(<span class="st">&quot;BulbWt ~ C(Fertilizer) + C(Sector)&quot;</span>, data <span class="op">=</span> block).fit()</span>
<span id="cb62-4"><a href="mlr.html#cb62-4" tabindex="-1"></a>model_b.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>         <td>BulbWt</td>      <th>  R-squared:         </th> <td>   0.736</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.611</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   5.861</td>
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th> <td>0.000328</td>
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:12</td>     <th>  Log-Likelihood:    </th> <td>  86.773</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>    32</td>      <th>  AIC:               </th> <td>  -151.5</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    21</td>      <th>  BIC:               </th> <td>  -135.4</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>          <td>    0.2642</td> <td>    0.012</td> <td>   22.713</td> <td> 0.000</td> <td>    0.240</td> <td>    0.288</td>
</tr>
<tr>
  <th>C(Fertilizer)[T.2]</th> <td>   -0.0251</td> <td>    0.010</td> <td>   -2.530</td> <td> 0.019</td> <td>   -0.046</td> <td>   -0.004</td>
</tr>
<tr>
  <th>C(Fertilizer)[T.3]</th> <td>   -0.0129</td> <td>    0.010</td> <td>   -1.305</td> <td> 0.206</td> <td>   -0.034</td> <td>    0.008</td>
</tr>
<tr>
  <th>C(Fertilizer)[T.4]</th> <td>   -0.0334</td> <td>    0.010</td> <td>   -3.363</td> <td> 0.003</td> <td>   -0.054</td> <td>   -0.013</td>
</tr>
<tr>
  <th>C(Sector)[T.2]</th>     <td>   -0.0521</td> <td>    0.014</td> <td>   -3.711</td> <td> 0.001</td> <td>   -0.081</td> <td>   -0.023</td>
</tr>
<tr>
  <th>C(Sector)[T.3]</th>     <td>   -0.0145</td> <td>    0.014</td> <td>   -1.034</td> <td> 0.313</td> <td>   -0.044</td> <td>    0.015</td>
</tr>
<tr>
  <th>C(Sector)[T.4]</th>     <td>   -0.0451</td> <td>    0.014</td> <td>   -3.211</td> <td> 0.004</td> <td>   -0.074</td> <td>   -0.016</td>
</tr>
<tr>
  <th>C(Sector)[T.5]</th>     <td>   -0.0616</td> <td>    0.014</td> <td>   -4.392</td> <td> 0.000</td> <td>   -0.091</td> <td>   -0.032</td>
</tr>
<tr>
  <th>C(Sector)[T.6]</th>     <td>   -0.0197</td> <td>    0.014</td> <td>   -1.402</td> <td> 0.176</td> <td>   -0.049</td> <td>    0.010</td>
</tr>
<tr>
  <th>C(Sector)[T.7]</th>     <td>    0.0085</td> <td>    0.014</td> <td>    0.605</td> <td> 0.551</td> <td>   -0.021</td> <td>    0.038</td>
</tr>
<tr>
  <th>C(Sector)[T.8]</th>     <td>   -0.0393</td> <td>    0.014</td> <td>   -2.803</td> <td> 0.011</td> <td>   -0.069</td> <td>   -0.010</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.984</td> <th>  Durbin-Watson:     </th> <td>   2.574</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.371</td> <th>  Jarque-Bera (JB):  </th> <td>   1.162</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.071</td> <th>  Prob(JB):          </th> <td>   0.559</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.077</td> <th>  Cond. No.          </th> <td>    9.67</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb63"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb63-1"><a href="mlr.html#cb63-1" tabindex="-1"></a>sm.stats.anova_lm(model_b, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                  sum_sq    df         F    PR(&gt;F)
## C(Fertilizer)  0.005086   3.0  4.306540  0.016222
## C(Sector)      0.017986   7.0  6.526673  0.000364
## Residual       0.008267  21.0       NaN       NaN</code></pre>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Multiple Linear Regression<a href="mlr.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most practical applications of of regression modeling involve using more complicated models than a simple linear regression with only one predictor variable to predict your target. Additional variables in a model can lead to better explanations and predictions of the target. These linear regressions with more than one variable are called <strong>multiple linear regression</strong> models. However, as we will see in this section and the following chapters, with more variables comes much more complication.</p>
<div id="model-structure" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Model Structure<a href="mlr.html#model-structure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Multiple linear regression models have the same structure as simple linear regression models, only with more variables. The multiple linear regression model with <span class="math inline">\(k\)</span> variables is structured like the following:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
\]</span></p>
<p>This model has the predictor variables <span class="math inline">\(x_1, x_2, ..., x_k\)</span> trying to either explain or predict the target variable <span class="math inline">\(y\)</span>. The intercept, <span class="math inline">\(\beta_0\)</span>, still gives the expected value of <span class="math inline">\(y\)</span>, when <strong>all</strong> of the predictor variables take a value of 0. With the addition of multiple predictors, the interpretation of the slope coefficients change slightly. The slopes, <span class="math inline">\(\beta_1, \beta_2, ..., \beta_k\)</span>, give the expected change in <span class="math inline">\(y\)</span> for a one unit change in the respective predictor variable, <strong>holding all other predictor variables constant</strong>. The random error term, <span class="math inline">\(\varepsilon\)</span>, is the error between our predicted value, <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \cdots + \hat{\beta}_k x_k\)</span>, and our actual value of <span class="math inline">\(y\)</span>.</p>
<p>Unlike simple linear regression that can be visualized as a line through a 2-dimensional scatterplot of data, a multiple linear regression is better thought of as a multi-dimensional plane through a multi-dimensional scatterplot of data.</p>
<p>Let’s visual an example with two predictor variables - the square footage of greater living area and the total number of rooms. These will predict sale price of a home. When none of the variables have any relationship with the target variable, we get a horizontal plane like the one shown below. This is similar in concept to a horizontal line in simple linear regression having a slope of 0, implying that the target variable does not change as the predictor variable changes.</p>
<div id="r-code-21" class="section level4 hasAnchor" number="3.4.1.1">
<h4><span class="header-section-number">3.4.1.1</span> R code:<a href="mlr.html#r-code-21" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="plotly html-widget html-fill-item" id="htmlwidget-7c989613c322c9b30053" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-7c989613c322c9b30053">{"x":{"visdat":{"2e5c73c26b8a":["function () ","plotlyVisDat"]},"cur_data":"2e5c73c26b8a","attrs":{"2e5c73c26b8a":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Living Area (SQFT)"},"yaxis":{"title":"Total Rooms"},"zaxis":{"title":"Sale Price"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"pred_mx","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(37,144,140,1)"],["1","rgba(37,144,140,1)"]],"showscale":true,"z":[[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967],[178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967,178987.30814236967]],"type":"surface","x":[300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000],"y":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Much like if the slope of a simple linear regression line is <strong>not</strong> 0 (a relationship exists between the predictor and target variable), then a relationship between any of the predictor variables and the target variable shifts and rotates the plane around like the one shown below.</p>
<div class="plotly html-widget html-fill-item" id="htmlwidget-52a679fb265f5fe63e24" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-52a679fb265f5fe63e24">{"x":{"visdat":{"2e5c3cef1e90":["function () ","plotlyVisDat"]},"cur_data":"2e5c3cef1e90","attrs":{"2e5c3cef1e90":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Living Area (SQFT)"},"yaxis":{"title":"Total Rooms"},"zaxis":{"title":"Sale Price"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"pred_mx","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[[67188.04070542485,61466.240315028263,55744.43992463169,50022.639534235102,44300.839143838522,38579.038753441942,32857.238363045355,27135.437972648782,21413.637582252195,15691.837191855608,9970.0368014590349,4248.2364110624476,-1473.5639793341397,-7195.3643697307125,-12917.164760127285,-18638.965150523887,-24360.76554092046,-30082.565931317033,-35804.366321713635,-41526.166712110207,-47247.967102506766,-52969.767492903382,-58691.56788329994,-64413.368273696527,-70135.168664093115],[97483.921487423315,91762.121097026728,86040.320706630155,80318.520316233567,74596.71992583698,68874.919535440407,63153.11914504382,57431.318754647247,51709.51836425066,45987.717973854073,40265.9175834575,34544.117193060913,28822.316802664325,23100.516412267752,17378.71602187118,11656.915631474578,5935.1152410780051,213.31485068143229,-5508.4855397151696,-11230.285930111742,-16952.086320508301,-22673.886710904917,-28395.687101301475,-34117.487491698063,-39839.28788209465],[127779.80226942177,122058.00187902518,116336.20148862861,110614.40109823202,104892.60070783543,99170.800317438858,93448.99992704227,87727.199536645698,82005.39914624911,76283.598755852523,70561.79836545595,64839.997975059363,59118.197584662776,53396.397194266203,47674.59680386963,41952.796413473028,36230.996023076455,30509.195632679883,24787.395242283281,19065.594851886708,13343.79446149015,7621.9940710935334,1900.1936806969752,-3821.6067096996121,-9543.4071000961994],[158075.68305142023,152353.88266102367,146632.08227062708,140910.2818802305,135188.48148983391,129466.68109943734,123744.88070904075,118023.08031864418,112301.27992824759,106579.479537851,100857.67914745443,95135.878757057842,89414.078366661255,83692.277976264682,77970.47758586811,72248.677195471508,66526.876805074935,60805.076414678362,55083.27602428176,49361.475633885188,43639.675243488629,37917.874853092013,32196.074462695455,26474.274072298867,20752.47368190228],[188371.56383341868,182649.76344302212,176927.96305262554,171206.16266222895,165484.36227183236,159762.5618814358,154040.76149103919,148318.96110064263,142597.16071024604,136875.36031984945,131153.55992945289,125431.75953905629,119709.95914865971,113988.15875826313,108266.35836786656,102544.55797746996,96822.757587073385,91100.957196676813,85379.156806280211,79657.356415883638,73935.55602548708,68213.755635090463,62491.955244693905,56770.154854297318,51048.35446390073],[218667.44461541716,212945.6442250206,207223.84383462401,201502.04344422743,195780.24305383084,190058.44266343425,184336.64227303769,178614.84188264111,172893.04149224452,167171.24110184793,161449.44071145135,155727.64032105479,150005.83993065817,144284.03954026161,138562.23914986505,132840.43875946844,127118.63836907186,121396.83797867529,115675.03758827869,109953.23719788212,104231.43680748556,98509.636417088943,92787.836026692385,87066.035636295797,81344.23524589921],[248963.32539741567,243241.52500701911,237519.72461662252,231797.92422622594,226076.12383582935,220354.32344543276,214632.5230550362,208910.72266463962,203188.92227424303,197467.12188384644,191745.32149344985,186023.5211030533,180301.72071265668,174579.92032226012,168858.11993186356,163136.31954146695,157414.51915107039,151692.7187606738,145970.91837027721,140249.11797988063,134527.31758948407,128805.51719908745,123083.71680869089,117361.91641829431,111640.11602789772],[279259.20617941406,273537.4057890175,267815.60539862089,262093.80500822433,256372.00461782774,250650.20422743115,244928.4038370346,239206.60344663801,233484.80305624142,227763.00266584483,222041.20227544825,216319.40188505169,210597.60149465507,204875.80110425851,199154.00071386195,193432.20032346534,187710.39993306878,181988.59954267219,176266.79915227561,170544.99876187902,164823.19837148246,159101.39798108584,153379.59759068929,147657.7972002927,141935.99680989611],[309555.08696141257,303833.28657101601,298111.48618061939,292389.68579022284,286667.88539982628,280946.08500942966,275224.2846190331,269502.48422863649,263780.68383823993,258058.88344784334,252337.08305744675,246615.2826670502,240893.48227665358,235171.68188625702,229449.88149586046,223728.08110546385,218006.28071506729,212284.4803246707,206562.67993427411,200840.87954387753,195119.07915348097,189397.27876308435,183675.47837268779,177953.67798229121,172231.87759189462],[339850.96774341108,334129.16735301452,328407.3669626179,322685.56657222135,316963.76618182479,311241.96579142817,305520.16540103161,299798.365010635,294076.56462023844,288354.76422984188,282632.96383944526,276911.1634490487,271189.36305865209,265467.56266825553,259745.76227785897,254023.96188746236,248302.1614970658,242580.36110666921,236858.56071627262,231136.76032587604,225414.95993547948,219693.15954508286,213971.3591546863,208249.55876428972,202527.75837389313],[370146.84852540947,364425.04813501291,358703.2477446163,352981.44735421974,347259.64696382318,341537.84657342656,335816.04618303,330094.24579263339,324372.44540223683,318650.64501184027,312928.84462144366,307207.0442310471,301485.24384065048,295763.44345025392,290041.64305985736,284319.84266946075,278598.04227906419,272876.24188866757,267154.44149827102,261432.64110787443,255710.84071747787,249989.04032708125,244267.23993668469,238545.43954628811,232823.63915589152],[400442.72930740798,394720.92891701142,388999.1285266148,383277.32813621825,377555.52774582169,371833.72735542507,366111.92696502851,360390.1265746319,354668.32618423534,348946.52579383878,343224.72540344216,337502.92501304561,331781.12462264899,326059.32423225243,320337.52384185587,314615.72345145926,308893.9230610627,303172.12267066608,297450.32228026952,291728.52188987297,286006.72149947635,280284.92110907973,274563.12071868323,268841.32032828662,263119.51993789],[430738.61008940649,425016.80969900993,419295.00930861331,413573.20891821675,407851.4085278202,402129.60813742358,396407.80774702702,390686.00735663041,384964.20696623385,379242.40657583729,373520.60618544067,367798.80579504411,362077.0054046475,356355.20501425094,350633.40462385438,344911.60423345777,339189.80384306121,333468.00345266459,327746.20306226803,322024.40267187147,316302.60228147486,310580.80189107824,304859.00150068174,299137.20111028512,293415.40071988851],[461034.49087140488,455312.69048100832,449590.89009061171,443869.08970021515,438147.28930981859,432425.48891942197,426703.68852902541,420981.8881386288,415260.08774823224,409538.28735783568,403816.48696743906,398094.68657704251,392372.88618664589,386651.08579624933,380929.28540585277,375207.48501545616,369485.6846250596,363763.88423466298,358042.08384426642,352320.28345386987,346598.48306347325,340876.68267307663,335154.88228268013,329433.08189228352,323711.2815018869],[491330.37165340339,485608.57126300683,479886.77087261021,474164.97048221366,468443.1700918171,462721.36970142048,456999.56931102392,451277.76892062731,445555.96853023075,439834.16813983419,434112.36774943757,428390.56735904102,422668.7669686444,416946.96657824784,411225.16618785128,405503.36579745467,399781.56540705811,394059.76501666149,388337.96462626493,382616.16423586837,376894.36384547176,371172.56345507514,365450.76306467864,359728.96267428203,354007.16228388541],[521626.2524354019,515904.45204500534,510182.65165460872,504460.85126421216,498739.05087381561,493017.25048341899,487295.45009302243,481573.64970262581,475851.84931222926,470130.0489218327,464408.24853143608,458686.44814103952,452964.64775064291,447242.84736024635,441521.04696984979,435799.24657945317,430077.44618905662,424355.64579866,418633.84540826344,412912.04501786688,407190.24462747027,401468.44423707365,395746.64384667715,390024.84345628053,384303.04306588392],[551922.13321740029,546200.33282700367,540478.53243660717,534756.73204621056,529034.93165581394,523313.13126541738,517591.33087502082,511869.53048462421,506147.73009422765,500425.92970383109,494704.12931343447,488982.32892303792,483260.5285326413,477538.72814224474,471816.92775184818,466095.12736145157,460373.32697105501,454651.52658065839,448929.72619026183,443207.92579986528,437486.12540946866,431764.32501907204,426042.52462867554,420320.72423827893,414598.92384788231],[582218.0139993988,576496.21360900218,570774.41321860568,565052.61282820906,559330.81243781245,553609.01204741595,547887.21165701933,542165.41126662272,536443.6108762261,530721.8104858296,525000.01009543298,519278.20970503642,513556.40931463981,507834.60892424325,502112.80853384669,496391.00814345008,490669.20775305352,484947.4073626569,479225.60697226034,473503.80658186378,467782.00619146717,462060.20580107055,456338.40541067405,450616.60502027743,444894.80462988082],[612513.89478139731,606792.09439100069,601070.29400060419,595348.49361020757,589626.69321981096,583904.89282941446,578183.09243901784,572461.29204862122,566739.49165822472,561017.69126782811,555295.89087743149,549574.09048703487,543852.29009663837,538130.48970624176,532408.68931584514,526686.88892544853,520965.08853505203,515243.28814465541,509521.48775425885,503799.68736386229,498077.88697346568,492356.08658306906,486634.28619267256,480912.48580227594,475190.68541187933],[642809.7755633957,637087.97517299908,631366.17478260258,625644.37439220597,619922.57400180935,614200.77361141285,608478.97322101623,602757.17283061962,597035.37244022312,591313.5720498265,585591.77165942988,579869.97126903327,574148.17087863677,568426.37048824015,562704.57009784353,556982.76970744692,551260.96931705042,545539.1689266538,539817.36853625719,534095.56814586069,528373.76775546407,522651.96736506745,516930.16697467095,511208.36658427434,505486.56619387772],[673105.65634539421,667383.85595499759,661662.05556460109,655940.25517420447,650218.45478380786,644496.65439341136,638774.85400301474,633053.05361261813,627331.25322222151,621609.45283182501,615887.65244142839,610165.85205103178,604444.05166063528,598722.25127023866,593000.45087984204,587278.65048944554,581556.85009904893,575835.04970865231,570113.24931825569,564391.44892785919,558669.64853746258,552947.84814706596,547226.04775666946,541504.24736627284,535782.44697587623],[703401.53712739272,697679.7367369961,691957.9363465996,686236.13595620298,680514.33556580637,674792.53517540987,669070.73478501325,663348.93439461663,657627.13400422013,651905.33361382352,646183.5332234269,640461.73283303028,634739.93244263378,629018.13205223717,623296.33166184055,617574.53127144394,611852.73088104744,606130.93049065082,600409.1301002542,594687.3297098577,588965.52931946109,583243.72892906447,577521.92853866797,571800.12814827135,566078.32775787474],[733697.41790939111,727975.61751899449,722253.81712859799,716532.01673820138,710810.21634780476,705088.41595740826,699366.61556701164,693644.81517661503,687923.01478621853,682201.21439582191,676479.41400542529,670757.61361502868,665035.81322463218,659314.01283423556,653592.21244383894,647870.41205344233,642148.61166304583,636426.81127264921,630705.01088225259,624983.21049185609,619261.41010145948,613539.60971106286,607817.80932066636,602096.00893026975,596374.20853987313],[763993.29869138962,758271.498300993,752549.6979105965,746827.89752019988,741106.09712980327,735384.29673940677,729662.49634901015,723940.69595861353,718218.89556821692,712497.09517782042,706775.2947874238,701053.49439702719,695331.69400663069,689609.89361623407,683888.09322583745,678166.29283544095,672444.49244504434,666722.69205464772,661000.8916642511,655279.0912738546,649557.29088345799,643835.49049306137,638113.69010266487,632391.88971226825,626670.08932187164],[794289.17947338813,788567.37908299151,782845.57869259501,777123.77830219839,771401.97791180178,765680.17752140528,759958.37713100866,754236.57674061204,748514.77635021554,742792.97595981893,737071.17556942231,731349.37517902569,725627.57478862919,719905.77439823258,714183.97400783596,708462.17361743934,702740.37322704284,697018.57283664623,691296.77244624961,685574.97205585311,679853.1716654565,674131.37127505988,668409.57088466338,662687.77049426676,656965.97010387015]],"type":"surface","x":[300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000],"y":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>To the naive viewer, the shifted plane would still make sense because of the model naming convention of multiple <strong>linear</strong> regression. However, the <strong>linear</strong> in linear regression doesn’t have to deal with the visualization of the fitted plane (or line in two dimensions), but instead refers to the <strong>linear combination of variables</strong>. A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results. For example, <span class="math inline">\(ax + by\)</span> is a linear combination of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Therefore, the linear model</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
\]</span>
is a linear combination of predictor variables in their relationship with the target variable <span class="math inline">\(y\)</span>. These predictor variables do not all have to contain linear effects though. For example, let’s look at a linear regression model with four predictor variables:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon
\]</span></p>
<p>One would not be hard pressed to call this model a linear regression. However, what if we defined <span class="math inline">\(x_3 = x_1^2\)</span> and <span class="math inline">\(x_4 = x_2^2\)</span>?</p>
<div class="plotly html-widget html-fill-item" id="htmlwidget-a93aaf1e55bab4545bae" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-a93aaf1e55bab4545bae">{"x":{"visdat":{"2e5c2bb11651":["function () ","plotlyVisDat"]},"cur_data":"2e5c2bb11651","attrs":{"2e5c2bb11651":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Living Area (SQFT)"},"yaxis":{"title":"Total Rooms"},"zaxis":{"title":"Sale Price"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"pred_mx","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[[33947.323797696932,28781.598789019183,23439.163605725957,17920.01824781725,12224.162715293078,6351.597008153416,302.32112639828483,-5923.6649299723213,-12326.361160958406,-18905.767566559975,-25661.884146777,-32594.71090160952,-39704.24783105751,-46990.494935120965,-54453.452213799901,-62093.119667094332,-69909.497295004214,-77902.585097529576,-86072.383074670433,-94418.891226426756,-102942.10955279853,-111642.03805378583,-120518.67672938856,-129572.02557960679,-138802.08460444052],[74873.353534654976,69707.628525977227,64365.193342683997,58846.047984775279,53150.192452251118,47277.626745111455,41228.350863356325,35002.364806985715,28599.668575999633,22020.262170398069,15264.14559018104,8331.3188353485202,1221.7819059005305,-6064.4651981629286,-13527.422476841863,-21167.089930136288,-28983.467558046177,-36976.555360571539,-45146.353337712397,-53492.861489468713,-62016.079815840501,-70716.008316827792,-79592.646992430527,-88645.995842648757,-97876.054867482482],[114263.22581347212,109097.50080479437,103755.06562150114,98235.92026359243,92540.064731068269,86667.499023928598,80618.223142173476,74392.237085802859,67989.540854816791,61410.134449215213,54654.017868998191,47721.19111416566,40611.654184717685,33325.407080654222,25862.449801975286,18222.782348680863,10406.404720770974,2413.3169182456113,-5756.481058895246,-14102.989210651562,-22626.207537023351,-31326.136038010649,-40202.774713613377,-49256.123563831599,-58486.182588665331],[152116.94063414837,146951.21562547059,141608.78044217741,136089.63508426867,130393.77955174452,124521.21384460485,118471.93796284971,112245.95190647909,105843.25567549303,99263.849269891449,92507.73268967442,85574.905934841896,78465.369005393906,71179.121901330451,63716.164622651515,56076.497169357091,48260.119541447202,40267.03173892184,32097.233761780983,23750.725610024667,15227.507283652878,6527.5787826655796,-2349.0598929371481,-11402.408743155371,-20632.467767989103],[188434.49799668361,183268.77298800583,177926.33780471265,172407.19244680391,166711.33691427976,160838.77120714009,154789.49532538495,148563.50926901435,142160.81303802828,135581.40663242669,128825.29005220969,121892.46329737715,114782.92636792918,107496.67926386571,100033.72198518677,92394.054531892354,84577.676903982487,76584.58910145711,68414.791124316238,60068.282972559937,51545.064646188148,42845.13614520085,33968.497469598122,24915.148619379899,15685.089594546167],[223215.89790107802,218050.17289240024,212707.73770910705,207188.59235119831,201492.73681867417,195620.1711115345,189570.89522977936,183344.90917340876,176942.21294242269,170362.8065368211,163606.6899566041,156673.86320177157,149564.32627232358,142278.0791682601,134815.12188958115,127175.45443628676,119359.07680837688,111365.9890058515,103196.19102871063,94849.682876954321,86326.464550582547,77626.536049595248,68749.897373992499,59696.548523774291,50466.489498940558],[256461.1403473315,251295.41533865372,245952.98015536051,240433.83479745183,234737.97926492765,228865.41355778798,222816.13767603284,216590.15161966227,210187.45538867617,203608.04898307458,196851.93240285758,189919.10564802506,182809.56871857707,175523.32161451358,168060.36433583463,160420.69688254024,152604.31925463036,144611.23145210498,136441.43347496411,128094.9253232078,119571.70699683603,110871.77849584873,101995.13982024598,92941.790970027767,83711.731945194042],[288170.22533544403,283004.50032676628,277662.06514347304,272142.91978556436,266447.06425304018,260574.49854590051,254525.22266414535,248299.2366077748,241896.5403767887,235317.13397118711,228561.01739097008,221628.19063613762,214518.6537066896,207232.40660262614,199769.44932394719,192129.78187065275,184313.40424274289,176320.31644021752,168150.51846307665,159804.01031132034,151280.79198494856,142580.86348396126,133704.22480835853,124650.8759581403,115420.81693330657],[318343.15286541567,313177.42785673792,307834.99267344468,302315.847315536,296619.99178301182,290747.42607587215,284698.15019411698,278472.16413774644,272069.46790676034,265490.06150115875,258733.94492094172,251801.11816610926,244691.58123666124,237405.33413259778,229942.37685391883,222302.70940062439,214486.33177271453,206493.24397018916,198323.44599304828,189976.93784129201,181453.7195149202,172753.7910139329,163877.15233833017,154823.80348811191,145593.74446327818],[346979.92293724651,341814.19792856876,336471.76274527551,330952.61738736677,325256.7618548426,319384.19614770293,313334.92026594776,307108.93420957727,300706.23797859118,294126.83157298958,287370.71499277255,280437.88823794003,273328.35130849201,266042.10420442856,258579.14692574961,250939.47947245528,243123.10184454537,235130.01404201999,226960.21606487912,218613.70791312278,210090.48958675101,201390.56108576371,192513.92241016094,183460.57355994277,174230.51453510905],[374080.53555093624,368914.81054225849,363572.37535896525,358053.23000105657,352357.37446853239,346484.80876139272,340435.53287963755,334209.54682326701,327806.85059228091,321227.44418667932,314471.32760646229,307538.50085162983,300428.96392218181,293142.71681811835,285679.7595394394,278040.09208614496,270223.71445823507,262230.6266557097,254060.82867856885,245714.32052681257,237191.10220044077,228491.17369945347,219614.53502385074,210561.18617363248,201331.12714879875],[399644.99070648511,394479.26569780736,389136.83051451412,383617.68515660544,377921.82962408126,372049.26391694159,365999.98803518643,359774.00197881588,353371.30574782979,346791.89934222819,340035.78276201116,333102.95600717864,325993.41907773068,318707.17197366722,311244.21469498827,303604.54724169389,295788.16961378395,287795.08181125857,279625.2838341177,271278.77568236139,262755.55735598964,254055.62885500235,245178.99017939961,236125.64132918141,226895.58230434768],[423673.28840389312,418507.56339521537,413165.12821192213,407645.98285401339,401950.12732148921,396077.56161434966,390028.28573259449,383802.29967622389,377399.60344523779,370820.1970396362,364064.08045941917,357131.25370458665,350021.71677513863,342735.46967107517,335272.51239239622,327632.84493910189,319816.46731119195,311823.37950866658,303653.58153152571,295307.0733797694,286783.85505339759,278083.9265524103,269207.28787680756,260153.93902658942,250923.88000175569],[446165.42864316015,440999.7036344824,435657.26845118916,430138.12309328042,424442.26756075624,418569.70185361669,412520.42597186152,406294.43991549092,399891.74368450482,393312.33727890323,386556.2206986862,379623.39394385368,372513.85701440566,365227.6099103422,357764.65263166337,350124.98517836892,342308.60755045898,334315.51974793361,326145.72177079273,317799.21361903643,309275.99529266474,300576.06679167744,291699.42811607471,282646.07926585648,273416.02024102269],[467121.41142428626,461955.68641560851,456613.25123231526,451094.10587440652,445398.25034188246,439525.68463474279,433476.40875298763,427250.42269661702,420847.72646563093,414268.32006002933,407512.20347981231,400579.37672497978,393469.83979553176,386183.59269146842,378720.63541278947,371080.96795949503,363264.59033158515,355271.50252905977,347101.7045519189,338755.19640016253,330231.97807379073,321532.04957280343,312655.41089720081,303602.06204698258,294372.00302214886],[486541.23674727161,481375.51173859386,476033.07655530062,470513.93119739188,464818.07566486771,458945.50995772815,452896.23407597299,446670.24801960238,440267.55178861629,433688.14538301469,426932.02880279766,419999.20204796514,412889.66511851712,405603.41801445367,398140.46073577483,390500.79328248039,382684.41565457045,374691.32785204507,366521.5298749042,358175.02172314789,349651.80339677609,340951.87489578879,332075.23622018617,323021.88736996794,313791.82834513416],[504424.90461211582,499259.17960343807,493916.74442014482,488397.59906223608,482701.74352971202,476829.17782257235,470779.90194081719,464553.91588444659,458151.21965346049,451571.8132478589,444815.69666764187,437882.86991280934,430773.33298336132,423487.08587929799,416024.12860061903,408384.46114732459,400568.08351941465,392574.99571688927,384405.1977397484,376058.68958799209,367535.47126162029,358835.54276063299,349958.90408503037,340905.55523481214,331675.49620997836],[520772.41501881927,515606.69001014152,510264.25482684828,504745.10946893954,499049.25393641548,493176.68822927581,487127.41234752064,480901.42629115004,474498.73006016394,467919.32365456235,461163.20707434532,454230.3803195128,447120.84339006478,439834.59628600144,432371.63900732249,424731.97155402804,416915.5939261181,408922.50612359273,400752.70814645186,392406.19999469555,383882.98166832374,375183.05316733645,366306.41449173383,357253.0656415156,348023.00661668181],[535583.76796738175,530418.04295870394,525075.60777541075,519556.46241750201,513860.60688497795,507988.04117783828,501938.76529608312,495712.77923971252,489310.08300872642,482730.67660312483,475974.5600229078,469041.73326807527,461932.19633862725,454645.94923456392,447182.99195588497,439543.32450259052,431726.94687468058,423733.8590721552,415564.06109501433,407217.55294325802,398694.33461688622,389994.40611589892,381117.7674402963,372064.41859007807,362834.35956524429],[548858.96345780336,543693.23844912543,538350.80326583236,532831.65790792357,527135.80237539939,521263.23666825989,515213.96078650467,508987.97473013407,502585.27849914797,496005.87209354638,489249.75551332935,482316.92875849683,475207.39182904881,467921.14472498535,460458.18744630652,452818.51999301207,445002.14236510219,437009.05456257681,428839.25658543594,420492.74843367958,411969.53010730777,403269.60160632047,394392.96293071785,385339.61408049962,376109.5550556659],[560598.00149008387,555432.27648140618,550089.84129811288,544570.69594020431,538874.84040767991,533002.27470054035,526952.99881878542,520727.01276241464,514324.31653142866,507744.91012582701,500988.79354560998,494055.96679077746,486946.42986132944,479660.18275726598,472197.22547858715,464557.5580252927,456741.18039738282,448748.09259485744,440578.29461771657,432231.78646596021,423708.5681395884,415008.6396386011,406132.00096299849,397078.65211278026,387848.59308794653],[570800.88206422376,565635.15705554595,560292.72187225276,554773.57651434408,549077.7209818199,543205.15527468023,537155.87939292518,530929.89333655452,524527.19710556837,517947.79069996683,511191.6741197498,504258.84736491728,497149.31043546926,489863.06333140592,482400.10605272697,474760.43859943253,466944.06097152259,458950.97316899721,450781.17519185634,442434.66704010003,433911.44871372823,425211.52021274093,416334.88153713831,407281.53268692008,398051.4736620863],[579467.60518022266,574301.88017154473,568959.44498825166,563440.29963034287,557744.44409781869,551871.87839067914,545822.60250892397,539596.61645255343,533193.92022156715,526614.51381596574,519858.39723574859,512925.57048091618,505816.03355146805,498529.78644740465,491066.82916872582,483427.16171543137,475610.78408752149,467617.69628499611,459447.89830785524,451101.39015609887,442578.17182972707,433878.24332873977,425001.60465313715,415948.25580291892,406718.1967780852],[586598.17083808046,581432.44582940266,576090.01064610947,570570.86528820079,564875.0097556765,559002.44404853694,552953.16816678189,546727.18211041123,540324.48587942508,533745.07947382354,526988.96289360651,520056.13613877399,512946.59920932597,505660.35210526251,498197.39482658368,490557.72737328924,482741.3497453793,474748.26194285392,466578.46396571305,458231.95581395674,449708.73748758493,441008.80898659764,432132.17031099502,423078.82146077679,413848.762435943],[592192.5790377974,587026.85402911971,581684.41884582641,576165.27348791785,570469.41795539344,564596.85224825388,558547.57636649872,552321.59031012817,545918.8940791419,539339.48767354048,532583.37109332357,525650.54433849093,518541.00740904291,511254.76030497946,503791.80302630062,496152.13557300618,488335.75794509624,480342.67014257086,472172.87216542999,463826.36401367368,455303.14568730188,446603.21718631458,437726.57851071196,428673.22966049373,419443.17063565995]],"type":"surface","x":[300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000],"y":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>This model is still a linear regression model. The structure of the model did not change. The model is still a linear combination of predictor variables related to the target variable. The predictor variables just do not all have a linear effect in terms of their relationship with <span class="math inline">\(y\)</span>. However, mathematically, it is still a linear combination and a linear regression model.</p>
</div>
</div>
<div id="global-local-inference" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Global &amp; Local Inference<a href="mlr.html#global-local-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In simple linear regression we could just look at the t-test for our slope parameter estimate to determine the utility of our model. With multiple parameter estimates comes multiple t-tests. Instead of looking at every individual parameter estimate initially, there is a way to determine the model adequacy for predicting the target variable overall.</p>
<p>The utility of a multiple regression model can be tested with a single test that encompasses all the coefficients from the model. This kind of test is called a <strong>global test</strong> since it tests all <span class="math inline">\(\beta\)</span>’s simultaneously. The <strong>Global F-Test</strong> uses the F-distribution to do just that for multiple linear regression models. The hypotheses for this test are the following:</p>
<p><span class="math display">\[
H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0 \\
H_A: \text{at least one } \beta \text{ is nonzero}
\]</span></p>
<p>In simpler terms, the null hypothesis is that none of the variables are useful in predicting the target variable. The alternative hypothesis is that <strong>at least one</strong> of these variables is useful in predicting the target variable.</p>
<p>The F-distribution is a distribution that has the following characteristics:</p>
<ul>
<li>Bounded below by 0</li>
<li>Right-skewed</li>
<li>Both <strong>numerator</strong> and <strong>denominator</strong> degrees of freedom</li>
</ul>
<p>A plot of a variety of F distributions is shown here:</p>
<pre><code>## Warning: Removed 1500 rows containing missing values or values outside the scale range
## (`geom_line()`).</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<p>If the global test is significant, the next step would be to examine the individual t-tests to see which variables are significant and which ones are not. This is similar to post-hoc testing in ANOVA where we explored which of the categories was statistically different when we knew at least one was.</p>
<p>These tests are all available using the <code>summary</code> function on an <code>lm</code> function for linear regression. To build a multiple linear regression in R using the <code>lm</code> function, you just add another variable to the formula element. Here we will predict the sales price (<code>Sale_Price</code>) based on the square footage of the greater living area of the home (<code>Gr_Liv_Area</code>) as well as total number of rooms above ground (<code>TotRms_AbvGrd</code>).</p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="mlr.html#cb66-1" tabindex="-1"></a>ames_lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> TotRms_AbvGrd, <span class="at">data =</span> train)</span>
<span id="cb66-2"><a href="mlr.html#cb66-2" tabindex="-1"></a></span>
<span id="cb66-3"><a href="mlr.html#cb66-3" tabindex="-1"></a><span class="fu">summary</span>(ames_lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -528656  -30077   -1230   21427  361465 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    42562.657   5365.721   7.932 3.51e-15 ***
## Gr_Liv_Area      136.982      4.207  32.558  &lt; 2e-16 ***
## TotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56630 on 2048 degrees of freedom
## Multiple R-squared:  0.5024, Adjusted R-squared:  0.5019 
## F-statistic:  1034 on 2 and 2048 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>At the bottom of the above output is the result of the global F-test. Since the p-value on this test is lower than the significance level of 0.05, we have statistical evidence that at least of the two variables - <code>Gr_Liv_Area</code> and <code>TotRms_AbvGrd</code> - is significant at predicting the sale price of the home. By looking at the individual t-tests in the output above, we can see that both variables are actually significant.</p>
</div>
<div id="assumptions-2" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Assumptions<a href="mlr.html#assumptions-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>main</strong> assumptions for the multiple linear regression model are the same as with the simple linear regression model:</p>
<ol style="list-style-type: decimal">
<li>The expected value of <span class="math inline">\(y\)</span> is linear in the <span class="math inline">\(x\)</span>’s (proper model specification).</li>
<li>The random errors are independent.</li>
<li>The random errors are normally distributed.</li>
<li>The random errors have equal variance (homoskedasticity).</li>
</ol>
<p>However, with multiple variables there is an additional assumption that people tend to add to multiple linear regression modeling:</p>
<ol start="5" style="list-style-type: decimal">
<li>No <strong>perfect</strong> collinearity (also called multicollinearity)</li>
</ol>
<p>The new assumption means that no combination of predictor variables is a perfect linear combination with any other predictor variables. Collinearity, also called multicollinearity, occurs when predictor variables are correlated with each other. People often misstate this additional assumption as having no collinearity at all. This is too restrictive and basically impossible to meet in a realistic setting. Only when collinearity has a drastic impact on the linear regression do we need to concern ourselves. In fact, linear regression only completely breaks when that collinearity is perfect. Dealing with multicollinearity is discussed later.</p>
<p>Similar to simple linear regression, we can evaluate the assumptions by looking at residual plots. The <code>plot</code> function on the <code>lm</code> object provides these.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="mlr.html#cb68-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb68-2"><a href="mlr.html#cb68-2" tabindex="-1"></a><span class="fu">plot</span>(ames_lm2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="mlr.html#cb69-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>These will again be covered in much more detail in Diagnostic Chapter.</p>
</div>
<div id="multiple-coefficients-of-determination" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Multiple Coefficients of Determination<a href="mlr.html#multiple-coefficients-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the main advantages of multiple linear regression is that the complexity of the model enables us to investigate the relationship among <span class="math inline">\(y\)</span> and several predictor variables simultaneously. However, this increased complexity makes it more difficult to not only interpret the models, but also ascertain which model is “best.”</p>
<p>One example of this would be the coefficient of determination, <span class="math inline">\(R^2\)</span>, that we discussed earlier. The calculation for <span class="math inline">\(R^2\)</span> is the exact same:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SSE}{TSS}
\]</span></p>
<p>However, the problem with the calculation of <span class="math inline">\(R^2\)</span> in a multiple linear regression is that the addition of any variable (useful or not) will never make the <span class="math inline">\(R^2\)</span> decrease. In fact, it typically increases even with the addition of a useless variable. The reason is rather intuitive. When adding information to a regression model, your predictions can only get better, not worse. If a new predictor variable has no impact on the target variable, then the predictions can not get any worse than what they already were before the addition of the useless variable. Therefore, the <span class="math inline">\(SSE\)</span> would never increase, making the <span class="math inline">\(R^2\)</span> never decrease.</p>
<p>To account for this problem, there is the <strong>adjusted coefficient of determination</strong>, <span class="math inline">\(R^2_a\)</span>. The calculation is the following:</p>
<p><span class="math display">\[
R^2_a = 1 - [(\frac{n-1}{n-(k+1)})\times (\frac{SSE}{TSS})]
\]</span></p>
<p>Notice what the calculation is doing. It takes the original ratio on the right hand side of the <span class="math inline">\(R^2\)</span> equation, <span class="math inline">\(SSE/TSS\)</span>, and penalizes it. It multiplies this number by a ratio that is always greater than 1 if <span class="math inline">\(k &gt; 0\)</span>. Remember, <span class="math inline">\(k\)</span> is the number of variables in the model. Therefore, as the number of variables increases, the calculation penalizes the model more and more. However, if the reduction of SSE from adding a useful variable is low enough, then even with the additional penalization, the <span class="math inline">\(R^2_a\)</span> will increase <strong>if the variable is a useful addition to the model</strong>. If the variable is not a useful addition to the model, the <span class="math inline">\(R^2_a\)</span> will decrease. The <span class="math inline">\(R^2_a\)</span> is only one of many ways to select the “best” model for multiple linear regression.</p>
<p>One downside of this new metric is that the <span class="math inline">\(R^2_a\)</span> loses its interpretation. Since <span class="math inline">\(R^2_a \le R^2\)</span>, it is no longer bounded below by zero. Therefore, it can no longer be the proportion of variation explained in the target variable by the model. However, we can easily use <span class="math inline">\(R^2_a\)</span> to select a model correctly and interpret that model with <span class="math inline">\(R^2\)</span>. Both of these numbers can be found using the <code>summary</code> function on the <code>lm</code> object from the previous model.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="mlr.html#cb70-1" tabindex="-1"></a><span class="fu">summary</span>(ames_lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -528656  -30077   -1230   21427  361465 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    42562.657   5365.721   7.932 3.51e-15 ***
## Gr_Liv_Area      136.982      4.207  32.558  &lt; 2e-16 ***
## TotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56630 on 2048 degrees of freedom
## Multiple R-squared:  0.5024, Adjusted R-squared:  0.5019 
## F-statistic:  1034 on 2 and 2048 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>From this output we can say that the combination of <code>Gr_Liv_Area</code> and <code>TotRmsAbvGrd</code> account for 50.24% of the variation in <code>Sale_Price</code>. Now let’s add a random variable to the model. This random variable will take random values from a normal distribution with mean of 0 and standard deviation of 1 and has no impact on the target variable.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="mlr.html#cb72-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb72-2"><a href="mlr.html#cb72-2" tabindex="-1"></a>ames_lm3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> TotRms_AbvGrd <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(Sale_Price), <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">data =</span> train)</span>
<span id="cb72-3"><a href="mlr.html#cb72-3" tabindex="-1"></a></span>
<span id="cb72-4"><a href="mlr.html#cb72-4" tabindex="-1"></a><span class="fu">summary</span>(ames_lm3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 
##     0, 1), data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -527926  -29943   -1298   21427  363925 
## 
## Coefficients:
##                                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                      42589.091   5364.877   7.939 3.34e-15 ***
## Gr_Liv_Area                        136.927      4.207  32.548  &lt; 2e-16 ***
## TotRms_AbvGrd                   -10552.425   1369.808  -7.704 2.05e-14 ***
## rnorm(length(Sale_Price), 0, 1)   1629.854   1259.478   1.294    0.196    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56620 on 2047 degrees of freedom
## Multiple R-squared:  0.5028, Adjusted R-squared:  0.502 
## F-statistic: 689.9 on 3 and 2047 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Notice that the <span class="math inline">\(R^2\)</span> of this model actually increased to 0.5028 from 0.5024. However, the <span class="math inline">\(R^2_a\)</span> value stayed approximately the same at 0.502 since the addition of this new variable did not provide enough predictive power to outweigh the penalty of adding it.</p>
</div>
<div id="categorical-predictor-variables" class="section level3 hasAnchor" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Categorical Predictor Variables<a href="mlr.html#categorical-predictor-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned in EDA Section, there are two types of variables typically used in modeling:</p>
<ul>
<li>Quantitative (or numeric)</li>
<li>Qualitative (or categorical)</li>
</ul>
<p>Categorical variables need to be coded differently because they are not numerical in nature. As mentioned in EDA Section, two common coding techniques for linear regression are <strong>reference</strong> and <strong>effects</strong> coding. The interpretation of the coefficients (<span class="math inline">\(\beta\)</span>’s) of these variables in a regression model depend on the specific coding used. The predictions from the model, however, will remain the same regardless of the specific coding that is used.</p>
<p>Let’s use the example of the <code>Central_Air</code> variable with 2 categories - Y and N. Using reference coding, the <strong>reference</strong> coded variable to describe these 2 categories (with N as the reference level) would be the following:</p>
<table>
<tr>
<td>
Central Air
<td>
X1
<tr>
<td>
Y
<td>
1
<tr>
<td>
N
<td>
0
</table>
<caption>
<span id="tab:centralair">Table 3.1: </span>Reference variable coding for the categorical attribute <em>Central Air</em>
</caption>
<p>The linear regression equation would be:</p>
<p><span class="math display">\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
\]</span></p>
<p>Let’s see the mathematical interpretation of the coefficient <span class="math inline">\(\hat{\beta}_1\)</span>. To do this, let’s get the average sale price of a home prediction for a home with central air (<span class="math inline">\(\hat{y}_Y\)</span>) and without central air (<span class="math inline">\(\hat{y}_N\)</span>):</p>
<p><span class="math display">\[
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot 0 = \hat{\beta}_0
\]</span></p>
<p>By subtracting these two equations (<span class="math inline">\(\hat{y}_Y - \hat{y}_N = \hat{\beta}_1\)</span>), we can get the prediction for the average difference in price between a home with central air and without central air. This shows that in reference coding, the coefficient on each dummy variable is the average difference between that category and the reference category (the category not represented with its own variable). The math can be extended to as many categories as needed.</p>
<p>Using effects coding, the <strong>effects</strong> coded variable to describe these 2 categories (with N as the reference level) would be the following:</p>
<table>
<tr>
<td>
Central Air
<td>
X1
<tr>
<td>
Y
<td>
1
<tr>
<td>
N
<td>
-1
</table>
<caption>
<span id="tab:centralair">Table 3.1: </span>Effects variable coding for the categorical attribute <em>Central Air</em>
</caption>
<p>The linear regression equation would be:</p>
<p><span class="math display">\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
\]</span></p>
<p>Let’s see the mathematical interpretation of the coefficient <span class="math inline">\(\hat{\beta}_1\)</span>. To do this, let’s get the average sale price of a home prediction for a home with central air (<span class="math inline">\(\hat{y}_Y\)</span>) and without central air (<span class="math inline">\(\hat{y}_N\)</span>):</p>
<p><span class="math display">\[
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot -1 = \hat{\beta}_0 - \hat{\beta}_1
\]</span></p>
<p>Similar to reference coding, the coefficient <span class="math inline">\(\hat{\beta}_1\)</span> is the average difference between homes with central air and <span class="math inline">\(\hat{\beta}_0\)</span>. However, what is <span class="math inline">\(\hat{\beta}_0\)</span>? By taking the average of our two predictions:</p>
<p><span class="math display">\[
\frac{1}{2} \times (\hat{y}_Y + \hat{y}_N) = \frac{1}{2} \times (\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_0 - \hat{\beta}_1) = \frac{1}{2} \times (2\hat{\beta}_0) = \hat{\beta}_0
\]</span></p>
<p>From this average we can get the prediction for the average difference in price between a home with central air and the average price across all homes. This shows that in effects coding, the coefficient on each dummy variable is the average difference between that category and the average price across <strong>all</strong> homes (including both with and without central air). The math can be extended to as many categories as needed.</p>
<p>Let’s see an example with <code>Central_Air</code> as a variable added to our multiple linear regression model as a reference coded variable.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="mlr.html#cb74-1" tabindex="-1"></a>ames_lm4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> TotRms_AbvGrd <span class="sc">+</span> Central_Air, <span class="at">data =</span> train)</span>
<span id="cb74-2"><a href="mlr.html#cb74-2" tabindex="-1"></a></span>
<span id="cb74-3"><a href="mlr.html#cb74-3" tabindex="-1"></a><span class="fu">summary</span>(ames_lm4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Central_Air, 
##     data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -510745  -28984   -2317   20273  356742 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -7169.259   6778.879  -1.058     0.29    
## Gr_Liv_Area     129.594      4.131  31.374  &lt; 2e-16 ***
## TotRms_AbvGrd -8980.938   1335.669  -6.724 2.29e-11 ***
## Central_AirY  54513.082   4762.926  11.445  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 54910 on 2047 degrees of freedom
## Multiple R-squared:  0.5323, Adjusted R-squared:  0.5316 
## F-statistic: 776.6 on 3 and 2047 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>With these results we estimate the average difference in sales price between homes with central air and without central air to be $54,513.08.</p>
<div id="python-code-21" class="section level4 hasAnchor" number="3.4.5.1">
<h4><span class="header-section-number">3.4.5.1</span> Python Code<a href="mlr.html#python-code-21" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Global and Local Inference</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb76-1"><a href="mlr.html#cb76-1" tabindex="-1"></a>model_mlr <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb76-2"><a href="mlr.html#cb76-2" tabindex="-1"></a>model_mlr.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.499</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.499</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1022.</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th> <td>1.82e-308</td>
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:13</td>     <th>  Log-Likelihood:    </th> <td> -25378.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.076e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2048</td>      <th>  BIC:               </th> <td>5.078e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>     <td> 4.435e+04</td> <td> 5356.883</td> <td>    8.279</td> <td> 0.000</td> <td> 3.38e+04</td> <td> 5.49e+04</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th>   <td>  137.0996</td> <td>    4.182</td> <td>   32.783</td> <td> 0.000</td> <td>  128.898</td> <td>  145.301</td>
</tr>
<tr>
  <th>TotRms_AbvGrd</th> <td>-1.061e+04</td> <td> 1344.500</td> <td>   -7.892</td> <td> 0.000</td> <td>-1.32e+04</td> <td>-7974.412</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>369.235</td> <th>  Durbin-Watson:     </th> <td>   2.044</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6501.598</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.299</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td>11.702</td>  <th>  Cond. No.          </th> <td>6.83e+03</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.83e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
<p>Assumptions</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="mlr.html#cb77-1" tabindex="-1"></a>train[<span class="st">&#39;pred_mlr&#39;</span>] <span class="op">=</span> model_mlr.predict()</span>
<span id="cb77-2"><a href="mlr.html#cb77-2" tabindex="-1"></a>train[<span class="st">&#39;resid_mlr&#39;</span>] <span class="op">=</span> model_mlr.resid</span>
<span id="cb77-3"><a href="mlr.html#cb77-3" tabindex="-1"></a></span>
<span id="cb77-4"><a href="mlr.html#cb77-4" tabindex="-1"></a>p <span class="op">=</span> (</span>
<span id="cb77-5"><a href="mlr.html#cb77-5" tabindex="-1"></a>    ggplot(train, aes(sample<span class="op">=</span><span class="st">&quot;resid_mlr&quot;</span>)) <span class="op">+</span></span>
<span id="cb77-6"><a href="mlr.html#cb77-6" tabindex="-1"></a>    geom_qq() <span class="op">+</span></span>
<span id="cb77-7"><a href="mlr.html#cb77-7" tabindex="-1"></a>    geom_qq_line(color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, linetype<span class="op">=</span><span class="st">&quot;dashed&quot;</span>) <span class="op">+</span></span>
<span id="cb77-8"><a href="mlr.html#cb77-8" tabindex="-1"></a>    labs(title<span class="op">=</span><span class="st">&quot;QQ Plot of Residuals&quot;</span>, x<span class="op">=</span><span class="st">&quot;Theoretical Quantiles&quot;</span>, y<span class="op">=</span><span class="st">&quot;Sample Quantiles&quot;</span>)</span>
<span id="cb77-9"><a href="mlr.html#cb77-9" tabindex="-1"></a>)</span>
<span id="cb77-10"><a href="mlr.html#cb77-10" tabindex="-1"></a></span>
<span id="cb77-11"><a href="mlr.html#cb77-11" tabindex="-1"></a>p.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-34-1.png" width="614" /></p>
<p>Multiple Coefficient of Determination</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb78-1"><a href="mlr.html#cb78-1" tabindex="-1"></a>model_mlr.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.499</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.499</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1022.</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th> <td>1.82e-308</td>
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:13</td>     <th>  Log-Likelihood:    </th> <td> -25378.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.076e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2048</td>      <th>  BIC:               </th> <td>5.078e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
        <td></td>           <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>     <td> 4.435e+04</td> <td> 5356.883</td> <td>    8.279</td> <td> 0.000</td> <td> 3.38e+04</td> <td> 5.49e+04</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th>   <td>  137.0996</td> <td>    4.182</td> <td>   32.783</td> <td> 0.000</td> <td>  128.898</td> <td>  145.301</td>
</tr>
<tr>
  <th>TotRms_AbvGrd</th> <td>-1.061e+04</td> <td> 1344.500</td> <td>   -7.892</td> <td> 0.000</td> <td>-1.32e+04</td> <td>-7974.412</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>369.235</td> <th>  Durbin-Watson:     </th> <td>   2.044</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6501.598</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.299</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td>11.702</td>  <th>  Cond. No.          </th> <td>6.83e+03</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.83e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
<p>Categorical Predictor Variables</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="mlr.html#cb79-1" tabindex="-1"></a>model_mlr2 <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + C(Central_Air)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb79-2"><a href="mlr.html#cb79-2" tabindex="-1"></a>model_mlr2.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.527</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.527</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   761.8</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Wed, 21 May 2025</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  
</tr>
<tr>
  <th>Time:</th>                 <td>10:35:14</td>     <th>  Log-Likelihood:    </th> <td> -25319.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.065e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2047</td>      <th>  BIC:               </th> <td>5.067e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
           <td></td>              <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>           <td>-6090.0603</td> <td> 6929.420</td> <td>   -0.879</td> <td> 0.380</td> <td>-1.97e+04</td> <td> 7499.388</td>
</tr>
<tr>
  <th>C(Central_Air)[T.Y]</th> <td>   5.5e+04</td> <td> 4986.530</td> <td>   11.029</td> <td> 0.000</td> <td> 4.52e+04</td> <td> 6.48e+04</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th>         <td>  129.0535</td> <td>    4.129</td> <td>   31.255</td> <td> 0.000</td> <td>  120.956</td> <td>  137.151</td>
</tr>
<tr>
  <th>TotRms_AbvGrd</th>       <td>-8868.5091</td> <td> 1316.089</td> <td>   -6.739</td> <td> 0.000</td> <td>-1.14e+04</td> <td>-6287.497</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>407.935</td> <th>  Durbin-Watson:     </th> <td>   2.036</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>6922.755</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.444</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td>11.956</td>  <th>  Cond. No.          </th> <td>1.02e+04</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.02e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="slr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-selection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/IAA-Faculty/statistical_foundations.git/edit/master/03-complex_ANOVA_Regression.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/IAA-Faculty/statistical_foundations.git/blob/master/03-complex_ANOVA_Regression.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "toc": null
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
