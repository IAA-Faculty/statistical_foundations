<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations</title>
  <meta name="description" content="Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="IAA-Faculty/statistical_foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Complex ANOVA and Multiple Linear Regression | Statistical Foundations" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="slr.html"/>
<link rel="next" href="model-selection.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a style="font-weight:bold, text-align:center" href="https://github.com/IAA-Faculty/statistical_foundations/">Statistical Foundations</a>
<img src="./img/iaaicon.png" alt="IAA"  class="center"</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-stat.html"><a href="intro-stat.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-stat.html"><a href="intro-stat.html#eda"><i class="fa fa-check"></i><b>1.1</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro-stat.html"><a href="intro-stat.html#vartypes"><i class="fa fa-check"></i><b>1.1.1</b> Types of Variables</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-stat.html"><a href="intro-stat.html#distributions"><i class="fa fa-check"></i><b>1.1.2</b> Distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-stat.html"><a href="intro-stat.html#normal"><i class="fa fa-check"></i><b>1.1.3</b> The Normal Distribution</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-stat.html"><a href="intro-stat.html#skew"><i class="fa fa-check"></i><b>1.1.4</b> Skewness</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-stat.html"><a href="intro-stat.html#kurt"><i class="fa fa-check"></i><b>1.1.5</b> Kurtosis</a></li>
<li class="chapter" data-level="1.1.6" data-path="intro-stat.html"><a href="intro-stat.html#graphdist"><i class="fa fa-check"></i><b>1.1.6</b> Graphical Displays of Distributions</a></li>
<li class="chapter" data-level="1.1.7" data-path="intro-stat.html"><a href="intro-stat.html#python-code"><i class="fa fa-check"></i><b>1.1.7</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro-stat.html"><a href="intro-stat.html#pointest"><i class="fa fa-check"></i><b>1.2</b> Point Estimates</a></li>
<li class="chapter" data-level="1.3" data-path="intro-stat.html"><a href="intro-stat.html#ci"><i class="fa fa-check"></i><b>1.3</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-stat.html"><a href="intro-stat.html#python-code-1"><i class="fa fa-check"></i><b>1.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-stat.html"><a href="intro-stat.html#hypotest"><i class="fa fa-check"></i><b>1.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro-stat.html"><a href="intro-stat.html#onesample"><i class="fa fa-check"></i><b>1.4.1</b> One-Sample T-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro-stat.html"><a href="intro-stat.html#python-code-2"><i class="fa fa-check"></i><b>1.4.2</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-stat.html"><a href="intro-stat.html#two-sample-t-tests"><i class="fa fa-check"></i><b>1.5</b> Two-Sample t-tests</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-stat.html"><a href="intro-stat.html#testnorm"><i class="fa fa-check"></i><b>1.5.1</b> Testing Normality of Groups</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-stat.html"><a href="intro-stat.html#ftest"><i class="fa fa-check"></i><b>1.5.2</b> Testing Equality of Variances</a></li>
<li class="chapter" data-level="1.5.3" data-path="intro-stat.html"><a href="intro-stat.html#tsttest"><i class="fa fa-check"></i><b>1.5.3</b> Testing Equality of Means</a></li>
<li class="chapter" data-level="1.5.4" data-path="intro-stat.html"><a href="intro-stat.html#python-code-3"><i class="fa fa-check"></i><b>1.5.4</b> Python Code</a></li>
<li class="chapter" data-level="1.5.5" data-path="intro-stat.html"><a href="intro-stat.html#wilcoxon"><i class="fa fa-check"></i><b>1.5.5</b> Mann-Whitney-Wilcoxon Test</a></li>
<li class="chapter" data-level="1.5.6" data-path="intro-stat.html"><a href="intro-stat.html#python-code-4"><i class="fa fa-check"></i><b>1.5.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="slr.html"><a href="slr.html"><i class="fa fa-check"></i><b>2</b> Introduction to ANOVA and Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="slr.html"><a href="slr.html#evp"><i class="fa fa-check"></i><b>2.1</b> Predictive vs. Explanatory</a></li>
<li class="chapter" data-level="2.2" data-path="slr.html"><a href="slr.html#trainvalidtest"><i class="fa fa-check"></i><b>2.2</b> Honest Assessment</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="slr.html"><a href="slr.html#python-code-5"><i class="fa fa-check"></i><b>2.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="slr.html"><a href="slr.html#bivariate-eda"><i class="fa fa-check"></i><b>2.3</b> Bivariate EDA</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="slr.html"><a href="slr.html#continuous-continuous-associations"><i class="fa fa-check"></i><b>2.3.1</b> Continuous-Continuous Associations</a></li>
<li class="chapter" data-level="2.3.2" data-path="slr.html"><a href="slr.html#continuous-categorical-associations"><i class="fa fa-check"></i><b>2.3.2</b> Continuous-Categorical Associations</a></li>
<li class="chapter" data-level="2.3.3" data-path="slr.html"><a href="slr.html#python-code-6"><i class="fa fa-check"></i><b>2.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="slr.html"><a href="slr.html#oneway"><i class="fa fa-check"></i><b>2.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="slr.html"><a href="slr.html#python-code-7"><i class="fa fa-check"></i><b>2.4.1</b> Python Code</a></li>
<li class="chapter" data-level="2.4.2" data-path="slr.html"><a href="slr.html#testing-assumptions"><i class="fa fa-check"></i><b>2.4.2</b> Testing Assumptions</a></li>
<li class="chapter" data-level="2.4.3" data-path="slr.html"><a href="slr.html#python-code-8"><i class="fa fa-check"></i><b>2.4.3</b> Python Code</a></li>
<li class="chapter" data-level="2.4.4" data-path="slr.html"><a href="slr.html#kruskal"><i class="fa fa-check"></i><b>2.4.4</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="2.4.5" data-path="slr.html"><a href="slr.html#python-code-9"><i class="fa fa-check"></i><b>2.4.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="slr.html"><a href="slr.html#posthoc"><i class="fa fa-check"></i><b>2.5</b> ANOVA Post-hoc Testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="slr.html"><a href="slr.html#tukey"><i class="fa fa-check"></i><b>2.5.1</b> Tukey-Kramer</a></li>
<li class="chapter" data-level="2.5.2" data-path="slr.html"><a href="slr.html#dunnett"><i class="fa fa-check"></i><b>2.5.2</b> Dunnett’s Test</a></li>
<li class="chapter" data-level="2.5.3" data-path="slr.html"><a href="slr.html#python-code-10"><i class="fa fa-check"></i><b>2.5.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="slr.html"><a href="slr.html#cor"><i class="fa fa-check"></i><b>2.6</b> Pearson Correlation</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="slr.html"><a href="slr.html#testcor"><i class="fa fa-check"></i><b>2.6.1</b> Statistical Test</a></li>
<li class="chapter" data-level="2.6.2" data-path="slr.html"><a href="slr.html#effect-of-anomalous-observations"><i class="fa fa-check"></i><b>2.6.2</b> Effect of Anomalous Observations</a></li>
<li class="chapter" data-level="2.6.3" data-path="slr.html"><a href="slr.html#the-correlation-matrix"><i class="fa fa-check"></i><b>2.6.3</b> The Correlation Matrix</a></li>
<li class="chapter" data-level="2.6.4" data-path="slr.html"><a href="slr.html#python-code-11"><i class="fa fa-check"></i><b>2.6.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="slr.html"><a href="slr.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="slr.html"><a href="slr.html#slrassumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions of Linear Regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="slr.html"><a href="slr.html#testing-for-association"><i class="fa fa-check"></i><b>2.7.2</b> Testing for Association</a></li>
<li class="chapter" data-level="2.7.3" data-path="slr.html"><a href="slr.html#python-code-12"><i class="fa fa-check"></i><b>2.7.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Complex ANOVA and Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#two-way-anova"><i class="fa fa-check"></i><b>3.1</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mlr.html"><a href="mlr.html#exploration"><i class="fa fa-check"></i><b>3.1.1</b> Exploration</a></li>
<li class="chapter" data-level="3.1.2" data-path="mlr.html"><a href="mlr.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
<li class="chapter" data-level="3.1.3" data-path="mlr.html"><a href="mlr.html#post-hoc-testing"><i class="fa fa-check"></i><b>3.1.3</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.1.4" data-path="mlr.html"><a href="mlr.html#python-code-13"><i class="fa fa-check"></i><b>3.1.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#two-way-anova-with-interactions"><i class="fa fa-check"></i><b>3.2</b> Two-Way ANOVA with Interactions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#post-hoc-testing-1"><i class="fa fa-check"></i><b>3.2.1</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.2.2" data-path="mlr.html"><a href="mlr.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="mlr.html"><a href="mlr.html#python-code-14"><i class="fa fa-check"></i><b>3.2.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#randomized-block-design"><i class="fa fa-check"></i><b>3.3</b> Randomized Block Design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mlr.html"><a href="mlr.html#garlic-bulb-weight-example"><i class="fa fa-check"></i><b>3.3.1</b> Garlic Bulb Weight Example</a></li>
<li class="chapter" data-level="3.3.2" data-path="mlr.html"><a href="mlr.html#assumptions-1"><i class="fa fa-check"></i><b>3.3.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.3.3" data-path="mlr.html"><a href="mlr.html#python-code-15"><i class="fa fa-check"></i><b>3.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model Structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="mlr.html"><a href="mlr.html#global-local-inference"><i class="fa fa-check"></i><b>3.4.2</b> Global &amp; Local Inference</a></li>
<li class="chapter" data-level="3.4.3" data-path="mlr.html"><a href="mlr.html#assumptions-2"><i class="fa fa-check"></i><b>3.4.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.4.4" data-path="mlr.html"><a href="mlr.html#multiple-coefficients-of-determination"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Coefficients of Determination</a></li>
<li class="chapter" data-level="3.4.5" data-path="mlr.html"><a href="mlr.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>3.4.5</b> Categorical Predictor Variables</a></li>
<li class="chapter" data-level="3.4.6" data-path="mlr.html"><a href="mlr.html#python-code-16"><i class="fa fa-check"></i><b>3.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>4</b> Model Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-selection.html"><a href="model-selection.html#selection-criteria"><i class="fa fa-check"></i><b>4.1</b> Selection Criteria</a></li>
<li class="chapter" data-level="4.2" data-path="model-selection.html"><a href="model-selection.html#stepwise-selection"><i class="fa fa-check"></i><b>4.2</b> Stepwise Selection</a>
<ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward"><i class="fa fa-check"></i>Backward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#stepwise"><i class="fa fa-check"></i>Stepwise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-selection.html"><a href="model-selection.html#significance-levels"><i class="fa fa-check"></i><b>4.3</b> Significance Levels</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-selection.html"><a href="model-selection.html#python-code-17"><i class="fa fa-check"></i><b>4.3.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>5</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="diag.html"><a href="diag.html#python-code-18"><i class="fa fa-check"></i><b>5.0.1</b> Python Code</a></li>
<li class="chapter" data-level="5.1" data-path="diag.html"><a href="diag.html#examining-residuals"><i class="fa fa-check"></i><b>5.1</b> Examining Residuals</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="diag.html"><a href="diag.html#python-code-19"><i class="fa fa-check"></i><b>5.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="diag.html"><a href="diag.html#misspecified-model"><i class="fa fa-check"></i><b>5.2</b> Misspecified Model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="diag.html"><a href="diag.html#python-code-20"><i class="fa fa-check"></i><b>5.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="diag.html"><a href="diag.html#constant-variance"><i class="fa fa-check"></i><b>5.3</b> Constant Variance</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="diag.html"><a href="diag.html#python-code-21"><i class="fa fa-check"></i><b>5.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="diag.html"><a href="diag.html#normality"><i class="fa fa-check"></i><b>5.4</b> Normality</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="diag.html"><a href="diag.html#python-code-22"><i class="fa fa-check"></i><b>5.4.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="diag.html"><a href="diag.html#correlated-errors"><i class="fa fa-check"></i><b>5.5</b> Correlated Errors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="diag.html"><a href="diag.html#python-code-23"><i class="fa fa-check"></i><b>5.5.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="diag.html"><a href="diag.html#influential-observations-and-outliers"><i class="fa fa-check"></i><b>5.6</b> Influential Observations and Outliers</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="diag.html"><a href="diag.html#python-code-24"><i class="fa fa-check"></i><b>5.6.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="diag.html"><a href="diag.html#multicollinearity"><i class="fa fa-check"></i><b>5.7</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="diag.html"><a href="diag.html#python-code-25"><i class="fa fa-check"></i><b>5.7.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html"><i class="fa fa-check"></i><b>6</b> Model Building and Scoring for Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#regularized-regression"><i class="fa fa-check"></i><b>6.1</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#penalties-in-models"><i class="fa fa-check"></i><b>6.1.1</b> Penalties in Models</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#lasso"><i class="fa fa-check"></i><b>6.1.3</b> LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="6.1.5" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-26"><i class="fa fa-check"></i><b>6.1.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#optimizing-penalties"><i class="fa fa-check"></i><b>6.2</b> Optimizing Penalties</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cross-validation"><i class="fa fa-check"></i><b>6.2.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cv-in-regularized-regression"><i class="fa fa-check"></i><b>6.2.2</b> CV in Regularized Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-comparisons"><i class="fa fa-check"></i><b>6.3</b> Model Comparisons</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-metrics"><i class="fa fa-check"></i><b>6.3.1</b> Model Metrics</a></li>
<li class="chapter" data-level="6.3.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#test-dataset-comparison"><i class="fa fa-check"></i><b>6.3.2</b> Test Dataset Comparison</a></li>
<li class="chapter" data-level="6.3.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-27"><i class="fa fa-check"></i><b>6.3.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#describing-categorical-data"><i class="fa fa-check"></i><b>7.1</b> Describing Categorical Data</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-28"><i class="fa fa-check"></i><b>7.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#tests-of-association"><i class="fa fa-check"></i><b>7.2</b> Tests of Association</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-29"><i class="fa fa-check"></i><b>7.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#measures-of-association"><i class="fa fa-check"></i><b>7.3</b> Measures of Association</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-30"><i class="fa fa-check"></i><b>7.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Introduction to Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#linear-probability-model"><i class="fa fa-check"></i><b>7.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.4.2</b> Binary Logistic Regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#adding-categorical-variables"><i class="fa fa-check"></i><b>7.4.3</b> Adding Categorical Variables</a></li>
<li class="chapter" data-level="7.4.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#model-assessment"><i class="fa fa-check"></i><b>7.4.4</b> Model Assessment</a></li>
<li class="chapter" data-level="7.4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#variable-selection-and-regularized-regression"><i class="fa fa-check"></i><b>7.4.5</b> Variable Selection and Regularized Regression</a></li>
<li class="chapter" data-level="7.4.6" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-31"><i class="fa fa-check"></i><b>7.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mlr" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> Complex ANOVA and Multiple Linear Regression<a href="mlr.html#mlr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the One-Way ANOVA and simple linear regression models, there was only one variable - categorical for ANOVA and continuous for simple linear regression - to explain and predict our target variable. Rarely do we believe that only a single variable will suffice in predicting a variable of interest. Here in this Chapter we will generalize these models to the <span class="math inline">\(n\)</span>-Way ANOVA and multiple linear regression models. These models contain multiple sets of variables to explain and predict our target variable.</p>
<p>This Chapter aims to answer the following questions:</p>
<ul>
<li>How do we include multiple variables in ANOVA?
<ul>
<li>Exploration</li>
<li>Assumptions</li>
<li>Predictions</li>
</ul></li>
<li>What is an interaction between two predictor variables?
<ul>
<li>Interpretation</li>
<li>Evaluation</li>
<li>Within Category Effects</li>
</ul></li>
<li>What is blocking in ANOVA?
<ul>
<li>Nuisance Factors</li>
<li>Differences Between Blocking and Two-Way ANOVA</li>
</ul></li>
<li>How do we include multiple variables in regression?
<ul>
<li>Model Structure</li>
<li>Global &amp; Local Inference</li>
<li>Assumptions</li>
<li>Adjusted <span class="math inline">\(R^2\)</span></li>
<li>Categorical Variables in Regression</li>
</ul></li>
</ul>
<div id="two-way-anova" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Two-Way ANOVA<a href="mlr.html#two-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the One-Way ANOVA model, we used a single categorical predictor variable with <span class="math inline">\(k\)</span> levels to predict our continuous target variable. Now we will generalize this model to include <span class="math inline">\(n\)</span> categorical variables that each have different numbers of levels (<span class="math inline">\(k_1, k_2, ..., k_n\)</span>).</p>
<div id="exploration" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Exploration<a href="mlr.html#exploration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s use the basic example of two categorical predictor variables in a Two-Way ANOVA. Previously, we talked about using heating quality as a factor to explain and predict sale price of homes in Ames, Iowa. Now, we also consider whether the home has central air. Although similar in nature, these two factors potentially provide important, unique pieces of information about the home. Similar to previous data science problems, let us first explore our variables and their potential relationships.</p>
<p>Now that we have two variables that we will use to explain and predict sale price, here are some summary statistics (mean, standard deviation, minimum, and maximum) for each combination of category. We will use the <code>group_by</code> function on both predictor variables of interest to split the data and then the <code>summarise</code> function to calculate the metrics we are interested in.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="mlr.html#cb162-1" aria-hidden="true" tabindex="-1"></a>train <span class="sc">%&gt;%</span> </span>
<span id="cb162-2"><a href="mlr.html#cb162-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Heating_QC, Central_Air) <span class="sc">%&gt;%</span></span>
<span id="cb162-3"><a href="mlr.html#cb162-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price), </span>
<span id="cb162-4"><a href="mlr.html#cb162-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price), </span>
<span id="cb162-5"><a href="mlr.html#cb162-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">max =</span> <span class="fu">max</span>(Sale_Price), </span>
<span id="cb162-6"><a href="mlr.html#cb162-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">min =</span> <span class="fu">min</span>(Sale_Price))</span></code></pre></div>
<pre><code>## `summarise()` has grouped output by &#39;Heating_QC&#39;. You can
## override using the `.groups` argument.</code></pre>
<pre><code>## # A tibble: 10 × 6
## # Groups:   Heating_QC [5]
##    Heating_QC Central_Air    mean     sd    max    min
##    &lt;ord&gt;      &lt;fct&gt;         &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt;
##  1 Poor       N            50050  52255.  87000  13100
##  2 Poor       Y           107000     NA  107000 107000
##  3 Fair       N            84748. 28267. 158000  37900
##  4 Fair       Y           145165. 38624. 230000  50000
##  5 Typical    N           103469. 34663. 209500  12789
##  6 Typical    Y           142003. 39657. 375000  60000
##  7 Good       N           110811. 38455. 214500  59000
##  8 Good       Y           160113. 54158. 415000  52000
##  9 Excellent  N           115062. 33271. 184900  64000
## 10 Excellent  Y           216401. 88518. 745000  58500</code></pre>
<p>We can already see above that there appears to be some differences in average sale price across the categories overall. Within each grouping of heating quality, homes with central air appear to have a higher sale price than homes without. Also, similar to before, homes with higher heating quality appear to have higher sale prices compared to homes with lower heating quality.</p>
<p>We also see these relationships in the bar chart in Figure <a href="mlr.html#fig:twomeans">3.1</a>.</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="mlr.html#cb165-1" aria-hidden="true" tabindex="-1"></a>CA_heat <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span> </span>
<span id="cb165-2"><a href="mlr.html#cb165-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Heating_QC, Central_Air) <span class="sc">%&gt;%</span></span>
<span id="cb165-3"><a href="mlr.html#cb165-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">summarise</span>(<span class="at">mean =</span> <span class="fu">mean</span>(Sale_Price), </span>
<span id="cb165-4"><a href="mlr.html#cb165-4" aria-hidden="true" tabindex="-1"></a>            <span class="at">sd =</span> <span class="fu">sd</span>(Sale_Price), </span>
<span id="cb165-5"><a href="mlr.html#cb165-5" aria-hidden="true" tabindex="-1"></a>            <span class="at">max =</span> <span class="fu">max</span>(Sale_Price), </span>
<span id="cb165-6"><a href="mlr.html#cb165-6" aria-hidden="true" tabindex="-1"></a>            <span class="at">min =</span> <span class="fu">min</span>(Sale_Price))</span></code></pre></div>
<pre><code>## `summarise()` has grouped output by &#39;Heating_QC&#39;. You can
## override using the `.groups` argument.</code></pre>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="mlr.html#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> CA_heat, <span class="fu">aes</span>(<span class="at">x =</span> Heating_QC, <span class="at">y =</span> mean<span class="sc">/</span><span class="dv">1000</span>, <span class="at">fill =</span> Central_Air)) <span class="sc">+</span></span>
<span id="cb167-2"><a href="mlr.html#cb167-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">position =</span> <span class="fu">position_dodge</span>()) <span class="sc">+</span></span>
<span id="cb167-3"><a href="mlr.html#cb167-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Heating Quality Category&quot;</span>) <span class="sc">+</span></span>
<span id="cb167-4"><a href="mlr.html#cb167-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Paired&quot;</span>) <span class="sc">+</span></span>
<span id="cb167-5"><a href="mlr.html#cb167-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twomeans"></span>
<img src="bookdownproj_files/figure-html/twomeans-1.png" alt="Distribution of Variables Heating_QC and Central_Air" width="672" />
<p class="caption">
Figure 3.1: Distribution of Variables Heating_QC and Central_Air
</p>
</div>
<p>As before, visually looking at bar charts and mean calculations only goes so far. We need to statistically be sure of any differences that exist between average sale price in categories.</p>
</div>
<div id="model" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Model<a href="mlr.html#model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are going to do this with the same approach as in the One-Way ANOVA, just with more variables as shown in the following equation:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + \varepsilon_{ijk}
\]</span>
where <span class="math inline">\(\mu\)</span> is the average baseline sales price of a home in Ames, Iowa, <span class="math inline">\(\alpha_i\)</span> is the variable representing the impacts of the levels of heating quality, and <span class="math inline">\(\beta_j\)</span> is the variable representing the impacts of the levels of central air. As mentioned previously, the unexplained error in this model is represented as <span class="math inline">\(\varepsilon_{ijk}\)</span>.</p>
<p>The same F test approach is also used, just for each one of the variables. Each variable’s test has a null hypothesis assuming all categories have the same mean. The alternative for each test is that at least one category’s mean is different.</p>
<p>Let’s view the results of the <code>aov</code> function.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="mlr.html#cb168-1" aria-hidden="true" tabindex="-1"></a>ames_aov2 <span class="ot">&lt;-</span> <span class="fu">aov</span>(Sale_Price <span class="sc">~</span> Heating_QC <span class="sc">+</span> Central_Air, <span class="at">data =</span> train)</span>
<span id="cb168-2"><a href="mlr.html#cb168-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-3"><a href="mlr.html#cb168-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_aov2)</span></code></pre></div>
<pre><code>##               Df    Sum Sq   Mean Sq F value   Pr(&gt;F)    
## Heating_QC     4 2.891e+12 7.228e+11  147.60  &lt; 2e-16 ***
## Central_Air    1 2.903e+11 2.903e+11   59.28 2.11e-14 ***
## Residuals   2045 1.002e+13 4.897e+09                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From the above results, we have low p-values for each of the variables’ F test. Heating quality had 4 degrees of freedom, derived from the 5 categories <span class="math inline">\((4 = 5-1)\)</span>. Similarly, central air’s 2 categories produce 1 <span class="math inline">\((= 2-1)\)</span> degree of freedom. The F values are calculated the exact same way as described before with the mean square for each variable divided by the mean square error. Based on these tests, at least one category in each variable is statistically different than the rest.</p>
</div>
<div id="post-hoc-testing" class="section level3 hasAnchor" number="3.1.3">
<h3><span class="header-section-number">3.1.3</span> Post-Hoc Testing<a href="mlr.html#post-hoc-testing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As with the One-Way ANOVA, the next logical question is which of these categories is different. We will use the same post-hoc tests as before with the <code>TukeyHSD</code> function.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="mlr.html#cb170-1" aria-hidden="true" tabindex="-1"></a>tukey.ames2 <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(ames_aov2)</span>
<span id="cb170-2"><a href="mlr.html#cb170-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tukey.ames2)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Sale_Price ~ Heating_QC + Central_Air, data = train)
## 
## $Heating_QC
##                        diff        lwr       upr     p adj
## Fair-Poor          49176.42 -63650.448 162003.29 0.7571980
## Typical-Poor       67781.01 -42800.320 178362.35 0.4506761
## Good-Poor          87753.89 -23040.253 198548.03 0.1945181
## Excellent-Poor    146288.89  35818.859 256758.92 0.0028361
## Typical-Fair       18604.59  -6326.425  43535.61 0.2484556
## Good-Fair          38577.47  12718.894  64436.04 0.0004622
## Excellent-Fair     97112.47  72679.867 121545.07 0.0000000
## Good-Typical       19972.87   7050.230  32895.52 0.0002470
## Excellent-Typical  78507.88  68746.678  88269.07 0.0000000
## Excellent-Good     58535.00  46602.229  70467.78 0.0000000
## 
## $Central_Air
##         diff      lwr      upr p adj
## Y-N 43256.57 31508.27 55004.87     0</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="mlr.html#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tukey.ames2, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-92-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-92-2.png" width="672" /></p>
<p>Starting with the variable for central air, we can see there is a statistical difference between the two categories. This is the exact same result as the overall F test for the variable since there are only two categories. For the heating quality variable, we can see some categories are different from each other, while others are not. Noticeably, the combination of poor with fair, good, and typical categories are <strong>not</strong> statistically different. Notice also the different widths of these confidence intervals do to the different combinations of sample sizes for the categories being tested.</p>
</div>
<div id="python-code-13" class="section level3 hasAnchor" number="3.1.4">
<h3><span class="header-section-number">3.1.4</span> Python Code<a href="mlr.html#python-code-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two-way ANOVA model</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb173-1"><a href="mlr.html#cb173-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb173-2"><a href="mlr.html#cb173-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-3"><a href="mlr.html#cb173-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC) + C(Central_Air)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb173-4"><a href="mlr.html#cb173-4" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb173-5"><a href="mlr.html#cb173-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-6"><a href="mlr.html#cb173-6" aria-hidden="true" tabindex="-1"></a>sm.api.stats.anova_lm(model, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-93-1.png" width="672" /></p>
<p>Post-hoc testing</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb174-1"><a href="mlr.html#cb174-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.stats.multicomp <span class="im">import</span> pairwise_tukeyhsd</span>
<span id="cb174-2"><a href="mlr.html#cb174-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-3"><a href="mlr.html#cb174-3" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;anova_combo&#39;</span>] <span class="op">=</span> train.Heating_QC.astype(<span class="st">&#39;string&#39;</span>) <span class="op">+</span> <span class="st">&quot; / &quot;</span> <span class="op">+</span> train.Central_Air</span>
<span id="cb174-4"><a href="mlr.html#cb174-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-5"><a href="mlr.html#cb174-5" aria-hidden="true" tabindex="-1"></a>mcomp <span class="op">=</span> pairwise_tukeyhsd(endog <span class="op">=</span> train[<span class="st">&#39;Sale_Price&#39;</span>], groups <span class="op">=</span> train[<span class="st">&#39;anova_combo&#39;</span>], alpha <span class="op">=</span> <span class="fl">0.05</span>)</span>
<span id="cb174-6"><a href="mlr.html#cb174-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-7"><a href="mlr.html#cb174-7" aria-hidden="true" tabindex="-1"></a>mcomp.summary()</span>
<span id="cb174-8"><a href="mlr.html#cb174-8" aria-hidden="true" tabindex="-1"></a>mcomp.plot_simultaneous()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-94-3.png" width="960" /></p>
</div>
</div>
<div id="two-way-anova-with-interactions" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Two-Way ANOVA with Interactions<a href="mlr.html#two-way-anova-with-interactions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>What if the relationship between a predictor and target variable changed depending on the value of another predictor variable? For our example, we would say that the average difference in sales price between having central air and not having central changed depending on what level of heating quality the home had. In the bar chart in Figure <a href="mlr.html#fig:twomeans">3.1</a>, a potential interaction effect is displayed when the differences between the two bars within each heating category is different across heating category. If the difference, was the same, then there is no interaction present. In other words, no matter the heating quality rating of the home, the average difference in sales price between having central air and not having central air is the same.</p>
<p>This interaction model is represented as follows:</p>
<p><span class="math display">\[
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \varepsilon_{ijk}
\]</span></p>
<p>with the interaction effect, <span class="math inline">\((\alpha \beta)_{ij}\)</span>, as the multiplication of the two variables involved in the interaction. Interactions can occur between more than two variables as well. Interactions are good to evaluate as they can mask the effects of individual variables. For example, imagine a hypothetical example as shown in Figure <a href="mlr.html#fig:twomeansint">3.2</a> where the impact of having central air is opposite depending on which category of heating quality you have.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="mlr.html#cb175-1" aria-hidden="true" tabindex="-1"></a>fake_HQ <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;Poor&quot;</span>, <span class="st">&quot;Poor&quot;</span>, <span class="st">&quot;Excellent&quot;</span>, <span class="st">&quot;Excellent&quot;</span>)</span>
<span id="cb175-2"><a href="mlr.html#cb175-2" aria-hidden="true" tabindex="-1"></a>fake_CA <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;N&quot;</span>, <span class="st">&quot;Y&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;Y&quot;</span>)</span>
<span id="cb175-3"><a href="mlr.html#cb175-3" aria-hidden="true" tabindex="-1"></a>fake_mean <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">100</span>, <span class="dv">150</span>, <span class="dv">150</span>, <span class="dv">100</span>)</span>
<span id="cb175-4"><a href="mlr.html#cb175-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb175-5"><a href="mlr.html#cb175-5" aria-hidden="true" tabindex="-1"></a>fake_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(fake_HQ, fake_CA, fake_mean))</span></code></pre></div>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="mlr.html#cb176-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> fake_df, <span class="fu">aes</span>(<span class="at">x =</span> fake_HQ, <span class="at">y =</span> <span class="fu">as.numeric</span>(fake_mean), <span class="at">fill =</span> fake_CA)) <span class="sc">+</span></span>
<span id="cb176-2"><a href="mlr.html#cb176-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>, <span class="at">position =</span> <span class="fu">position_dodge</span>()) <span class="sc">+</span></span>
<span id="cb176-3"><a href="mlr.html#cb176-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Fake Sales Price (Thousands $)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Fake Heating Quality Category&quot;</span>) <span class="sc">+</span></span>
<span id="cb176-4"><a href="mlr.html#cb176-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette =</span> <span class="st">&quot;Paired&quot;</span>) <span class="sc">+</span></span>
<span id="cb176-5"><a href="mlr.html#cb176-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twomeansint"></span>
<img src="bookdownproj_files/figure-html/twomeansint-1.png" alt="Distribution of Variables Heating_QC and Central_Air" width="672" />
<p class="caption">
Figure 3.2: Distribution of Variables Heating_QC and Central_Air
</p>
</div>
<p>If you were to only look at the average sales price across heating quality, you would notice no difference between the two groups (average for both heating categories is 125). However, when the interaction is accounted for, you can clearly see in the bar heights that sales price is different depending on the value of central air.</p>
<p>Let’s evaluate the interaction term in our actual data. To do so, we just incorporate the multiplication of the two variables in the model statement by using the formula <code>Sale_Price ~ Heating_QC + Central_Air + Heating_QC:Central_Air</code>. You could also use the shorthand version of this by using the formula <code>Sale_Price ~ Heating_QC*Central_Air</code>. The <code>*</code> will include both main effects (the individual variables) and the interaction between them.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="mlr.html#cb177-1" aria-hidden="true" tabindex="-1"></a>ames_aov_int <span class="ot">&lt;-</span> <span class="fu">aov</span>(Sale_Price <span class="sc">~</span> Heating_QC<span class="sc">*</span>Central_Air, <span class="at">data =</span> train)</span>
<span id="cb177-2"><a href="mlr.html#cb177-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-3"><a href="mlr.html#cb177-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_aov_int)</span></code></pre></div>
<pre><code>##                          Df    Sum Sq   Mean Sq F value   Pr(&gt;F)
## Heating_QC                4 2.891e+12 7.228e+11 147.897  &lt; 2e-16
## Central_Air               1 2.903e+11 2.903e+11  59.403 1.99e-14
## Heating_QC:Central_Air    4 3.972e+10 9.930e+09   2.032   0.0875
## Residuals              2041 9.975e+12 4.887e+09                 
##                           
## Heating_QC             ***
## Central_Air            ***
## Heating_QC:Central_Air .  
## Residuals                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>As seen by the output above, the interaction effect between heating quality and central air is <strong>not</strong> significant at the 0.05 level. Again, this implies that the average difference in sale price of the home between having central air and not does not differ depending on which category of heating quality the home belongs to. If our interaction was significant (say a 0.02 p-value instead) then we would keep it in our model, but here we would remove the interaction term from our model and rerun the analysis.</p>
<div id="post-hoc-testing-1" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Post-Hoc Testing<a href="mlr.html#post-hoc-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Post-hoc tests are also available for interaction models as well to determine where the statistical differences exist in all the combinations of possible categories. We evaluate these post-hoc tests using the same <code>TukeyHSD</code> function and its corresponding <code>plot</code> element.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="mlr.html#cb179-1" aria-hidden="true" tabindex="-1"></a>tukey.ames_int <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(ames_aov_int)</span>
<span id="cb179-2"><a href="mlr.html#cb179-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tukey.ames_int)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = Sale_Price ~ Heating_QC * Central_Air, data = train)
## 
## $Heating_QC
##                        diff        lwr       upr     p adj
## Fair-Poor          49176.42 -63536.973 161889.81 0.7565086
## Typical-Poor       67781.01 -42689.104 178251.13 0.4496163
## Good-Poor          87753.89 -22928.822 198436.60 0.1936558
## Excellent-Poor    146288.89  35929.963 256647.82 0.0027979
## Typical-Fair       18604.59  -6301.351  43510.54 0.2474998
## Good-Fair          38577.47  12744.901  64410.03 0.0004543
## Excellent-Fair     97112.47  72704.440 121520.50 0.0000000
## Good-Typical       19972.87   7063.227  32882.52 0.0002425
## Excellent-Typical  78507.88  68756.495  88259.26 0.0000000
## Excellent-Good     58535.00  46614.230  70455.77 0.0000000
## 
## $Central_Air
##         diff      lwr      upr p adj
## Y-N 43256.57 31520.09 54993.05     0
## 
## $`Heating_QC:Central_Air`
##                               diff         lwr       upr
## Fair:N-Poor:N            34698.276 -127178.615 196575.17
## Typical:N-Poor:N         53419.220 -105046.645 211885.08
## Good:N-Poor:N            60760.870 -102472.555 223994.29
## Excellent:N-Poor:N       65011.727 -105195.635 235219.09
## Poor:Y-Poor:N            56950.000 -214233.733 328133.73
## Fair:Y-Poor:N            95114.833  -65743.496 255973.16
## Typical:Y-Poor:N         91952.772  -64912.040 248817.58
## Good:Y-Poor:N           110062.553  -46997.028 267122.13
## Excellent:Y-Poor:N      166351.347    9630.224 323072.47
## Typical:N-Fair:N         18720.944  -29117.117  66559.00
## Good:N-Fair:N            26062.594  -35761.358  87886.55
## Excellent:N-Fair:N       30313.451  -48093.155 108720.06
## Poor:Y-Fair:N            22251.724 -202954.108 247457.56
## Fair:Y-Fair:N            60416.557    5167.556 115665.56
## Typical:Y-Fair:N         57254.496   15021.578  99487.41
## Good:Y-Fair:N            75364.278   32413.584 118314.97
## Excellent:Y-Fair:N      131653.071   89957.021 173349.12
## Good:N-Typical:N          7341.650  -44902.998  59586.30
## Excellent:N-Typical:N    11592.508  -59505.300  82690.32
## Poor:Y-Typical:N          3530.780 -219235.844 226297.41
## Fair:Y-Typical:N         41695.614   -2573.500  85964.73
## Typical:Y-Typical:N      38533.553   12248.163  64818.94
## Good:Y-Typical:N         56643.334   29219.541  84067.13
## Excellent:Y-Typical:N   112932.128   87518.295 138345.96
## Excellent:N-Good:N        4250.858  -76919.452  85421.17
## Poor:Y-Good:N            -3810.870 -229993.738 222372.00
## Fair:Y-Good:N            34353.964  -24751.665  93459.59
## Typical:Y-Good:N         31191.903  -15974.214  78358.02
## Good:Y-Good:N            49301.684    1491.797  97111.57
## Excellent:Y-Good:N      105590.478   58904.465 152276.49
## Poor:Y-Excellent:N       -8061.727 -239327.991 223204.54
## Fair:Y-Excellent:N       30103.106  -46178.414 106384.63
## Typical:Y-Excellent:N    26941.045  -40512.921  94395.01
## Good:Y-Excellent:N       45050.826  -22854.846 112956.50
## Excellent:Y-Excellent:N 101339.620   34220.481 168458.76
## Fair:Y-Poor:Y            38164.833 -186309.978 262639.65
## Typical:Y-Poor:Y         35002.772 -186627.795 256633.34
## Good:Y-Poor:Y            53112.553 -168655.909 274881.02
## Excellent:Y-Poor:Y      109401.347 -112127.544 330930.24
## Typical:Y-Fair:Y         -3162.061  -41305.131  34981.01
## Good:Y-Fair:Y            14947.720  -23988.593  53884.03
## Excellent:Y-Fair:Y       71236.514   33688.745 108784.28
## Good:Y-Typical:Y         18109.781    2387.068  33832.49
## Excellent:Y-Typical:Y    74398.575   62524.140  86273.01
## Excellent:Y-Good:Y       56288.794   42071.027  70506.56
##                             p adj
## Fair:N-Poor:N           0.9996249
## Typical:N-Poor:N        0.9876643
## Good:N-Poor:N           0.9755371
## Excellent:N-Poor:N      0.9709555
## Poor:Y-Poor:N           0.9996829
## Fair:Y-Poor:N           0.6876708
## Typical:Y-Poor:N        0.6985070
## Good:Y-Poor:N           0.4435353
## Excellent:Y-Poor:N      0.0271785
## Typical:N-Fair:N        0.9659507
## Good:N-Fair:N           0.9454988
## Excellent:N-Fair:N      0.9685428
## Poor:Y-Fair:N           0.9999995
## Fair:Y-Fair:N           0.0193847
## Typical:Y-Fair:N        0.0007697
## Good:Y-Fair:N           0.0000014
## Excellent:Y-Fair:N      0.0000000
## Good:N-Typical:N        0.9999894
## Excellent:N-Typical:N   0.9999620
## Poor:Y-Typical:N        1.0000000
## Fair:Y-Typical:N        0.0848888
## Typical:Y-Typical:N     0.0001576
## Good:Y-Typical:N        0.0000000
## Excellent:Y-Typical:N   0.0000000
## Excellent:N-Good:N      1.0000000
## Poor:Y-Good:N           1.0000000
## Fair:Y-Good:N           0.7089196
## Typical:Y-Good:N        0.5315494
## Good:Y-Good:N           0.0369201
## Excellent:Y-Good:N      0.0000000
## Poor:Y-Excellent:N      1.0000000
## Fair:Y-Excellent:N      0.9640522
## Typical:Y-Excellent:N   0.9611660
## Good:Y-Excellent:N      0.5267698
## Excellent:Y-Excellent:N 0.0000809
## Fair:Y-Poor:Y           0.9999458
## Typical:Y-Poor:Y        0.9999711
## Good:Y-Poor:Y           0.9990789
## Excellent:Y-Poor:Y      0.8652766
## Typical:Y-Fair:Y        0.9999999
## Good:Y-Fair:Y           0.9699661
## Excellent:Y-Fair:Y      0.0000001
## Good:Y-Typical:Y        0.0101482
## Excellent:Y-Typical:Y   0.0000000
## Excellent:Y-Good:Y      0.0000000</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="mlr.html#cb181-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tukey.ames_int, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-97-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-97-2.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-97-3.png" width="672" /></p>
<p>In the giant table above as well as the confidence interval plots, you are able to inspect which combination of categories are statistically different.</p>
<p>With interactions present in ANOVA models, post-hoc tests might get overwhelming in trying to find where differences exist. To help guide the exploration of post-hoc tests with interactions, we can do <strong>slicing</strong>. Slicing is when you perform One-Way ANOVA on subsets of data within categories of other variables. Even though the interaction in our model was not significant, let’s imagine that it was for the sake of example. For example, to help discover differences in the interaction between central air and heating quality, we could subset the data into two groups - homes with central air and homes without. Within these two groups we perform One-Way ANOVA across heating quality to find where differences might exist within subgroups.</p>
<p>This can easily be done with the <code>group_by</code> function to subset the data. The <code>nest</code> and <code>mutate</code> functions are also used to applied the <code>aov</code> function to each subgroup. Here we run a One-Way ANOVA for heating quality within each subset of central air being present or not.</p>
<div class="sourceCode" id="cb182"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb182-1"><a href="mlr.html#cb182-1" aria-hidden="true" tabindex="-1"></a>CA_aov <span class="ot">&lt;-</span> train <span class="sc">%&gt;%</span> </span>
<span id="cb182-2"><a href="mlr.html#cb182-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Central_Air) <span class="sc">%&gt;%</span></span>
<span id="cb182-3"><a href="mlr.html#cb182-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">nest</span>() <span class="sc">%&gt;%</span></span>
<span id="cb182-4"><a href="mlr.html#cb182-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">aov =</span> <span class="fu">map</span>(data, <span class="sc">~</span><span class="fu">summary</span>(<span class="fu">aov</span>(Sale_Price <span class="sc">~</span> Heating_QC, <span class="at">data =</span> .x))))</span>
<span id="cb182-5"><a href="mlr.html#cb182-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb182-6"><a href="mlr.html#cb182-6" aria-hidden="true" tabindex="-1"></a>CA_aov</span></code></pre></div>
<pre><code>## # A tibble: 2 × 3
## # Groups:   Central_Air [2]
##   Central_Air data                  aov       
##   &lt;fct&gt;       &lt;list&gt;                &lt;list&gt;    
## 1 Y           &lt;tibble [1,904 × 83]&gt; &lt;summry.v&gt;
## 2 N           &lt;tibble [147 × 83]&gt;   &lt;summry.v&gt;</code></pre>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="mlr.html#cb184-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(CA_aov<span class="sc">$</span>aov)</span></code></pre></div>
<pre><code>## [[1]]
##               Df    Sum Sq   Mean Sq F value Pr(&gt;F)    
## Heating_QC     4 2.242e+12 5.606e+11   108.5 &lt;2e-16 ***
## Residuals   1899 9.809e+12 5.165e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## [[2]]
##              Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   
## Heating_QC    4 1.774e+10 4.435e+09   3.793 0.00582 **
## Residuals   142 1.660e+11 1.169e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can see that both of these results are significant at the 0.05 level. This implies that there are statistical differences in average sale price across heating quality within homes that have central air as well as those that don’t have central air.</p>
</div>
<div id="assumptions" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Assumptions<a href="mlr.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The assumptions for the <span class="math inline">\(n\)</span>-Way ANOVA are the same as with the One-Way ANOVA - independence of observations, normality for each category of variable, and equal variances. With the inclusion of two or more variables (<span class="math inline">\(n\)</span>-Way ANOVA with <span class="math inline">\(n &gt; 1\)</span>), these assumptions can be harder to evaluate and test. These assumptions are then applied to the residuals of the model.</p>
<p>For equal variances, we can still apply the Levene Test for equal variances using the same <code>leveneTest</code> function as in Chapter <a href="slr.html#slr">2</a>.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="mlr.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="fu">leveneTest</span>(Sale_Price <span class="sc">~</span> Heating_QC<span class="sc">*</span>Central_Air, <span class="at">data =</span> train)</span></code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##         Df F value    Pr(&gt;F)    
## group    9   24.17 &lt; 2.2e-16 ***
##       2041                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From this test, we can see that we <strong>do not</strong> meet our assumption of equal variance.</p>
<p>Let’s explore the normality assumption. Again, we will assume this normality on the random error component, <span class="math inline">\(\varepsilon_{ijk}\)</span>, of the ANOVA model. More details are found for diagnostic testing using the error component in Diagnostic chapter. We can check normality using the same approaches of the QQ-plot or statistical testing as in the section on EDA. Here we will use the <code>plot</code> function on the <code>aov</code> object. Specifically, we want the second plot which is why we have a <code>2</code> in the <code>plot</code> function option. We then use the <code>shapiro.test</code> function on the error component. The estimate of the error component is calculated using the <code>residuals</code> function.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="mlr.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ames_aov_int, <span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="mlr.html#cb189-1" aria-hidden="true" tabindex="-1"></a>ames_res <span class="ot">&lt;-</span> <span class="fu">residuals</span>(ames_aov_int)</span>
<span id="cb189-2"><a href="mlr.html#cb189-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb189-3"><a href="mlr.html#cb189-3" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="at">x =</span> ames_res)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  ames_res
## W = 0.8838, p-value &lt; 2.2e-16</code></pre>
<p>Neither of the normality or equal variance assumptions appear to be met here. This would be a good scenario to have a non-parametric approach. Unfortunately, the Kruskal-Wallis approach is not applicable to <span class="math inline">\(n\)</span>-Way ANOVA where <span class="math inline">\(n &gt; 1\)</span>. These approaches would need more non-parametric versions of multiple regression models to account for them.</p>
</div>
<div id="python-code-14" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Python Code<a href="mlr.html#python-code-14" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two-way ANOVA with interactions</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb191-1"><a href="mlr.html#cb191-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC)*C(Central_Air)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb191-2"><a href="mlr.html#cb191-2" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb191-3"><a href="mlr.html#cb191-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-4"><a href="mlr.html#cb191-4" aria-hidden="true" tabindex="-1"></a>sm.api.stats.anova_lm(model, typ<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb191-5"><a href="mlr.html#cb191-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb191-6"><a href="mlr.html#cb191-6" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;resid_anova_2way&#39;</span>] <span class="op">=</span> model.resid</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-101-1.png" width="960" /></p>
<p>Post-hoc testing</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb192-1"><a href="mlr.html#cb192-1" aria-hidden="true" tabindex="-1"></a>model_slice1 <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC)&quot;</span>, data <span class="op">=</span> train[train[<span class="st">&quot;Central_Air&quot;</span>] <span class="op">==</span> <span class="st">&#39;Y&#39;</span>]).fit()</span>
<span id="cb192-2"><a href="mlr.html#cb192-2" aria-hidden="true" tabindex="-1"></a>model_slice1.summary()</span>
<span id="cb192-3"><a href="mlr.html#cb192-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-4"><a href="mlr.html#cb192-4" aria-hidden="true" tabindex="-1"></a>sm.api.stats.anova_lm(model_slice1, typ<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb192-5"><a href="mlr.html#cb192-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-6"><a href="mlr.html#cb192-6" aria-hidden="true" tabindex="-1"></a>model_slice2 <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Heating_QC)&quot;</span>, data <span class="op">=</span> train[train[<span class="st">&quot;Central_Air&quot;</span>] <span class="op">==</span> <span class="st">&#39;N&#39;</span>]).fit()</span>
<span id="cb192-7"><a href="mlr.html#cb192-7" aria-hidden="true" tabindex="-1"></a>model_slice2.summary()</span>
<span id="cb192-8"><a href="mlr.html#cb192-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-9"><a href="mlr.html#cb192-9" aria-hidden="true" tabindex="-1"></a>sm.api.stats.anova_lm(model_slice2, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-102-3.png" width="960" /></p>
<p>Assumptions</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb193-1"><a href="mlr.html#cb193-1" aria-hidden="true" tabindex="-1"></a>sm.api.qqplot(train[<span class="st">&#39;resid_anova_2way&#39;</span>])</span>
<span id="cb193-2"><a href="mlr.html#cb193-2" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-103-5.png" width="672" /></p>
<div class="sourceCode" id="cb194"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb194-1"><a href="mlr.html#cb194-1" aria-hidden="true" tabindex="-1"></a>sp.stats.shapiro(model.resid)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-103-6.png" width="672" /></p>
</div>
</div>
<div id="randomized-block-design" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Randomized Block Design<a href="mlr.html#randomized-block-design" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are two typical groups of data analysis studies that are conducted. The first are observational/retrospective studies which are the typical data problems people try to solve. The primary characteristic of these analysis are looking at what already happened (retrospective) and potentially inferring those results to further data. These studies have little control over other factors contributing to the target of interest because data is collected after the fact.</p>
<p>The other type of data analysis study are controlled experiments. In these situations, you often want to look at the outcome measure prospectively. The focus of the controlled experiment is to control for other factors that might contribute to the target variable. By manipulating these factors of interest, one can more reasonably claim causation. We can more reasonably claim causation when random assignment is used to eliminate potential <strong>nuisance factors</strong>. Nuisance factors are variables that can potentially impact the target variable, but are not of interest in the analysis directly.</p>
<div id="garlic-bulb-weight-example" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Garlic Bulb Weight Example<a href="mlr.html#garlic-bulb-weight-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For this analysis we will use a new dataset. This dataset contains the average garlic bulb weight from different plots of land. We want to compare the effects of fertilizer on average bulb weight. However, different plots of land could have different levels of sun exposure, pH for the soil, and rain amounts. Since we cannot alter the pH of the soil easily, or control the sun and rain, we can use blocking to account for these nuisance factors. Each fertilizer was randomly applied in quadrants of 8 plots of land. These 8 plots have different values for sun exposure, pH, and rain amount. Therefore, if we only put one fertilizer on each plot, we would not know if the fertilizer was the reason the garlic crop grew or if it was one of the nuisance factors. Since we <strong>blocked</strong> these 8 plots and applied all four fertilizers in each we have essentially accounted for (or removed the effect of) the nuisance factors.</p>
<p>Let’s briefly explore this new dataset by looking at all 32 values using the <code>print</code> function.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="mlr.html#cb195-1" aria-hidden="true" tabindex="-1"></a>block <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="at">file =</span> <span class="st">&quot;garlic_block.csv&quot;</span>)</span></code></pre></div>
<pre><code>## Rows: 32 Columns: 6
## ── Column specification ──────────────────────────────────────────
## Delimiter: &quot;,&quot;
## dbl (6): Sector, Position, Fertilizer, BulbWt, Cloves, BedId
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="mlr.html#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(block, <span class="at">n =</span> <span class="dv">32</span>)</span></code></pre></div>
<pre><code>## # A tibble: 32 × 6
##    Sector Position Fertilizer BulbWt Cloves BedId
##     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
##  1      1        1          3  0.259   11.6 22961
##  2      1        2          4  0.207   12.6 23884
##  3      1        3          1  0.275   12.1 19642
##  4      1        4          2  0.245   12.1 20384
##  5      2        1          3  0.215   11.6 20303
##  6      2        2          4  0.170   12.7 21004
##  7      2        3          1  0.225   12.0 16117
##  8      2        4          2  0.168   11.9 19686
##  9      3        1          4  0.217   12.4 26527
## 10      3        2          3  0.226   11.7 23574
## # ℹ 22 more rows</code></pre>
<p>How do we account for this blocking in an ANOVA model context? This blocking ANOVA model is the exact same as the Two-Way ANOVA model. The variable that identifies which sector (block) an observation is in serves as another variable in the model. Think about this variable as the variable that accounts for all the nuisance factors in your ANOVA. That means we have two variables in this ANOVA model - fertilizer and sector (the block that accounts for sun exposure, pH level of soil, rain amount, etc.).</p>
<p>For this we can use the same <code>aov</code> function we described above.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="mlr.html#cb199-1" aria-hidden="true" tabindex="-1"></a>block_aov <span class="ot">&lt;-</span> <span class="fu">aov</span>(BulbWt <span class="sc">~</span> <span class="fu">factor</span>(Fertilizer) <span class="sc">+</span> <span class="fu">factor</span>(Sector), <span class="at">data =</span> block)</span>
<span id="cb199-2"><a href="mlr.html#cb199-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-3"><a href="mlr.html#cb199-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(block_aov)</span></code></pre></div>
<pre><code>##                    Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    
## factor(Fertilizer)  3 0.005086 0.0016954   4.307 0.016222 *  
## factor(Sector)      7 0.017986 0.0025695   6.527 0.000364 ***
## Residuals          21 0.008267 0.0003937                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Using the <code>summary</code> function we can see that both the sector (block) and fertilizer variables are significant in the model at the 0.05 level. What are the interpretations of this?
First, let’s address the blocking variable. Whether it is significant or not, it should <strong>always</strong> be included in the model. This is due to the fact that the data is structured in that way. It is a construct of the data that should be accounted for regardless of the significance. However, since the blocking variable (sector) was significant, that implies that different plots of land have different impacts of the average bulb weight of garlic. Again, this is most likely due to the differences between the plots of land - namely sun exposure, pH of soil, rain fall, etc.</p>
<p>Second, the variable of interest is the fertilizer variable. It is significant, implying that there is a difference in the average bulb weight of garlic for different fertilizers. To examine which fertilizer pairs are statistically difference we can use post-hos testing as described in the previous parts of this chapter using the <code>TukeyHSD</code> function.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="mlr.html#cb201-1" aria-hidden="true" tabindex="-1"></a>tukey.block <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(block_aov)</span>
<span id="cb201-2"><a href="mlr.html#cb201-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(tukey.block)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = BulbWt ~ factor(Fertilizer) + factor(Sector), data = block)
## 
## $`factor(Fertilizer)`
##            diff         lwr          upr     p adj
## 2-1 -0.02509875 -0.05275125  0.002553751 0.0840024
## 3-1 -0.01294875 -0.04060125  0.014703751 0.5698678
## 4-1 -0.03336125 -0.06101375 -0.005708749 0.0144260
## 3-2  0.01215000 -0.01550250  0.039802501 0.6186232
## 4-2 -0.00826250 -0.03591500  0.019390001 0.8382800
## 4-3 -0.02041250 -0.04806500  0.007240001 0.1995492
## 
## $`factor(Sector)`
##           diff          lwr           upr     p adj
## 2-1 -0.0520675 -0.099126544 -5.008456e-03 0.0234315
## 3-1 -0.0145075 -0.061566544  3.255154e-02 0.9634255
## 4-1 -0.0450550 -0.092114044  2.004044e-03 0.0669646
## 5-1 -0.0616250 -0.108684044 -1.456596e-02 0.0051483
## 6-1 -0.0196650 -0.066724044  2.739404e-02 0.8466335
## 7-1  0.0084950 -0.038564044  5.555404e-02 0.9984089
## 8-1 -0.0393325 -0.086391544  7.726544e-03 0.1469768
## 3-2  0.0375600 -0.009499044  8.461904e-02 0.1841786
## 4-2  0.0070125 -0.040046544  5.407154e-02 0.9995370
## 5-2 -0.0095575 -0.056616544  3.750154e-02 0.9966777
## 6-2  0.0324025 -0.014656544  7.946154e-02 0.3337758
## 7-2  0.0605625  0.013503456  1.076215e-01 0.0061094
## 8-2  0.0127350 -0.034324044  5.979404e-02 0.9819446
## 4-3 -0.0305475 -0.077606544  1.651154e-02 0.4025951
## 5-3 -0.0471175 -0.094176544 -5.845586e-05 0.0495704
## 6-3 -0.0051575 -0.052216544  4.190154e-02 0.9999400
## 7-3  0.0230025 -0.024056544  7.006154e-02 0.7227812
## 8-3 -0.0248250 -0.071884044  2.223404e-02 0.6454690
## 5-4 -0.0165700 -0.063629044  3.048904e-02 0.9286987
## 6-4  0.0253900 -0.021669044  7.244904e-02 0.6208608
## 7-4  0.0535500  0.006490956  1.006090e-01 0.0186102
## 8-4  0.0057225 -0.041336544  5.278154e-02 0.9998793
## 6-5  0.0419600 -0.005099044  8.901904e-02 0.1034664
## 7-5  0.0701200  0.023060956  1.171790e-01 0.0012997
## 8-5  0.0222925 -0.024766544  6.935154e-02 0.7514897
## 7-6  0.0281600 -0.018899044  7.521904e-02 0.5004099
## 8-6 -0.0196675 -0.066726544  2.739154e-02 0.8465530
## 8-7 -0.0478275 -0.094886544 -7.684559e-04 0.0446174</code></pre>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="mlr.html#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(tukey.block, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-106-1.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-106-2.png" width="672" /></p>
</div>
<div id="assumptions-1" class="section level3 hasAnchor" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Assumptions<a href="mlr.html#assumptions-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Outside of the typical assumptions for ANOVA that still hold here, there are two additional assumptions to be met:</p>
<ul>
<li>Treatments are randomly assigned within each block</li>
<li>The effects of the treatment variable are constant across the levels of the blocking variable</li>
</ul>
<p>The first, new assumption of randomness deals with the reliability of the analysis. Randomness is key to removing the impact of the nuisance factors. The second, new assumption implies there is <strong>no interaction</strong> between the treatment variable and the blocking variable. For example, we are implying that the fertilizers’ impacts ob garlic bulb weight are not changed depending on what block you are on. In other words, fertilizers have the same impact regardless of sun exposure, pH levels, rain fall, etc. We are <strong>not</strong> saying these nuisance factors do not impact the target variable or bulb weight of garlic, just that they do not change the effect of the fertilizer on bulb weight.</p>
</div>
<div id="python-code-15" class="section level3 hasAnchor" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Python Code<a href="mlr.html#python-code-15" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb204"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb204-1"><a href="mlr.html#cb204-1" aria-hidden="true" tabindex="-1"></a>block<span class="op">=</span>pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/garlic_block.csv&#39;</span>)</span>
<span id="cb204-2"><a href="mlr.html#cb204-2" aria-hidden="true" tabindex="-1"></a>block.head(n <span class="op">=</span> <span class="dv">32</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-107-1.png" width="672" /></p>
<div class="sourceCode" id="cb205"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb205-1"><a href="mlr.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb205-2"><a href="mlr.html#cb205-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-3"><a href="mlr.html#cb205-3" aria-hidden="true" tabindex="-1"></a>model_b <span class="op">=</span> smf.ols(<span class="st">&quot;BulbWt ~ C(Fertilizer) + C(Sector)&quot;</span>, data <span class="op">=</span> block).fit()</span>
<span id="cb205-4"><a href="mlr.html#cb205-4" aria-hidden="true" tabindex="-1"></a>model_b.summary()</span>
<span id="cb205-5"><a href="mlr.html#cb205-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb205-6"><a href="mlr.html#cb205-6" aria-hidden="true" tabindex="-1"></a>sm.api.stats.anova_lm(model_b, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-108-3.png" width="672" /></p>
</div>
</div>
<div id="multiple-linear-regression" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Multiple Linear Regression<a href="mlr.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most practical applications of of regression modeling involve using more complicated models than a simple linear regression with only one predictor variable to predict your target. Additional variables in a model can lead to better explanations and predictions of the target. These linear regressions with more than one variable are called <strong>multiple linear regression</strong> models. However, as we will see in this section and the following chapters, with more variables comes much more complication.</p>
<div id="model-structure" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Model Structure<a href="mlr.html#model-structure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Multiple linear regression models have the same structure as simple linear regression models, only with more variables. The multiple linear regression model with <span class="math inline">\(k\)</span> variables is structured like the following:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
\]</span></p>
<p>This model has the predictor variables <span class="math inline">\(x_1, x_2, ..., x_k\)</span> trying to either explain or predict the target variable <span class="math inline">\(y\)</span>. The intercept, <span class="math inline">\(\beta_0\)</span>, still gives the expected value of <span class="math inline">\(y\)</span>, when <strong>all</strong> of the predictor variables take a value of 0. With the addition of multiple predictors, the interpretation of the slope coefficients change slightly. The slopes, <span class="math inline">\(\beta_1, \beta_2, ..., \beta_k\)</span>, give the expected change in <span class="math inline">\(y\)</span> for a one unit change in the respective predictor variable, <strong>holding all other predictor variables constant</strong>. The random error term, <span class="math inline">\(\varepsilon\)</span>, is the error between our predicted value, <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \cdots + \hat{\beta}_k x_k\)</span>, and our actual value of <span class="math inline">\(y\)</span>.</p>
<p>Unlike simple linear regression that can be visualized as a line through a 2-dimensional scatterplot of data, a multiple linear regression is better thought of as a multi-dimensional plane through a multi-dimensional scatterplot of data.</p>
<p>Let’s visual an example with two predictor variables - the square footage of greater living area and the total number of rooms. These will predict sale price of a home. When none of the variables have any relationship with the target variable, we get a horizontal plane like the one shown below. This is similar in concept to a horizontal line in simple linear regression having a slope of 0, implying that the target variable does not change as the predictor variable changes.</p>
<div class="plotly html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-3bd8d6e51a75f268729c" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-3bd8d6e51a75f268729c">{"x":{"visdat":{"52c036605359":["function () ","plotlyVisDat"]},"cur_data":"52c036605359","attrs":{"52c036605359":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Living Area (SQFT)"},"yaxis":{"title":"Total Rooms"},"zaxis":{"title":"Sale Price"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"pred_mx","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(37,144,140,1)"],["1","rgba(37,144,140,1)"]],"showscale":true,"z":[[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237],[178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237,178987.30814237]],"type":"surface","x":[300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000],"y":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>Much like if the slope of a simple linear regression line is <strong>not</strong> 0 (a relationship exists between the predictor and target variable), then a relationship between any of the predictor variables and the target variable shifts and rotates the plane around like the one shown below.</p>
<div class="plotly html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-c058b9278bff4810eb92" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-c058b9278bff4810eb92">{"x":{"visdat":{"52c04e544cd1":["function () ","plotlyVisDat"]},"cur_data":"52c04e544cd1","attrs":{"52c04e544cd1":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Living Area (SQFT)"},"yaxis":{"title":"Total Rooms"},"zaxis":{"title":"Sale Price"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"pred_mx","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[[67188.0407054248,61466.2403150283,55744.4399246317,50022.6395342351,44300.8391438385,38579.0387534419,32857.2383630454,27135.4379726488,21413.6375822522,15691.8371918556,9970.03680145903,4248.23641106245,-1473.56397933414,-7195.36436973071,-12917.1647601273,-18638.9651505239,-24360.7655409205,-30082.565931317,-35804.3663217136,-41526.1667121102,-47247.9671025068,-52969.7674929034,-58691.5678832999,-64413.3682736965,-70135.1686640931],[97483.9214874233,91762.1210970267,86040.3207066302,80318.5203162336,74596.719925837,68874.9195354404,63153.1191450438,57431.3187546472,51709.5183642507,45987.7179738541,40265.9175834575,34544.1171930609,28822.3168026643,23100.5164122678,17378.7160218712,11656.9156314746,5935.11524107801,213.314850681432,-5508.48553971517,-11230.2859301117,-16952.0863205083,-22673.8867109049,-28395.6871013015,-34117.4874916981,-39839.2878820946],[127779.802269422,122058.001879025,116336.201488629,110614.401098232,104892.600707835,99170.8003174389,93448.9999270423,87727.1995366457,82005.3991462491,76283.5987558525,70561.798365456,64839.9979750594,59118.1975846628,53396.3971942662,47674.5968038696,41952.796413473,36230.9960230765,30509.1956326799,24787.3952422833,19065.5948518867,13343.7944614901,7621.99407109353,1900.19368069698,-3821.60670969961,-9543.4071000962],[158075.68305142,152353.882661024,146632.082270627,140910.28188023,135188.481489834,129466.681099437,123744.880709041,118023.080318644,112301.279928248,106579.479537851,100857.679147454,95135.8787570578,89414.0783666613,83692.2779762647,77970.4775858681,72248.6771954715,66526.8768050749,60805.0764146784,55083.2760242818,49361.4756338852,43639.6752434886,37917.874853092,32196.0744626955,26474.2740722989,20752.4736819023],[188371.563833419,182649.763443022,176927.963052626,171206.162662229,165484.362271832,159762.561881436,154040.761491039,148318.961100643,142597.160710246,136875.360319849,131153.559929453,125431.759539056,119709.95914866,113988.158758263,108266.358367867,102544.55797747,96822.7575870734,91100.9571966768,85379.1568062802,79657.3564158836,73935.5560254871,68213.7556350905,62491.9552446939,56770.1548542973,51048.3544639007],[218667.444615417,212945.644225021,207223.843834624,201502.043444227,195780.243053831,190058.442663434,184336.642273038,178614.841882641,172893.041492245,167171.241101848,161449.440711451,155727.640321055,150005.839930658,144284.039540262,138562.239149865,132840.438759468,127118.638369072,121396.837978675,115675.037588279,109953.237197882,104231.436807486,98509.6364170889,92787.8360266924,87066.0356362958,81344.2352458992],[248963.325397416,243241.525007019,237519.724616623,231797.924226226,226076.123835829,220354.323445433,214632.523055036,208910.72266464,203188.922274243,197467.121883846,191745.32149345,186023.521103053,180301.720712657,174579.92032226,168858.119931864,163136.319541467,157414.51915107,151692.718760674,145970.918370277,140249.117979881,134527.317589484,128805.517199087,123083.716808691,117361.916418294,111640.116027898],[279259.206179414,273537.405789018,267815.605398621,262093.805008224,256372.004617828,250650.204227431,244928.403837035,239206.603446638,233484.803056241,227763.002665845,222041.202275448,216319.401885052,210597.601494655,204875.801104259,199154.000713862,193432.200323465,187710.399933069,181988.599542672,176266.799152276,170544.998761879,164823.198371482,159101.397981086,153379.597590689,147657.797200293,141935.996809896],[309555.086961413,303833.286571016,298111.486180619,292389.685790223,286667.885399826,280946.08500943,275224.284619033,269502.484228636,263780.68383824,258058.883447843,252337.083057447,246615.28266705,240893.482276654,235171.681886257,229449.88149586,223728.081105464,218006.280715067,212284.480324671,206562.679934274,200840.879543878,195119.079153481,189397.278763084,183675.478372688,177953.677982291,172231.877591895],[339850.967743411,334129.167353015,328407.366962618,322685.566572221,316963.766181825,311241.965791428,305520.165401032,299798.365010635,294076.564620238,288354.764229842,282632.963839445,276911.163449049,271189.363058652,265467.562668256,259745.762277859,254023.961887462,248302.161497066,242580.361106669,236858.560716273,231136.760325876,225414.959935479,219693.159545083,213971.359154686,208249.55876429,202527.758373893],[370146.848525409,364425.048135013,358703.247744616,352981.44735422,347259.646963823,341537.846573427,335816.04618303,330094.245792633,324372.445402237,318650.64501184,312928.844621444,307207.044231047,301485.24384065,295763.443450254,290041.643059857,284319.842669461,278598.042279064,272876.241888668,267154.441498271,261432.641107874,255710.840717478,249989.040327081,244267.239936685,238545.439546288,232823.639155892],[400442.729307408,394720.928917011,388999.128526615,383277.328136218,377555.527745822,371833.727355425,366111.926965029,360390.126574632,354668.326184235,348946.525793839,343224.725403442,337502.925013046,331781.124622649,326059.324232252,320337.523841856,314615.723451459,308893.923061063,303172.122670666,297450.32228027,291728.521889873,286006.721499476,280284.92110908,274563.120718683,268841.320328287,263119.51993789],[430738.610089406,425016.80969901,419295.009308613,413573.208918217,407851.40852782,402129.608137424,396407.807747027,390686.00735663,384964.206966234,379242.406575837,373520.606185441,367798.805795044,362077.005404647,356355.205014251,350633.404623854,344911.604233458,339189.803843061,333468.003452665,327746.203062268,322024.402671871,316302.602281475,310580.801891078,304859.001500682,299137.201110285,293415.400719889],[461034.490871405,455312.690481008,449590.890090612,443869.089700215,438147.289309819,432425.488919422,426703.688529025,420981.888138629,415260.087748232,409538.287357836,403816.486967439,398094.686577043,392372.886186646,386651.085796249,380929.285405853,375207.485015456,369485.68462506,363763.884234663,358042.083844266,352320.28345387,346598.483063473,340876.682673077,335154.88228268,329433.081892284,323711.281501887],[491330.371653403,485608.571263007,479886.77087261,474164.970482214,468443.170091817,462721.36970142,456999.569311024,451277.768920627,445555.968530231,439834.168139834,434112.367749438,428390.567359041,422668.766968644,416946.966578248,411225.166187851,405503.365797455,399781.565407058,394059.765016661,388337.964626265,382616.164235868,376894.363845472,371172.563455075,365450.763064679,359728.962674282,354007.162283885],[521626.252435402,515904.452045005,510182.651654609,504460.851264212,498739.050873816,493017.250483419,487295.450093022,481573.649702626,475851.849312229,470130.048921833,464408.248531436,458686.44814104,452964.647750643,447242.847360246,441521.04696985,435799.246579453,430077.446189057,424355.64579866,418633.845408263,412912.045017867,407190.24462747,401468.444237074,395746.643846677,390024.843456281,384303.043065884],[551922.1332174,546200.332827004,540478.532436607,534756.732046211,529034.931655814,523313.131265417,517591.330875021,511869.530484624,506147.730094228,500425.929703831,494704.129313434,488982.328923038,483260.528532641,477538.728142245,471816.927751848,466095.127361452,460373.326971055,454651.526580658,448929.726190262,443207.925799865,437486.125409469,431764.325019072,426042.524628676,420320.724238279,414598.923847882],[582218.013999399,576496.213609002,570774.413218606,565052.612828209,559330.812437812,553609.012047416,547887.211657019,542165.411266623,536443.610876226,530721.81048583,525000.010095433,519278.209705036,513556.40931464,507834.608924243,502112.808533847,496391.00814345,490669.207753054,484947.407362657,479225.60697226,473503.806581864,467782.006191467,462060.205801071,456338.405410674,450616.605020277,444894.804629881],[612513.894781397,606792.094391001,601070.294000604,595348.493610208,589626.693219811,583904.892829414,578183.092439018,572461.292048621,566739.491658225,561017.691267828,555295.890877431,549574.090487035,543852.290096638,538130.489706242,532408.689315845,526686.888925449,520965.088535052,515243.288144655,509521.487754259,503799.687363862,498077.886973466,492356.086583069,486634.286192673,480912.485802276,475190.685411879],[642809.775563396,637087.975172999,631366.174782603,625644.374392206,619922.574001809,614200.773611413,608478.973221016,602757.17283062,597035.372440223,591313.572049827,585591.77165943,579869.971269033,574148.170878637,568426.37048824,562704.570097844,556982.769707447,551260.96931705,545539.168926654,539817.368536257,534095.568145861,528373.767755464,522651.967365067,516930.166974671,511208.366584274,505486.566193878],[673105.656345394,667383.855954998,661662.055564601,655940.255174204,650218.454783808,644496.654393411,638774.854003015,633053.053612618,627331.253222222,621609.452831825,615887.652441428,610165.852051032,604444.051660635,598722.251270239,593000.450879842,587278.650489446,581556.850099049,575835.049708652,570113.249318256,564391.448927859,558669.648537463,552947.848147066,547226.047756669,541504.247366273,535782.446975876],[703401.537127393,697679.736736996,691957.9363466,686236.135956203,680514.335565806,674792.53517541,669070.734785013,663348.934394617,657627.13400422,651905.333613824,646183.533223427,640461.73283303,634739.932442634,629018.132052237,623296.331661841,617574.531271444,611852.730881047,606130.930490651,600409.130100254,594687.329709858,588965.529319461,583243.728929064,577521.928538668,571800.128148271,566078.327757875],[733697.417909391,727975.617518994,722253.817128598,716532.016738201,710810.216347805,705088.415957408,699366.615567012,693644.815176615,687923.014786219,682201.214395822,676479.414005425,670757.613615029,665035.813224632,659314.012834236,653592.212443839,647870.412053442,642148.611663046,636426.811272649,630705.010882253,624983.210491856,619261.410101459,613539.609711063,607817.809320666,602096.00893027,596374.208539873],[763993.29869139,758271.498300993,752549.697910597,746827.8975202,741106.097129803,735384.296739407,729662.49634901,723940.695958614,718218.895568217,712497.09517782,706775.294787424,701053.494397027,695331.694006631,689609.893616234,683888.093225837,678166.292835441,672444.492445044,666722.692054648,661000.891664251,655279.091273855,649557.290883458,643835.490493061,638113.690102665,632391.889712268,626670.089321872],[794289.179473388,788567.379082992,782845.578692595,777123.778302198,771401.977911802,765680.177521405,759958.377131009,754236.576740612,748514.776350216,742792.975959819,737071.175569422,731349.375179026,725627.574788629,719905.774398233,714183.974007836,708462.173617439,702740.373227043,697018.572836646,691296.77244625,685574.972055853,679853.171665456,674131.37127506,668409.570884663,662687.770494267,656965.97010387]],"type":"surface","x":[300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000],"y":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>To the naive viewer, the shifted plane would still make sense because of the model naming convention of multiple <strong>linear</strong> regression. However, the <strong>linear</strong> in linear regression doesn’t have to deal with the visualization of the fitted plane (or line in two dimensions), but instead refers to the <strong>linear combination of variables</strong>. A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results. For example, <span class="math inline">\(ax + by\)</span> is a linear combination of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Therefore, the linear model</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
\]</span>
is a linear combination of predictor variables in their relationship with the target variable <span class="math inline">\(y\)</span>. These predictor variables do not all have to contain linear effects though. For example, let’s look at a linear regression model with four predictor variables:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon
\]</span></p>
<p>One would not be hard pressed to call this model a linear regression. However, what if we defined <span class="math inline">\(x_3 = x_1^2\)</span> and <span class="math inline">\(x_4 = x_2^2\)</span>?</p>
<div class="plotly html-widget html-fill-item-overflow-hidden html-fill-item" id="htmlwidget-e35fb26f0b44b8606e57" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-e35fb26f0b44b8606e57">{"x":{"visdat":{"52c022b75c2c":["function () ","plotlyVisDat"]},"cur_data":"52c022b75c2c","attrs":{"52c022b75c2c":{"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"z":{},"type":"surface","x":{},"y":{},"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":"Living Area (SQFT)"},"yaxis":{"title":"Total Rooms"},"zaxis":{"title":"Sale Price"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"pred_mx","ticklen":2,"len":0.5,"lenmode":"fraction","y":1,"yanchor":"top"},"colorscale":[["0","rgba(68,1,84,1)"],["0.0416666666666667","rgba(70,19,97,1)"],["0.0833333333333333","rgba(72,32,111,1)"],["0.125","rgba(71,45,122,1)"],["0.166666666666667","rgba(68,58,128,1)"],["0.208333333333333","rgba(64,70,135,1)"],["0.25","rgba(60,82,138,1)"],["0.291666666666667","rgba(56,93,140,1)"],["0.333333333333333","rgba(49,104,142,1)"],["0.375","rgba(46,114,142,1)"],["0.416666666666667","rgba(42,123,142,1)"],["0.458333333333333","rgba(38,133,141,1)"],["0.5","rgba(37,144,140,1)"],["0.541666666666667","rgba(33,154,138,1)"],["0.583333333333333","rgba(39,164,133,1)"],["0.625","rgba(47,174,127,1)"],["0.666666666666667","rgba(53,183,121,1)"],["0.708333333333333","rgba(79,191,110,1)"],["0.75","rgba(98,199,98,1)"],["0.791666666666667","rgba(119,207,85,1)"],["0.833333333333333","rgba(147,214,70,1)"],["0.875","rgba(172,220,52,1)"],["0.916666666666667","rgba(199,225,42,1)"],["0.958333333333333","rgba(226,228,40,1)"],["1","rgba(253,231,37,1)"]],"showscale":true,"z":[[33947.3237976969,28781.5987890192,23439.163605726,17920.0182478173,12224.1627152931,6351.59700815342,302.321126398285,-5923.66492997232,-12326.3611609584,-18905.76756656,-25661.884146777,-32594.7109016095,-39704.2478310575,-46990.494935121,-54453.4522137999,-62093.1196670943,-69909.4972950042,-77902.5850975296,-86072.3830746704,-94418.8912264268,-102942.109552799,-111642.038053786,-120518.676729389,-129572.025579607,-138802.084604441],[74873.353534655,69707.6285259772,64365.193342684,58846.0479847753,53150.1924522511,47277.6267451115,41228.3508633563,35002.3648069857,28599.6685759996,22020.2621703981,15264.145590181,8331.31883534852,1221.78190590053,-6064.46519816293,-13527.4224768419,-21167.0899301363,-28983.4675580462,-36976.5553605715,-45146.3533377124,-53492.8614894687,-62016.0798158405,-70716.0083168278,-79592.6469924305,-88645.9958426488,-97876.0548674825],[114263.225813472,109097.500804794,103755.065621501,98235.9202635924,92540.0647310683,86667.4990239286,80618.2231421735,74392.2370858029,67989.5408548168,61410.1344492152,54654.0178689982,47721.1911141657,40611.6541847177,33325.4070806542,25862.4498019753,18222.7823486809,10406.404720771,2413.31691824561,-5756.48105889525,-14102.9892106516,-22626.2075370234,-31326.1360380106,-40202.7747136134,-49256.1235638316,-58486.1825886653],[152116.940634148,146951.215625471,141608.780442177,136089.635084269,130393.779551745,124521.213844605,118471.93796285,112245.951906479,105843.255675493,99263.8492698914,92507.7326896744,85574.9059348419,78465.3690053939,71179.1219013305,63716.1646226515,56076.4971693571,48260.1195414472,40267.0317389218,32097.233761781,23750.7256100247,15227.5072836529,6527.57878266558,-2349.05989293715,-11402.4087431554,-20632.4677679891],[188434.497996684,183268.772988006,177926.337804713,172407.192446804,166711.33691428,160838.77120714,154789.495325385,148563.509269014,142160.813038028,135581.406632427,128825.29005221,121892.463297377,114782.926367929,107496.679263866,100033.721985187,92394.0545318924,84577.6769039825,76584.5891014571,68414.7911243162,60068.2829725599,51545.0646461881,42845.1361452008,33968.4974695981,24915.1486193799,15685.0895945462],[223215.897901078,218050.1728924,212707.737709107,207188.592351198,201492.736818674,195620.171111534,189570.895229779,183344.909173409,176942.212942423,170362.806536821,163606.689956604,156673.863201772,149564.326272324,142278.07916826,134815.121889581,127175.454436287,119359.076808377,111365.989005852,103196.191028711,94849.6828769543,86326.4645505825,77626.5360495952,68749.8973739925,59696.5485237743,50466.4894989406],[256461.140347332,251295.415338654,245952.980155361,240433.834797452,234737.979264928,228865.413557788,222816.137676033,216590.151619662,210187.455388676,203608.048983075,196851.932402858,189919.105648025,182809.568718577,175523.321614514,168060.364335835,160420.69688254,152604.31925463,144611.231452105,136441.433474964,128094.925323208,119571.706996836,110871.778495849,101995.139820246,92941.7909700278,83711.731945194],[288170.225335444,283004.500326766,277662.065143473,272142.919785564,266447.06425304,260574.498545901,254525.222664145,248299.236607775,241896.540376789,235317.133971187,228561.01739097,221628.190636138,214518.65370669,207232.406602626,199769.449323947,192129.781870653,184313.404242743,176320.316440218,168150.518463077,159804.01031132,151280.791984949,142580.863483961,133704.224808359,124650.87595814,115420.816933307],[318343.152865416,313177.427856738,307834.992673445,302315.847315536,296619.991783012,290747.426075872,284698.150194117,278472.164137746,272069.46790676,265490.061501159,258733.944920942,251801.118166109,244691.581236661,237405.334132598,229942.376853919,222302.709400624,214486.331772715,206493.243970189,198323.445993048,189976.937841292,181453.71951492,172753.791013933,163877.15233833,154823.803488112,145593.744463278],[346979.922937247,341814.197928569,336471.762745276,330952.617387367,325256.761854843,319384.196147703,313334.920265948,307108.934209577,300706.237978591,294126.83157299,287370.714992773,280437.88823794,273328.351308492,266042.104204429,258579.14692575,250939.479472455,243123.101844545,235130.01404202,226960.216064879,218613.707913123,210090.489586751,201390.561085764,192513.922410161,183460.573559943,174230.514535109],[374080.535550936,368914.810542258,363572.375358965,358053.230001057,352357.374468532,346484.808761393,340435.532879638,334209.546823267,327806.850592281,321227.444186679,314471.327606462,307538.50085163,300428.963922182,293142.716818118,285679.759539439,278040.092086145,270223.714458235,262230.62665571,254060.828678569,245714.320526813,237191.102200441,228491.173699453,219614.535023851,210561.186173632,201331.127148799],[399644.990706485,394479.265697807,389136.830514514,383617.685156605,377921.829624081,372049.263916942,365999.988035186,359774.001978816,353371.30574783,346791.899342228,340035.782762011,333102.956007179,325993.419077731,318707.171973667,311244.214694988,303604.547241694,295788.169613784,287795.081811259,279625.283834118,271278.775682361,262755.55735599,254055.628855002,245178.9901794,236125.641329181,226895.582304348],[423673.288403893,418507.563395215,413165.128211922,407645.982854013,401950.127321489,396077.56161435,390028.285732594,383802.299676224,377399.603445238,370820.197039636,364064.080459419,357131.253704587,350021.716775139,342735.469671075,335272.512392396,327632.844939102,319816.467311192,311823.379508667,303653.581531526,295307.073379769,286783.855053398,278083.92655241,269207.287876808,260153.939026589,250923.880001756],[446165.42864316,440999.703634482,435657.268451189,430138.12309328,424442.267560756,418569.701853617,412520.425971862,406294.439915491,399891.743684505,393312.337278903,386556.220698686,379623.393943854,372513.857014406,365227.609910342,357764.652631663,350124.985178369,342308.607550459,334315.519747934,326145.721770793,317799.213619036,309275.995292665,300576.066791677,291699.428116075,282646.079265856,273416.020241023],[467121.411424286,461955.686415609,456613.251232315,451094.105874407,445398.250341882,439525.684634743,433476.408752988,427250.422696617,420847.726465631,414268.320060029,407512.203479812,400579.37672498,393469.839795532,386183.592691468,378720.635412789,371080.967959495,363264.590331585,355271.50252906,347101.704551919,338755.196400163,330231.978073791,321532.049572803,312655.410897201,303602.062046983,294372.003022149],[486541.236747272,481375.511738594,476033.076555301,470513.931197392,464818.075664868,458945.509957728,452896.234075973,446670.248019602,440267.551788616,433688.145383015,426932.028802798,419999.202047965,412889.665118517,405603.418014454,398140.460735775,390500.79328248,382684.41565457,374691.327852045,366521.529874904,358175.021723148,349651.803396776,340951.874895789,332075.236220186,323021.887369968,313791.828345134],[504424.904612116,499259.179603438,493916.744420145,488397.599062236,482701.743529712,476829.177822572,470779.901940817,464553.915884447,458151.21965346,451571.813247859,444815.696667642,437882.869912809,430773.332983361,423487.085879298,416024.128600619,408384.461147325,400568.083519415,392574.995716889,384405.197739748,376058.689587992,367535.47126162,358835.542760633,349958.90408503,340905.555234812,331675.496209978],[520772.415018819,515606.690010142,510264.254826848,504745.10946894,499049.253936415,493176.688229276,487127.412347521,480901.42629115,474498.730060164,467919.323654562,461163.207074345,454230.380319513,447120.843390065,439834.596286001,432371.639007322,424731.971554028,416915.593926118,408922.506123593,400752.708146452,392406.199994696,383882.981668324,375183.053167336,366306.414491734,357253.065641516,348023.006616682],[535583.767967382,530418.042958704,525075.607775411,519556.462417502,513860.606884978,507988.041177838,501938.765296083,495712.779239713,489310.083008726,482730.676603125,475974.560022908,469041.733268075,461932.196338627,454645.949234564,447182.991955885,439543.324502591,431726.946874681,423733.859072155,415564.061095014,407217.552943258,398694.334616886,389994.406115899,381117.767440296,372064.418590078,362834.359565244],[548858.963457803,543693.238449125,538350.803265832,532831.657907924,527135.802375399,521263.23666826,515213.960786505,508987.974730134,502585.278499148,496005.872093546,489249.755513329,482316.928758497,475207.391829049,467921.144724985,460458.187446307,452818.519993012,445002.142365102,437009.054562577,428839.256585436,420492.74843368,411969.530107308,403269.60160632,394392.962930718,385339.6140805,376109.555055666],[560598.001490084,555432.276481406,550089.841298113,544570.695940204,538874.84040768,533002.27470054,526952.998818785,520727.012762415,514324.316531429,507744.910125827,500988.79354561,494055.966790777,486946.429861329,479660.182757266,472197.225478587,464557.558025293,456741.180397383,448748.092594857,440578.294617717,432231.78646596,423708.568139588,415008.639638601,406132.000962998,397078.65211278,387848.593087947],[570800.882064224,565635.157055546,560292.721872253,554773.576514344,549077.72098182,543205.15527468,537155.879392925,530929.893336555,524527.197105568,517947.790699967,511191.67411975,504258.847364917,497149.310435469,489863.063331406,482400.106052727,474760.438599433,466944.060971523,458950.973168997,450781.175191856,442434.6670401,433911.448713728,425211.520212741,416334.881537138,407281.53268692,398051.473662086],[579467.605180223,574301.880171545,568959.444988252,563440.299630343,557744.444097819,551871.878390679,545822.602508924,539596.616452553,533193.920221567,526614.513815966,519858.397235749,512925.570480916,505816.033551468,498529.786447405,491066.829168726,483427.161715431,475610.784087521,467617.696284996,459447.898307855,451101.390156099,442578.171829727,433878.24332874,425001.604653137,415948.255802919,406718.196778085],[586598.17083808,581432.445829403,576090.010646109,570570.865288201,564875.009755676,559002.444048537,552953.168166782,546727.182110411,540324.485879425,533745.079473824,526988.962893607,520056.136138774,512946.599209326,505660.352105263,498197.394826584,490557.727373289,482741.349745379,474748.261942854,466578.463965713,458231.955813957,449708.737487585,441008.808986598,432132.170310995,423078.821460777,413848.762435943],[592192.579037797,587026.85402912,581684.418845826,576165.273487918,570469.417955393,564596.852248254,558547.576366499,552321.590310128,545918.894079142,539339.48767354,532583.371093324,525650.544338491,518541.007409043,511254.760304979,503791.803026301,496152.135573006,488335.757945096,480342.670142571,472172.87216543,463826.364013674,455303.145687302,446603.217186315,437726.578510712,428673.229660494,419443.17063566]],"type":"surface","x":[300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001,1002,1003,1004,1005,1006,1007,1008,1009,1010,1011,1012,1013,1014,1015,1016,1017,1018,1019,1020,1021,1022,1023,1024,1025,1026,1027,1028,1029,1030,1031,1032,1033,1034,1035,1036,1037,1038,1039,1040,1041,1042,1043,1044,1045,1046,1047,1048,1049,1050,1051,1052,1053,1054,1055,1056,1057,1058,1059,1060,1061,1062,1063,1064,1065,1066,1067,1068,1069,1070,1071,1072,1073,1074,1075,1076,1077,1078,1079,1080,1081,1082,1083,1084,1085,1086,1087,1088,1089,1090,1091,1092,1093,1094,1095,1096,1097,1098,1099,1100,1101,1102,1103,1104,1105,1106,1107,1108,1109,1110,1111,1112,1113,1114,1115,1116,1117,1118,1119,1120,1121,1122,1123,1124,1125,1126,1127,1128,1129,1130,1131,1132,1133,1134,1135,1136,1137,1138,1139,1140,1141,1142,1143,1144,1145,1146,1147,1148,1149,1150,1151,1152,1153,1154,1155,1156,1157,1158,1159,1160,1161,1162,1163,1164,1165,1166,1167,1168,1169,1170,1171,1172,1173,1174,1175,1176,1177,1178,1179,1180,1181,1182,1183,1184,1185,1186,1187,1188,1189,1190,1191,1192,1193,1194,1195,1196,1197,1198,1199,1200,1201,1202,1203,1204,1205,1206,1207,1208,1209,1210,1211,1212,1213,1214,1215,1216,1217,1218,1219,1220,1221,1222,1223,1224,1225,1226,1227,1228,1229,1230,1231,1232,1233,1234,1235,1236,1237,1238,1239,1240,1241,1242,1243,1244,1245,1246,1247,1248,1249,1250,1251,1252,1253,1254,1255,1256,1257,1258,1259,1260,1261,1262,1263,1264,1265,1266,1267,1268,1269,1270,1271,1272,1273,1274,1275,1276,1277,1278,1279,1280,1281,1282,1283,1284,1285,1286,1287,1288,1289,1290,1291,1292,1293,1294,1295,1296,1297,1298,1299,1300,1301,1302,1303,1304,1305,1306,1307,1308,1309,1310,1311,1312,1313,1314,1315,1316,1317,1318,1319,1320,1321,1322,1323,1324,1325,1326,1327,1328,1329,1330,1331,1332,1333,1334,1335,1336,1337,1338,1339,1340,1341,1342,1343,1344,1345,1346,1347,1348,1349,1350,1351,1352,1353,1354,1355,1356,1357,1358,1359,1360,1361,1362,1363,1364,1365,1366,1367,1368,1369,1370,1371,1372,1373,1374,1375,1376,1377,1378,1379,1380,1381,1382,1383,1384,1385,1386,1387,1388,1389,1390,1391,1392,1393,1394,1395,1396,1397,1398,1399,1400,1401,1402,1403,1404,1405,1406,1407,1408,1409,1410,1411,1412,1413,1414,1415,1416,1417,1418,1419,1420,1421,1422,1423,1424,1425,1426,1427,1428,1429,1430,1431,1432,1433,1434,1435,1436,1437,1438,1439,1440,1441,1442,1443,1444,1445,1446,1447,1448,1449,1450,1451,1452,1453,1454,1455,1456,1457,1458,1459,1460,1461,1462,1463,1464,1465,1466,1467,1468,1469,1470,1471,1472,1473,1474,1475,1476,1477,1478,1479,1480,1481,1482,1483,1484,1485,1486,1487,1488,1489,1490,1491,1492,1493,1494,1495,1496,1497,1498,1499,1500,1501,1502,1503,1504,1505,1506,1507,1508,1509,1510,1511,1512,1513,1514,1515,1516,1517,1518,1519,1520,1521,1522,1523,1524,1525,1526,1527,1528,1529,1530,1531,1532,1533,1534,1535,1536,1537,1538,1539,1540,1541,1542,1543,1544,1545,1546,1547,1548,1549,1550,1551,1552,1553,1554,1555,1556,1557,1558,1559,1560,1561,1562,1563,1564,1565,1566,1567,1568,1569,1570,1571,1572,1573,1574,1575,1576,1577,1578,1579,1580,1581,1582,1583,1584,1585,1586,1587,1588,1589,1590,1591,1592,1593,1594,1595,1596,1597,1598,1599,1600,1601,1602,1603,1604,1605,1606,1607,1608,1609,1610,1611,1612,1613,1614,1615,1616,1617,1618,1619,1620,1621,1622,1623,1624,1625,1626,1627,1628,1629,1630,1631,1632,1633,1634,1635,1636,1637,1638,1639,1640,1641,1642,1643,1644,1645,1646,1647,1648,1649,1650,1651,1652,1653,1654,1655,1656,1657,1658,1659,1660,1661,1662,1663,1664,1665,1666,1667,1668,1669,1670,1671,1672,1673,1674,1675,1676,1677,1678,1679,1680,1681,1682,1683,1684,1685,1686,1687,1688,1689,1690,1691,1692,1693,1694,1695,1696,1697,1698,1699,1700,1701,1702,1703,1704,1705,1706,1707,1708,1709,1710,1711,1712,1713,1714,1715,1716,1717,1718,1719,1720,1721,1722,1723,1724,1725,1726,1727,1728,1729,1730,1731,1732,1733,1734,1735,1736,1737,1738,1739,1740,1741,1742,1743,1744,1745,1746,1747,1748,1749,1750,1751,1752,1753,1754,1755,1756,1757,1758,1759,1760,1761,1762,1763,1764,1765,1766,1767,1768,1769,1770,1771,1772,1773,1774,1775,1776,1777,1778,1779,1780,1781,1782,1783,1784,1785,1786,1787,1788,1789,1790,1791,1792,1793,1794,1795,1796,1797,1798,1799,1800,1801,1802,1803,1804,1805,1806,1807,1808,1809,1810,1811,1812,1813,1814,1815,1816,1817,1818,1819,1820,1821,1822,1823,1824,1825,1826,1827,1828,1829,1830,1831,1832,1833,1834,1835,1836,1837,1838,1839,1840,1841,1842,1843,1844,1845,1846,1847,1848,1849,1850,1851,1852,1853,1854,1855,1856,1857,1858,1859,1860,1861,1862,1863,1864,1865,1866,1867,1868,1869,1870,1871,1872,1873,1874,1875,1876,1877,1878,1879,1880,1881,1882,1883,1884,1885,1886,1887,1888,1889,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,1902,1903,1904,1905,1906,1907,1908,1909,1910,1911,1912,1913,1914,1915,1916,1917,1918,1919,1920,1921,1922,1923,1924,1925,1926,1927,1928,1929,1930,1931,1932,1933,1934,1935,1936,1937,1938,1939,1940,1941,1942,1943,1944,1945,1946,1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025,2026,2027,2028,2029,2030,2031,2032,2033,2034,2035,2036,2037,2038,2039,2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075,2076,2077,2078,2079,2080,2081,2082,2083,2084,2085,2086,2087,2088,2089,2090,2091,2092,2093,2094,2095,2096,2097,2098,2099,2100,2101,2102,2103,2104,2105,2106,2107,2108,2109,2110,2111,2112,2113,2114,2115,2116,2117,2118,2119,2120,2121,2122,2123,2124,2125,2126,2127,2128,2129,2130,2131,2132,2133,2134,2135,2136,2137,2138,2139,2140,2141,2142,2143,2144,2145,2146,2147,2148,2149,2150,2151,2152,2153,2154,2155,2156,2157,2158,2159,2160,2161,2162,2163,2164,2165,2166,2167,2168,2169,2170,2171,2172,2173,2174,2175,2176,2177,2178,2179,2180,2181,2182,2183,2184,2185,2186,2187,2188,2189,2190,2191,2192,2193,2194,2195,2196,2197,2198,2199,2200,2201,2202,2203,2204,2205,2206,2207,2208,2209,2210,2211,2212,2213,2214,2215,2216,2217,2218,2219,2220,2221,2222,2223,2224,2225,2226,2227,2228,2229,2230,2231,2232,2233,2234,2235,2236,2237,2238,2239,2240,2241,2242,2243,2244,2245,2246,2247,2248,2249,2250,2251,2252,2253,2254,2255,2256,2257,2258,2259,2260,2261,2262,2263,2264,2265,2266,2267,2268,2269,2270,2271,2272,2273,2274,2275,2276,2277,2278,2279,2280,2281,2282,2283,2284,2285,2286,2287,2288,2289,2290,2291,2292,2293,2294,2295,2296,2297,2298,2299,2300,2301,2302,2303,2304,2305,2306,2307,2308,2309,2310,2311,2312,2313,2314,2315,2316,2317,2318,2319,2320,2321,2322,2323,2324,2325,2326,2327,2328,2329,2330,2331,2332,2333,2334,2335,2336,2337,2338,2339,2340,2341,2342,2343,2344,2345,2346,2347,2348,2349,2350,2351,2352,2353,2354,2355,2356,2357,2358,2359,2360,2361,2362,2363,2364,2365,2366,2367,2368,2369,2370,2371,2372,2373,2374,2375,2376,2377,2378,2379,2380,2381,2382,2383,2384,2385,2386,2387,2388,2389,2390,2391,2392,2393,2394,2395,2396,2397,2398,2399,2400,2401,2402,2403,2404,2405,2406,2407,2408,2409,2410,2411,2412,2413,2414,2415,2416,2417,2418,2419,2420,2421,2422,2423,2424,2425,2426,2427,2428,2429,2430,2431,2432,2433,2434,2435,2436,2437,2438,2439,2440,2441,2442,2443,2444,2445,2446,2447,2448,2449,2450,2451,2452,2453,2454,2455,2456,2457,2458,2459,2460,2461,2462,2463,2464,2465,2466,2467,2468,2469,2470,2471,2472,2473,2474,2475,2476,2477,2478,2479,2480,2481,2482,2483,2484,2485,2486,2487,2488,2489,2490,2491,2492,2493,2494,2495,2496,2497,2498,2499,2500,2501,2502,2503,2504,2505,2506,2507,2508,2509,2510,2511,2512,2513,2514,2515,2516,2517,2518,2519,2520,2521,2522,2523,2524,2525,2526,2527,2528,2529,2530,2531,2532,2533,2534,2535,2536,2537,2538,2539,2540,2541,2542,2543,2544,2545,2546,2547,2548,2549,2550,2551,2552,2553,2554,2555,2556,2557,2558,2559,2560,2561,2562,2563,2564,2565,2566,2567,2568,2569,2570,2571,2572,2573,2574,2575,2576,2577,2578,2579,2580,2581,2582,2583,2584,2585,2586,2587,2588,2589,2590,2591,2592,2593,2594,2595,2596,2597,2598,2599,2600,2601,2602,2603,2604,2605,2606,2607,2608,2609,2610,2611,2612,2613,2614,2615,2616,2617,2618,2619,2620,2621,2622,2623,2624,2625,2626,2627,2628,2629,2630,2631,2632,2633,2634,2635,2636,2637,2638,2639,2640,2641,2642,2643,2644,2645,2646,2647,2648,2649,2650,2651,2652,2653,2654,2655,2656,2657,2658,2659,2660,2661,2662,2663,2664,2665,2666,2667,2668,2669,2670,2671,2672,2673,2674,2675,2676,2677,2678,2679,2680,2681,2682,2683,2684,2685,2686,2687,2688,2689,2690,2691,2692,2693,2694,2695,2696,2697,2698,2699,2700,2701,2702,2703,2704,2705,2706,2707,2708,2709,2710,2711,2712,2713,2714,2715,2716,2717,2718,2719,2720,2721,2722,2723,2724,2725,2726,2727,2728,2729,2730,2731,2732,2733,2734,2735,2736,2737,2738,2739,2740,2741,2742,2743,2744,2745,2746,2747,2748,2749,2750,2751,2752,2753,2754,2755,2756,2757,2758,2759,2760,2761,2762,2763,2764,2765,2766,2767,2768,2769,2770,2771,2772,2773,2774,2775,2776,2777,2778,2779,2780,2781,2782,2783,2784,2785,2786,2787,2788,2789,2790,2791,2792,2793,2794,2795,2796,2797,2798,2799,2800,2801,2802,2803,2804,2805,2806,2807,2808,2809,2810,2811,2812,2813,2814,2815,2816,2817,2818,2819,2820,2821,2822,2823,2824,2825,2826,2827,2828,2829,2830,2831,2832,2833,2834,2835,2836,2837,2838,2839,2840,2841,2842,2843,2844,2845,2846,2847,2848,2849,2850,2851,2852,2853,2854,2855,2856,2857,2858,2859,2860,2861,2862,2863,2864,2865,2866,2867,2868,2869,2870,2871,2872,2873,2874,2875,2876,2877,2878,2879,2880,2881,2882,2883,2884,2885,2886,2887,2888,2889,2890,2891,2892,2893,2894,2895,2896,2897,2898,2899,2900,2901,2902,2903,2904,2905,2906,2907,2908,2909,2910,2911,2912,2913,2914,2915,2916,2917,2918,2919,2920,2921,2922,2923,2924,2925,2926,2927,2928,2929,2930,2931,2932,2933,2934,2935,2936,2937,2938,2939,2940,2941,2942,2943,2944,2945,2946,2947,2948,2949,2950,2951,2952,2953,2954,2955,2956,2957,2958,2959,2960,2961,2962,2963,2964,2965,2966,2967,2968,2969,2970,2971,2972,2973,2974,2975,2976,2977,2978,2979,2980,2981,2982,2983,2984,2985,2986,2987,2988,2989,2990,2991,2992,2993,2994,2995,2996,2997,2998,2999,3000,3001,3002,3003,3004,3005,3006,3007,3008,3009,3010,3011,3012,3013,3014,3015,3016,3017,3018,3019,3020,3021,3022,3023,3024,3025,3026,3027,3028,3029,3030,3031,3032,3033,3034,3035,3036,3037,3038,3039,3040,3041,3042,3043,3044,3045,3046,3047,3048,3049,3050,3051,3052,3053,3054,3055,3056,3057,3058,3059,3060,3061,3062,3063,3064,3065,3066,3067,3068,3069,3070,3071,3072,3073,3074,3075,3076,3077,3078,3079,3080,3081,3082,3083,3084,3085,3086,3087,3088,3089,3090,3091,3092,3093,3094,3095,3096,3097,3098,3099,3100,3101,3102,3103,3104,3105,3106,3107,3108,3109,3110,3111,3112,3113,3114,3115,3116,3117,3118,3119,3120,3121,3122,3123,3124,3125,3126,3127,3128,3129,3130,3131,3132,3133,3134,3135,3136,3137,3138,3139,3140,3141,3142,3143,3144,3145,3146,3147,3148,3149,3150,3151,3152,3153,3154,3155,3156,3157,3158,3159,3160,3161,3162,3163,3164,3165,3166,3167,3168,3169,3170,3171,3172,3173,3174,3175,3176,3177,3178,3179,3180,3181,3182,3183,3184,3185,3186,3187,3188,3189,3190,3191,3192,3193,3194,3195,3196,3197,3198,3199,3200,3201,3202,3203,3204,3205,3206,3207,3208,3209,3210,3211,3212,3213,3214,3215,3216,3217,3218,3219,3220,3221,3222,3223,3224,3225,3226,3227,3228,3229,3230,3231,3232,3233,3234,3235,3236,3237,3238,3239,3240,3241,3242,3243,3244,3245,3246,3247,3248,3249,3250,3251,3252,3253,3254,3255,3256,3257,3258,3259,3260,3261,3262,3263,3264,3265,3266,3267,3268,3269,3270,3271,3272,3273,3274,3275,3276,3277,3278,3279,3280,3281,3282,3283,3284,3285,3286,3287,3288,3289,3290,3291,3292,3293,3294,3295,3296,3297,3298,3299,3300,3301,3302,3303,3304,3305,3306,3307,3308,3309,3310,3311,3312,3313,3314,3315,3316,3317,3318,3319,3320,3321,3322,3323,3324,3325,3326,3327,3328,3329,3330,3331,3332,3333,3334,3335,3336,3337,3338,3339,3340,3341,3342,3343,3344,3345,3346,3347,3348,3349,3350,3351,3352,3353,3354,3355,3356,3357,3358,3359,3360,3361,3362,3363,3364,3365,3366,3367,3368,3369,3370,3371,3372,3373,3374,3375,3376,3377,3378,3379,3380,3381,3382,3383,3384,3385,3386,3387,3388,3389,3390,3391,3392,3393,3394,3395,3396,3397,3398,3399,3400,3401,3402,3403,3404,3405,3406,3407,3408,3409,3410,3411,3412,3413,3414,3415,3416,3417,3418,3419,3420,3421,3422,3423,3424,3425,3426,3427,3428,3429,3430,3431,3432,3433,3434,3435,3436,3437,3438,3439,3440,3441,3442,3443,3444,3445,3446,3447,3448,3449,3450,3451,3452,3453,3454,3455,3456,3457,3458,3459,3460,3461,3462,3463,3464,3465,3466,3467,3468,3469,3470,3471,3472,3473,3474,3475,3476,3477,3478,3479,3480,3481,3482,3483,3484,3485,3486,3487,3488,3489,3490,3491,3492,3493,3494,3495,3496,3497,3498,3499,3500,3501,3502,3503,3504,3505,3506,3507,3508,3509,3510,3511,3512,3513,3514,3515,3516,3517,3518,3519,3520,3521,3522,3523,3524,3525,3526,3527,3528,3529,3530,3531,3532,3533,3534,3535,3536,3537,3538,3539,3540,3541,3542,3543,3544,3545,3546,3547,3548,3549,3550,3551,3552,3553,3554,3555,3556,3557,3558,3559,3560,3561,3562,3563,3564,3565,3566,3567,3568,3569,3570,3571,3572,3573,3574,3575,3576,3577,3578,3579,3580,3581,3582,3583,3584,3585,3586,3587,3588,3589,3590,3591,3592,3593,3594,3595,3596,3597,3598,3599,3600,3601,3602,3603,3604,3605,3606,3607,3608,3609,3610,3611,3612,3613,3614,3615,3616,3617,3618,3619,3620,3621,3622,3623,3624,3625,3626,3627,3628,3629,3630,3631,3632,3633,3634,3635,3636,3637,3638,3639,3640,3641,3642,3643,3644,3645,3646,3647,3648,3649,3650,3651,3652,3653,3654,3655,3656,3657,3658,3659,3660,3661,3662,3663,3664,3665,3666,3667,3668,3669,3670,3671,3672,3673,3674,3675,3676,3677,3678,3679,3680,3681,3682,3683,3684,3685,3686,3687,3688,3689,3690,3691,3692,3693,3694,3695,3696,3697,3698,3699,3700,3701,3702,3703,3704,3705,3706,3707,3708,3709,3710,3711,3712,3713,3714,3715,3716,3717,3718,3719,3720,3721,3722,3723,3724,3725,3726,3727,3728,3729,3730,3731,3732,3733,3734,3735,3736,3737,3738,3739,3740,3741,3742,3743,3744,3745,3746,3747,3748,3749,3750,3751,3752,3753,3754,3755,3756,3757,3758,3759,3760,3761,3762,3763,3764,3765,3766,3767,3768,3769,3770,3771,3772,3773,3774,3775,3776,3777,3778,3779,3780,3781,3782,3783,3784,3785,3786,3787,3788,3789,3790,3791,3792,3793,3794,3795,3796,3797,3798,3799,3800,3801,3802,3803,3804,3805,3806,3807,3808,3809,3810,3811,3812,3813,3814,3815,3816,3817,3818,3819,3820,3821,3822,3823,3824,3825,3826,3827,3828,3829,3830,3831,3832,3833,3834,3835,3836,3837,3838,3839,3840,3841,3842,3843,3844,3845,3846,3847,3848,3849,3850,3851,3852,3853,3854,3855,3856,3857,3858,3859,3860,3861,3862,3863,3864,3865,3866,3867,3868,3869,3870,3871,3872,3873,3874,3875,3876,3877,3878,3879,3880,3881,3882,3883,3884,3885,3886,3887,3888,3889,3890,3891,3892,3893,3894,3895,3896,3897,3898,3899,3900,3901,3902,3903,3904,3905,3906,3907,3908,3909,3910,3911,3912,3913,3914,3915,3916,3917,3918,3919,3920,3921,3922,3923,3924,3925,3926,3927,3928,3929,3930,3931,3932,3933,3934,3935,3936,3937,3938,3939,3940,3941,3942,3943,3944,3945,3946,3947,3948,3949,3950,3951,3952,3953,3954,3955,3956,3957,3958,3959,3960,3961,3962,3963,3964,3965,3966,3967,3968,3969,3970,3971,3972,3973,3974,3975,3976,3977,3978,3979,3980,3981,3982,3983,3984,3985,3986,3987,3988,3989,3990,3991,3992,3993,3994,3995,3996,3997,3998,3999,4000,4001,4002,4003,4004,4005,4006,4007,4008,4009,4010,4011,4012,4013,4014,4015,4016,4017,4018,4019,4020,4021,4022,4023,4024,4025,4026,4027,4028,4029,4030,4031,4032,4033,4034,4035,4036,4037,4038,4039,4040,4041,4042,4043,4044,4045,4046,4047,4048,4049,4050,4051,4052,4053,4054,4055,4056,4057,4058,4059,4060,4061,4062,4063,4064,4065,4066,4067,4068,4069,4070,4071,4072,4073,4074,4075,4076,4077,4078,4079,4080,4081,4082,4083,4084,4085,4086,4087,4088,4089,4090,4091,4092,4093,4094,4095,4096,4097,4098,4099,4100,4101,4102,4103,4104,4105,4106,4107,4108,4109,4110,4111,4112,4113,4114,4115,4116,4117,4118,4119,4120,4121,4122,4123,4124,4125,4126,4127,4128,4129,4130,4131,4132,4133,4134,4135,4136,4137,4138,4139,4140,4141,4142,4143,4144,4145,4146,4147,4148,4149,4150,4151,4152,4153,4154,4155,4156,4157,4158,4159,4160,4161,4162,4163,4164,4165,4166,4167,4168,4169,4170,4171,4172,4173,4174,4175,4176,4177,4178,4179,4180,4181,4182,4183,4184,4185,4186,4187,4188,4189,4190,4191,4192,4193,4194,4195,4196,4197,4198,4199,4200,4201,4202,4203,4204,4205,4206,4207,4208,4209,4210,4211,4212,4213,4214,4215,4216,4217,4218,4219,4220,4221,4222,4223,4224,4225,4226,4227,4228,4229,4230,4231,4232,4233,4234,4235,4236,4237,4238,4239,4240,4241,4242,4243,4244,4245,4246,4247,4248,4249,4250,4251,4252,4253,4254,4255,4256,4257,4258,4259,4260,4261,4262,4263,4264,4265,4266,4267,4268,4269,4270,4271,4272,4273,4274,4275,4276,4277,4278,4279,4280,4281,4282,4283,4284,4285,4286,4287,4288,4289,4290,4291,4292,4293,4294,4295,4296,4297,4298,4299,4300,4301,4302,4303,4304,4305,4306,4307,4308,4309,4310,4311,4312,4313,4314,4315,4316,4317,4318,4319,4320,4321,4322,4323,4324,4325,4326,4327,4328,4329,4330,4331,4332,4333,4334,4335,4336,4337,4338,4339,4340,4341,4342,4343,4344,4345,4346,4347,4348,4349,4350,4351,4352,4353,4354,4355,4356,4357,4358,4359,4360,4361,4362,4363,4364,4365,4366,4367,4368,4369,4370,4371,4372,4373,4374,4375,4376,4377,4378,4379,4380,4381,4382,4383,4384,4385,4386,4387,4388,4389,4390,4391,4392,4393,4394,4395,4396,4397,4398,4399,4400,4401,4402,4403,4404,4405,4406,4407,4408,4409,4410,4411,4412,4413,4414,4415,4416,4417,4418,4419,4420,4421,4422,4423,4424,4425,4426,4427,4428,4429,4430,4431,4432,4433,4434,4435,4436,4437,4438,4439,4440,4441,4442,4443,4444,4445,4446,4447,4448,4449,4450,4451,4452,4453,4454,4455,4456,4457,4458,4459,4460,4461,4462,4463,4464,4465,4466,4467,4468,4469,4470,4471,4472,4473,4474,4475,4476,4477,4478,4479,4480,4481,4482,4483,4484,4485,4486,4487,4488,4489,4490,4491,4492,4493,4494,4495,4496,4497,4498,4499,4500,4501,4502,4503,4504,4505,4506,4507,4508,4509,4510,4511,4512,4513,4514,4515,4516,4517,4518,4519,4520,4521,4522,4523,4524,4525,4526,4527,4528,4529,4530,4531,4532,4533,4534,4535,4536,4537,4538,4539,4540,4541,4542,4543,4544,4545,4546,4547,4548,4549,4550,4551,4552,4553,4554,4555,4556,4557,4558,4559,4560,4561,4562,4563,4564,4565,4566,4567,4568,4569,4570,4571,4572,4573,4574,4575,4576,4577,4578,4579,4580,4581,4582,4583,4584,4585,4586,4587,4588,4589,4590,4591,4592,4593,4594,4595,4596,4597,4598,4599,4600,4601,4602,4603,4604,4605,4606,4607,4608,4609,4610,4611,4612,4613,4614,4615,4616,4617,4618,4619,4620,4621,4622,4623,4624,4625,4626,4627,4628,4629,4630,4631,4632,4633,4634,4635,4636,4637,4638,4639,4640,4641,4642,4643,4644,4645,4646,4647,4648,4649,4650,4651,4652,4653,4654,4655,4656,4657,4658,4659,4660,4661,4662,4663,4664,4665,4666,4667,4668,4669,4670,4671,4672,4673,4674,4675,4676,4677,4678,4679,4680,4681,4682,4683,4684,4685,4686,4687,4688,4689,4690,4691,4692,4693,4694,4695,4696,4697,4698,4699,4700,4701,4702,4703,4704,4705,4706,4707,4708,4709,4710,4711,4712,4713,4714,4715,4716,4717,4718,4719,4720,4721,4722,4723,4724,4725,4726,4727,4728,4729,4730,4731,4732,4733,4734,4735,4736,4737,4738,4739,4740,4741,4742,4743,4744,4745,4746,4747,4748,4749,4750,4751,4752,4753,4754,4755,4756,4757,4758,4759,4760,4761,4762,4763,4764,4765,4766,4767,4768,4769,4770,4771,4772,4773,4774,4775,4776,4777,4778,4779,4780,4781,4782,4783,4784,4785,4786,4787,4788,4789,4790,4791,4792,4793,4794,4795,4796,4797,4798,4799,4800,4801,4802,4803,4804,4805,4806,4807,4808,4809,4810,4811,4812,4813,4814,4815,4816,4817,4818,4819,4820,4821,4822,4823,4824,4825,4826,4827,4828,4829,4830,4831,4832,4833,4834,4835,4836,4837,4838,4839,4840,4841,4842,4843,4844,4845,4846,4847,4848,4849,4850,4851,4852,4853,4854,4855,4856,4857,4858,4859,4860,4861,4862,4863,4864,4865,4866,4867,4868,4869,4870,4871,4872,4873,4874,4875,4876,4877,4878,4879,4880,4881,4882,4883,4884,4885,4886,4887,4888,4889,4890,4891,4892,4893,4894,4895,4896,4897,4898,4899,4900,4901,4902,4903,4904,4905,4906,4907,4908,4909,4910,4911,4912,4913,4914,4915,4916,4917,4918,4919,4920,4921,4922,4923,4924,4925,4926,4927,4928,4929,4930,4931,4932,4933,4934,4935,4936,4937,4938,4939,4940,4941,4942,4943,4944,4945,4946,4947,4948,4949,4950,4951,4952,4953,4954,4955,4956,4957,4958,4959,4960,4961,4962,4963,4964,4965,4966,4967,4968,4969,4970,4971,4972,4973,4974,4975,4976,4977,4978,4979,4980,4981,4982,4983,4984,4985,4986,4987,4988,4989,4990,4991,4992,4993,4994,4995,4996,4997,4998,4999,5000,5001,5002,5003,5004,5005,5006,5007,5008,5009,5010,5011,5012,5013,5014,5015,5016,5017,5018,5019,5020,5021,5022,5023,5024,5025,5026,5027,5028,5029,5030,5031,5032,5033,5034,5035,5036,5037,5038,5039,5040,5041,5042,5043,5044,5045,5046,5047,5048,5049,5050,5051,5052,5053,5054,5055,5056,5057,5058,5059,5060,5061,5062,5063,5064,5065,5066,5067,5068,5069,5070,5071,5072,5073,5074,5075,5076,5077,5078,5079,5080,5081,5082,5083,5084,5085,5086,5087,5088,5089,5090,5091,5092,5093,5094,5095,5096,5097,5098,5099,5100,5101,5102,5103,5104,5105,5106,5107,5108,5109,5110,5111,5112,5113,5114,5115,5116,5117,5118,5119,5120,5121,5122,5123,5124,5125,5126,5127,5128,5129,5130,5131,5132,5133,5134,5135,5136,5137,5138,5139,5140,5141,5142,5143,5144,5145,5146,5147,5148,5149,5150,5151,5152,5153,5154,5155,5156,5157,5158,5159,5160,5161,5162,5163,5164,5165,5166,5167,5168,5169,5170,5171,5172,5173,5174,5175,5176,5177,5178,5179,5180,5181,5182,5183,5184,5185,5186,5187,5188,5189,5190,5191,5192,5193,5194,5195,5196,5197,5198,5199,5200,5201,5202,5203,5204,5205,5206,5207,5208,5209,5210,5211,5212,5213,5214,5215,5216,5217,5218,5219,5220,5221,5222,5223,5224,5225,5226,5227,5228,5229,5230,5231,5232,5233,5234,5235,5236,5237,5238,5239,5240,5241,5242,5243,5244,5245,5246,5247,5248,5249,5250,5251,5252,5253,5254,5255,5256,5257,5258,5259,5260,5261,5262,5263,5264,5265,5266,5267,5268,5269,5270,5271,5272,5273,5274,5275,5276,5277,5278,5279,5280,5281,5282,5283,5284,5285,5286,5287,5288,5289,5290,5291,5292,5293,5294,5295,5296,5297,5298,5299,5300,5301,5302,5303,5304,5305,5306,5307,5308,5309,5310,5311,5312,5313,5314,5315,5316,5317,5318,5319,5320,5321,5322,5323,5324,5325,5326,5327,5328,5329,5330,5331,5332,5333,5334,5335,5336,5337,5338,5339,5340,5341,5342,5343,5344,5345,5346,5347,5348,5349,5350,5351,5352,5353,5354,5355,5356,5357,5358,5359,5360,5361,5362,5363,5364,5365,5366,5367,5368,5369,5370,5371,5372,5373,5374,5375,5376,5377,5378,5379,5380,5381,5382,5383,5384,5385,5386,5387,5388,5389,5390,5391,5392,5393,5394,5395,5396,5397,5398,5399,5400,5401,5402,5403,5404,5405,5406,5407,5408,5409,5410,5411,5412,5413,5414,5415,5416,5417,5418,5419,5420,5421,5422,5423,5424,5425,5426,5427,5428,5429,5430,5431,5432,5433,5434,5435,5436,5437,5438,5439,5440,5441,5442,5443,5444,5445,5446,5447,5448,5449,5450,5451,5452,5453,5454,5455,5456,5457,5458,5459,5460,5461,5462,5463,5464,5465,5466,5467,5468,5469,5470,5471,5472,5473,5474,5475,5476,5477,5478,5479,5480,5481,5482,5483,5484,5485,5486,5487,5488,5489,5490,5491,5492,5493,5494,5495,5496,5497,5498,5499,5500,5501,5502,5503,5504,5505,5506,5507,5508,5509,5510,5511,5512,5513,5514,5515,5516,5517,5518,5519,5520,5521,5522,5523,5524,5525,5526,5527,5528,5529,5530,5531,5532,5533,5534,5535,5536,5537,5538,5539,5540,5541,5542,5543,5544,5545,5546,5547,5548,5549,5550,5551,5552,5553,5554,5555,5556,5557,5558,5559,5560,5561,5562,5563,5564,5565,5566,5567,5568,5569,5570,5571,5572,5573,5574,5575,5576,5577,5578,5579,5580,5581,5582,5583,5584,5585,5586,5587,5588,5589,5590,5591,5592,5593,5594,5595,5596,5597,5598,5599,5600,5601,5602,5603,5604,5605,5606,5607,5608,5609,5610,5611,5612,5613,5614,5615,5616,5617,5618,5619,5620,5621,5622,5623,5624,5625,5626,5627,5628,5629,5630,5631,5632,5633,5634,5635,5636,5637,5638,5639,5640,5641,5642,5643,5644,5645,5646,5647,5648,5649,5650,5651,5652,5653,5654,5655,5656,5657,5658,5659,5660,5661,5662,5663,5664,5665,5666,5667,5668,5669,5670,5671,5672,5673,5674,5675,5676,5677,5678,5679,5680,5681,5682,5683,5684,5685,5686,5687,5688,5689,5690,5691,5692,5693,5694,5695,5696,5697,5698,5699,5700,5701,5702,5703,5704,5705,5706,5707,5708,5709,5710,5711,5712,5713,5714,5715,5716,5717,5718,5719,5720,5721,5722,5723,5724,5725,5726,5727,5728,5729,5730,5731,5732,5733,5734,5735,5736,5737,5738,5739,5740,5741,5742,5743,5744,5745,5746,5747,5748,5749,5750,5751,5752,5753,5754,5755,5756,5757,5758,5759,5760,5761,5762,5763,5764,5765,5766,5767,5768,5769,5770,5771,5772,5773,5774,5775,5776,5777,5778,5779,5780,5781,5782,5783,5784,5785,5786,5787,5788,5789,5790,5791,5792,5793,5794,5795,5796,5797,5798,5799,5800,5801,5802,5803,5804,5805,5806,5807,5808,5809,5810,5811,5812,5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828,5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844,5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860,5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876,5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892,5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908,5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924,5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940,5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956,5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972,5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988,5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000],"y":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>This model is still a linear regression model. The structure of the model did not change. The model is still a linear combination of predictor variables related to the target variable. The predictor variables just do not all have a linear effect in terms of their relationship with <span class="math inline">\(y\)</span>. However, mathematically, it is still a linear combination and a linear regression model.</p>
</div>
<div id="global-local-inference" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Global &amp; Local Inference<a href="mlr.html#global-local-inference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In simple linear regression we could just look at the t-test for our slope parameter estimate to determine the utility of our model. With multiple parameter estimates comes multiple t-tests. Instead of looking at every individual parameter estimate initially, there is a way to determine the model adequacy for predicting the target variable overall.</p>
<p>The utility of a multiple regression model can be tested with a single test that encompasses all the coefficients from the model. This kind of test is called a <strong>global test</strong> since it tests all <span class="math inline">\(\beta\)</span>’s simultaneously. The <strong>Global F-Test</strong> uses the F-distribution to do just that for multiple linear regression models. The hypotheses for this test are the following:</p>
<p><span class="math display">\[
H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0 \\
H_A: \text{at least one } \beta \text{ is nonzero}
\]</span></p>
<p>In simpler terms, the null hypothesis is that none of the variables are useful in predicting the target variable. The alternative hypothesis is that <strong>at least one</strong> of these variables is useful in predicting the target variable.</p>
<p>The F-distribution is a distribution that has the following characteristics:</p>
<ul>
<li>Bounded below by 0</li>
<li>Right-skewed</li>
<li>Both <strong>numerator</strong> and <strong>denominator</strong> degrees of freedom</li>
</ul>
<p>A plot of a variety of F distributions is shown here:</p>
<pre><code>## Warning: Removed 1500 rows containing missing values
## (`geom_line()`).</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-112-1.png" width="672" /></p>
<p>If the global test is significant, the next step would be to examine the individual t-tests to see which variables are significant and which ones are not. This is similar to post-hoc testing in ANOVA where we explored which of the categories was statistically different when we knew at least one was.</p>
<p>These tests are all available using the <code>summary</code> function on an <code>lm</code> function for linear regression. To build a multiple linear regression in R using the <code>lm</code> function, you just add another variable to the formula element. Here we will predict the sales price (<code>Sale_Price</code>) based on the square footage of the greater living area of the home (<code>Gr_Liv_Area</code>) as well as total number of rooms above ground (<code>TotRms_AbvGrd</code>).</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="mlr.html#cb207-1" aria-hidden="true" tabindex="-1"></a>ames_lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> TotRms_AbvGrd, <span class="at">data =</span> train)</span>
<span id="cb207-2"><a href="mlr.html#cb207-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb207-3"><a href="mlr.html#cb207-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -528656  -30077   -1230   21427  361465 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    42562.657   5365.721   7.932 3.51e-15 ***
## Gr_Liv_Area      136.982      4.207  32.558  &lt; 2e-16 ***
## TotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56630 on 2048 degrees of freedom
## Multiple R-squared:  0.5024, Adjusted R-squared:  0.5019 
## F-statistic:  1034 on 2 and 2048 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>At the bottom of the above output is the result of the global F-test. Since the p-value on this test is lower than the significance level of 0.05, we have statistical evidence that at least of the two variables - <code>Gr_Liv_Area</code> and <code>TotRms_AbvGrd</code> - is significant at predicting the sale price of the home. By looking at the individual t-tests in the output above, we can see that both variables are actually significant.</p>
</div>
<div id="assumptions-2" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Assumptions<a href="mlr.html#assumptions-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>main</strong> assumptions for the multiple linear regression model are the same as with the simple linear regression model:</p>
<ol style="list-style-type: decimal">
<li>The expected value of <span class="math inline">\(y\)</span> is linear in the <span class="math inline">\(x\)</span>’s (proper model specification).</li>
<li>The random errors are independent.</li>
<li>The random errors are normally distributed.</li>
<li>The random errors have equal variance (homoskedasticity).</li>
</ol>
<p>However, with multiple variables there is an additional assumption that people tend to add to multiple linear regression modeling:</p>
<ol start="5" style="list-style-type: decimal">
<li>No <strong>perfect</strong> collinearity (also called multicollinearity)</li>
</ol>
<p>The new assumption means that no combination of predictor variables is a perfect linear combination with any other predictor variables. Collinearity, also called multicollinearity, occurs when predictor variables are correlated with each other. People often misstate this additional assumption as having no collinearity at all. This is too restrictive and basically impossible to meet in a realistic setting. Only when collinearity has a drastic impact on the linear regression do we need to concern ourselves. In fact, linear regression only completely breaks when that collinearity is perfect. Dealing with multicollinearity is discussed later.</p>
<p>Similar to simple linear regression, we can evaluate the assumptions by looking at residual plots. The <code>plot</code> function on the <code>lm</code> object provides these.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="mlr.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb209-2"><a href="mlr.html#cb209-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ames_lm2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-114-1.png" width="672" /></p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="mlr.html#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>These will again be covered in much more detail in Diagnostic Chapter.</p>
</div>
<div id="multiple-coefficients-of-determination" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Multiple Coefficients of Determination<a href="mlr.html#multiple-coefficients-of-determination" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the main advantages of multiple linear regression is that the complexity of the model enables us to investigate the relationship among <span class="math inline">\(y\)</span> and several predictor variables simultaneously. However, this increased complexity makes it more difficult to not only interpret the models, but also ascertain which model is “best.”</p>
<p>One example of this would be the coefficient of determination, <span class="math inline">\(R^2\)</span>, that we discussed earlier. The calculation for <span class="math inline">\(R^2\)</span> is the exact same:</p>
<p><span class="math display">\[
R^2 = 1 - \frac{SSE}{TSS}
\]</span></p>
<p>However, the problem with the calculation of <span class="math inline">\(R^2\)</span> in a multiple linear regression is that the addition of any variable (useful or not) will never make the <span class="math inline">\(R^2\)</span> decrease. In fact, it typically increases even with the addition of a useless variable. The reason is rather intuitive. When adding information to a regression model, your predictions can only get better, not worse. If a new predictor variable has no impact on the target variable, then the predictions can not get any worse than what they already were before the addition of the useless variable. Therefore, the <span class="math inline">\(SSE\)</span> would never increase, making the <span class="math inline">\(R^2\)</span> never decrease.</p>
<p>To account for this problem, there is the <strong>adjusted coefficient of determination</strong>, <span class="math inline">\(R^2_a\)</span>. The calculation is the following:</p>
<p><span class="math display">\[
R^2_a = 1 - [(\frac{n-1}{n-(k+1)})\times (\frac{SSE}{TSS})]
\]</span></p>
<p>Notice what the calculation is doing. It takes the original ratio on the right hand side of the <span class="math inline">\(R^2\)</span> equation, <span class="math inline">\(SSE/TSS\)</span>, and penalizes it. It multiplies this number by a ratio that is always greater than 1 if <span class="math inline">\(k &gt; 0\)</span>. Remember, <span class="math inline">\(k\)</span> is the number of variables in the model. Therefore, as the number of variables increases, the calculation penalizes the model more and more. However, if the reduction of SSE from adding a useful variable is low enough, then even with the additional penalization, the <span class="math inline">\(R^2_a\)</span> will increase <strong>if the variable is a useful addition to the model</strong>. If the variable is not a useful addition to the model, the <span class="math inline">\(R^2_a\)</span> will decrease. The <span class="math inline">\(R^2_a\)</span> is only one of many ways to select the “best” model for multiple linear regression.</p>
<p>One downside of this new metric is that the <span class="math inline">\(R^2_a\)</span> loses its interpretation. Since <span class="math inline">\(R^2_a \le R^2\)</span>, it is no longer bounded below by zero. Therefore, it can no longer be the proportion of variation explained in the target variable by the model. However, we can easily use <span class="math inline">\(R^2_a\)</span> to select a model correctly and interpret that model with <span class="math inline">\(R^2\)</span>. Both of these numbers can be found using the <code>summary</code> function on the <code>lm</code> object from the previous model.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="mlr.html#cb211-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -528656  -30077   -1230   21427  361465 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    42562.657   5365.721   7.932 3.51e-15 ***
## Gr_Liv_Area      136.982      4.207  32.558  &lt; 2e-16 ***
## TotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56630 on 2048 degrees of freedom
## Multiple R-squared:  0.5024, Adjusted R-squared:  0.5019 
## F-statistic:  1034 on 2 and 2048 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>From this output we can say that the combination of <code>Gr_Liv_Area</code> and <code>TotRmsAbvGrd</code> account for 50.24% of the variation in <code>Sale_Price</code>. Now let’s add a random variable to the model. This random variable will take random values from a normal distribution with mean of 0 and standard deviation of 1 and has no impact on the target variable.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="mlr.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb213-2"><a href="mlr.html#cb213-2" aria-hidden="true" tabindex="-1"></a>ames_lm3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> TotRms_AbvGrd <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(Sale_Price), <span class="dv">0</span>, <span class="dv">1</span>), <span class="at">data =</span> train)</span>
<span id="cb213-3"><a href="mlr.html#cb213-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb213-4"><a href="mlr.html#cb213-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_lm3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 
##     0, 1), data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -527926  -29943   -1298   21427  363925 
## 
## Coefficients:
##                                   Estimate Std. Error t value
## (Intercept)                      42589.091   5364.877   7.939
## Gr_Liv_Area                        136.927      4.207  32.548
## TotRms_AbvGrd                   -10552.425   1369.808  -7.704
## rnorm(length(Sale_Price), 0, 1)   1629.854   1259.478   1.294
##                                 Pr(&gt;|t|)    
## (Intercept)                     3.34e-15 ***
## Gr_Liv_Area                      &lt; 2e-16 ***
## TotRms_AbvGrd                   2.05e-14 ***
## rnorm(length(Sale_Price), 0, 1)    0.196    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56620 on 2047 degrees of freedom
## Multiple R-squared:  0.5028, Adjusted R-squared:  0.502 
## F-statistic: 689.9 on 3 and 2047 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Notice that the <span class="math inline">\(R^2\)</span> of this model actually increased to 0.5028 from 0.5024. However, the <span class="math inline">\(R^2_a\)</span> value stayed approximately the same at 0.502 since the addition of this new variable did not provide enough predictive power to outweigh the penalty of adding it.</p>
</div>
<div id="categorical-predictor-variables" class="section level3 hasAnchor" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Categorical Predictor Variables<a href="mlr.html#categorical-predictor-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As mentioned in EDA Section, there are two types of variables typically used in modeling:</p>
<ul>
<li>Quantitative (or numeric)</li>
<li>Qualitative (or categorical)</li>
</ul>
<p>Categorical variables need to be coded differently because they are not numerical in nature. As mentioned in EDA Section, two common coding techniques for linear regression are <strong>reference</strong> and <strong>effects</strong> coding. The interpretation of the coefficients (<span class="math inline">\(\beta\)</span>’s) of these variables in a regression model depend on the specific coding used. The predictions from the model, however, will remain the same regardless of the specific coding that is used.</p>
<p>Let’s use the example of the <code>Central_Air</code> variable with 2 categories - Y and N. Using reference coding, the <strong>reference</strong> coded variable to describe these 2 categories (with N as the reference level) would be the following:</p>
<table>
<tr>
<td>
Central Air
<td>
X1
<tr>
<td>
Y
<td>
1
<tr>
<td>
N
<td>
0
</table>
<caption>
<span id="tab:centralair">Table 3.1: </span>Reference variable coding for the categorical attribute <em>Central Air</em>
</caption>
<p>The linear regression equation would be:</p>
<p><span class="math display">\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
\]</span></p>
<p>Let’s see the mathematical interpretation of the coefficient <span class="math inline">\(\hat{\beta}_1\)</span>. To do this, let’s get the average sale price of a home prediction for a home with central air (<span class="math inline">\(\hat{y}_Y\)</span>) and without central air (<span class="math inline">\(\hat{y}_N\)</span>):</p>
<p><span class="math display">\[
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot 0 = \hat{\beta}_0
\]</span></p>
<p>By subtracting these two equations (<span class="math inline">\(\hat{y}_Y - \hat{y}_N = \hat{\beta}_1\)</span>), we can get the prediction for the average difference in price between a home with central air and without central air. This shows that in reference coding, the coefficient on each dummy variable is the average difference between that category and the reference category (the category not represented with its own variable). The math can be extended to as many categories as needed.</p>
<p>Using effects coding, the <strong>effects</strong> coded variable to describe these 2 categories (with N as the reference level) would be the following:</p>
<table>
<tr>
<td>
Central Air
<td>
X1
<tr>
<td>
Y
<td>
1
<tr>
<td>
N
<td>
-1
</table>
<caption>
<span id="tab:centralair">Table 3.1: </span>Effects variable coding for the categorical attribute <em>Central Air</em>
</caption>
<p>The linear regression equation would be:</p>
<p><span class="math display">\[
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
\]</span></p>
<p>Let’s see the mathematical interpretation of the coefficient <span class="math inline">\(\hat{\beta}_1\)</span>. To do this, let’s get the average sale price of a home prediction for a home with central air (<span class="math inline">\(\hat{y}_Y\)</span>) and without central air (<span class="math inline">\(\hat{y}_N\)</span>):</p>
<p><span class="math display">\[
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot -1 = \hat{\beta}_0 - \hat{\beta}_1
\]</span></p>
<p>Similar to reference coding, the coefficient <span class="math inline">\(\hat{\beta}_1\)</span> is the average difference between homes with central air and <span class="math inline">\(\hat{\beta}_0\)</span>. However, what is <span class="math inline">\(\hat{\beta}_0\)</span>? By taking the average of our two predictions:</p>
<p><span class="math display">\[
\frac{1}{2} \times (\hat{y}_Y + \hat{y}_N) = \frac{1}{2} \times (\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_0 - \hat{\beta}_1) = \frac{1}{2} \times (2\hat{\beta}_0) = \hat{\beta}_0
\]</span></p>
<p>From this average we can get the prediction for the average difference in price between a home with central air and the average price across all homes. This shows that in effects coding, the coefficient on each dummy variable is the average difference between that category and the average price across <strong>all</strong> homes (including both with and without central air). The math can be extended to as many categories as needed.</p>
<p>Let’s see an example with <code>Central_Air</code> as a variable added to our multiple linear regression model as a reference coded variable.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="mlr.html#cb215-1" aria-hidden="true" tabindex="-1"></a>ames_lm4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area <span class="sc">+</span> TotRms_AbvGrd <span class="sc">+</span> Central_Air, <span class="at">data =</span> train)</span>
<span id="cb215-2"><a href="mlr.html#cb215-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-3"><a href="mlr.html#cb215-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ames_lm4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Central_Air, 
##     data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -510745  -28984   -2317   20273  356742 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -7169.259   6778.879  -1.058     0.29    
## Gr_Liv_Area     129.594      4.131  31.374  &lt; 2e-16 ***
## TotRms_AbvGrd -8980.938   1335.669  -6.724 2.29e-11 ***
## Central_AirY  54513.082   4762.926  11.445  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 54910 on 2047 degrees of freedom
## Multiple R-squared:  0.5323, Adjusted R-squared:  0.5316 
## F-statistic: 776.6 on 3 and 2047 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>With these results we estimate the average difference in sales price between homes with central air and without central air to be $54,513.08.</p>
</div>
<div id="python-code-16" class="section level3 hasAnchor" number="3.4.6">
<h3><span class="header-section-number">3.4.6</span> Python Code<a href="mlr.html#python-code-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Global and Local Inference</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb217-1"><a href="mlr.html#cb217-1" aria-hidden="true" tabindex="-1"></a>model_mlr <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb217-2"><a href="mlr.html#cb217-2" aria-hidden="true" tabindex="-1"></a>model_mlr.summary()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-118-1.png" width="672" /></p>
<p>Assumptions</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb218-1"><a href="mlr.html#cb218-1" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;pred_mlr&#39;</span>] <span class="op">=</span> model_mlr.predict()</span>
<span id="cb218-2"><a href="mlr.html#cb218-2" aria-hidden="true" tabindex="-1"></a>train[<span class="st">&#39;resid_mlr&#39;</span>] <span class="op">=</span> model_mlr.resid</span>
<span id="cb218-3"><a href="mlr.html#cb218-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb218-4"><a href="mlr.html#cb218-4" aria-hidden="true" tabindex="-1"></a>train[[<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;pred_mlr&#39;</span>, <span class="st">&#39;resid_mlr&#39;</span>]].head(n <span class="op">=</span> <span class="dv">10</span>)</span>
<span id="cb218-5"><a href="mlr.html#cb218-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb218-6"><a href="mlr.html#cb218-6" aria-hidden="true" tabindex="-1"></a>sm.api.qqplot(train[<span class="st">&#39;resid_mlr&#39;</span>])</span>
<span id="cb218-7"><a href="mlr.html#cb218-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-119-3.png" width="672" /><img src="bookdownproj_files/figure-html/unnamed-chunk-119-4.png" width="672" /></p>
<p>Multiple Coefficient of Determination</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb219-1"><a href="mlr.html#cb219-1" aria-hidden="true" tabindex="-1"></a>model_mlr.summary()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-120-7.png" width="672" /></p>
<p>Categorical Predictor Variables</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb220-1"><a href="mlr.html#cb220-1" aria-hidden="true" tabindex="-1"></a>model_mlr2 <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + C(Central_Air)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb220-2"><a href="mlr.html#cb220-2" aria-hidden="true" tabindex="-1"></a>model_mlr2.summary()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-121-9.png" width="672" /></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="slr.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-selection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/edit/master/03-complex_ANOVA_Regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/blob/master/03-complex_ANOVA_Regression.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"toc": null
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
