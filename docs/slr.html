<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to ANOVA and Linear Regression | Statistical Foundations</title>
  <meta name="description" content="Chapter 2 Introduction to ANOVA and Linear Regression | Statistical Foundations" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to ANOVA and Linear Regression | Statistical Foundations" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="IAA-Faculty/statistical_foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to ANOVA and Linear Regression | Statistical Foundations" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intro-stat.html"/>
<link rel="next" href="mlr.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a style="font-weight:bold, text-align:center" href="https://github.com/IAA-Faculty/statistical_foundations/">Statistical Foundations</a>
<img src="./img/iaaicon.png" alt="IAA"  class="center"</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-stat.html"><a href="intro-stat.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro-stat.html"><a href="intro-stat.html#eda"><i class="fa fa-check"></i><b>1.1</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro-stat.html"><a href="intro-stat.html#vartypes"><i class="fa fa-check"></i><b>1.1.1</b> Types of Variables</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-stat.html"><a href="intro-stat.html#distributions"><i class="fa fa-check"></i><b>1.1.2</b> Distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-stat.html"><a href="intro-stat.html#normal"><i class="fa fa-check"></i><b>1.1.3</b> The Normal Distribution</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-stat.html"><a href="intro-stat.html#skew"><i class="fa fa-check"></i><b>1.1.4</b> Skewness</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-stat.html"><a href="intro-stat.html#kurt"><i class="fa fa-check"></i><b>1.1.5</b> Kurtosis</a></li>
<li class="chapter" data-level="1.1.6" data-path="intro-stat.html"><a href="intro-stat.html#graphdist"><i class="fa fa-check"></i><b>1.1.6</b> Graphical Displays of Distributions</a></li>
<li class="chapter" data-level="1.1.7" data-path="intro-stat.html"><a href="intro-stat.html#python-code"><i class="fa fa-check"></i><b>1.1.7</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro-stat.html"><a href="intro-stat.html#pointest"><i class="fa fa-check"></i><b>1.2</b> Point Estimates</a></li>
<li class="chapter" data-level="1.3" data-path="intro-stat.html"><a href="intro-stat.html#ci"><i class="fa fa-check"></i><b>1.3</b> Confidence Intervals</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="intro-stat.html"><a href="intro-stat.html#python-code-1"><i class="fa fa-check"></i><b>1.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="intro-stat.html"><a href="intro-stat.html#hypotest"><i class="fa fa-check"></i><b>1.4</b> Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="intro-stat.html"><a href="intro-stat.html#onesample"><i class="fa fa-check"></i><b>1.4.1</b> One-Sample T-Test</a></li>
<li class="chapter" data-level="1.4.2" data-path="intro-stat.html"><a href="intro-stat.html#python-code-2"><i class="fa fa-check"></i><b>1.4.2</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-stat.html"><a href="intro-stat.html#two-sample-t-tests"><i class="fa fa-check"></i><b>1.5</b> Two-Sample t-tests</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="intro-stat.html"><a href="intro-stat.html#testnorm"><i class="fa fa-check"></i><b>1.5.1</b> Testing Normality of Groups</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-stat.html"><a href="intro-stat.html#ftest"><i class="fa fa-check"></i><b>1.5.2</b> Testing Equality of Variances</a></li>
<li class="chapter" data-level="1.5.3" data-path="intro-stat.html"><a href="intro-stat.html#tsttest"><i class="fa fa-check"></i><b>1.5.3</b> Testing Equality of Means</a></li>
<li class="chapter" data-level="1.5.4" data-path="intro-stat.html"><a href="intro-stat.html#python-code-3"><i class="fa fa-check"></i><b>1.5.4</b> Python Code</a></li>
<li class="chapter" data-level="1.5.5" data-path="intro-stat.html"><a href="intro-stat.html#wilcoxon"><i class="fa fa-check"></i><b>1.5.5</b> Mann-Whitney-Wilcoxon Test</a></li>
<li class="chapter" data-level="1.5.6" data-path="intro-stat.html"><a href="intro-stat.html#python-code-4"><i class="fa fa-check"></i><b>1.5.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="slr.html"><a href="slr.html"><i class="fa fa-check"></i><b>2</b> Introduction to ANOVA and Linear Regression</a>
<ul>
<li class="chapter" data-level="2.1" data-path="slr.html"><a href="slr.html#evp"><i class="fa fa-check"></i><b>2.1</b> Predictive vs. Explanatory</a></li>
<li class="chapter" data-level="2.2" data-path="slr.html"><a href="slr.html#trainvalidtest"><i class="fa fa-check"></i><b>2.2</b> Honest Assessment</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="slr.html"><a href="slr.html#python-code-5"><i class="fa fa-check"></i><b>2.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="slr.html"><a href="slr.html#bivariate-eda"><i class="fa fa-check"></i><b>2.3</b> Bivariate EDA</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="slr.html"><a href="slr.html#continuous-continuous-associations"><i class="fa fa-check"></i><b>2.3.1</b> Continuous-Continuous Associations</a></li>
<li class="chapter" data-level="2.3.2" data-path="slr.html"><a href="slr.html#continuous-categorical-associations"><i class="fa fa-check"></i><b>2.3.2</b> Continuous-Categorical Associations</a></li>
<li class="chapter" data-level="2.3.3" data-path="slr.html"><a href="slr.html#python-code-6"><i class="fa fa-check"></i><b>2.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="slr.html"><a href="slr.html#oneway"><i class="fa fa-check"></i><b>2.4</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="slr.html"><a href="slr.html#python-code-7"><i class="fa fa-check"></i><b>2.4.1</b> Python Code</a></li>
<li class="chapter" data-level="2.4.2" data-path="slr.html"><a href="slr.html#kruskal"><i class="fa fa-check"></i><b>2.4.2</b> Kruskal-Wallis</a></li>
<li class="chapter" data-level="2.4.3" data-path="slr.html"><a href="slr.html#python-code-8"><i class="fa fa-check"></i><b>2.4.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="slr.html"><a href="slr.html#posthoc"><i class="fa fa-check"></i><b>2.5</b> ANOVA Post-hoc Testing</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="slr.html"><a href="slr.html#tukey"><i class="fa fa-check"></i><b>2.5.1</b> Tukey-Kramer</a></li>
<li class="chapter" data-level="2.5.2" data-path="slr.html"><a href="slr.html#dunnett"><i class="fa fa-check"></i><b>2.5.2</b> Dunnett’s Test</a></li>
<li class="chapter" data-level="2.5.3" data-path="slr.html"><a href="slr.html#python-code-9"><i class="fa fa-check"></i><b>2.5.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="slr.html"><a href="slr.html#cor"><i class="fa fa-check"></i><b>2.6</b> Pearson Correlation</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="slr.html"><a href="slr.html#testcor"><i class="fa fa-check"></i><b>2.6.1</b> Statistical Test</a></li>
<li class="chapter" data-level="2.6.2" data-path="slr.html"><a href="slr.html#effect-of-anomalous-observations"><i class="fa fa-check"></i><b>2.6.2</b> Effect of Anomalous Observations</a></li>
<li class="chapter" data-level="2.6.3" data-path="slr.html"><a href="slr.html#the-correlation-matrix"><i class="fa fa-check"></i><b>2.6.3</b> The Correlation Matrix</a></li>
<li class="chapter" data-level="2.6.4" data-path="slr.html"><a href="slr.html#python-code-10"><i class="fa fa-check"></i><b>2.6.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="slr.html"><a href="slr.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.7</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="slr.html"><a href="slr.html#slrassumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions of Linear Regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="slr.html"><a href="slr.html#testing-for-association"><i class="fa fa-check"></i><b>2.7.2</b> Testing for Association</a></li>
<li class="chapter" data-level="2.7.3" data-path="slr.html"><a href="slr.html#python-code-11"><i class="fa fa-check"></i><b>2.7.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Complex ANOVA and Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#two-way-anova"><i class="fa fa-check"></i><b>3.1</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mlr.html"><a href="mlr.html#exploration"><i class="fa fa-check"></i><b>3.1.1</b> Exploration</a></li>
<li class="chapter" data-level="3.1.2" data-path="mlr.html"><a href="mlr.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
<li class="chapter" data-level="3.1.3" data-path="mlr.html"><a href="mlr.html#post-hoc-testing"><i class="fa fa-check"></i><b>3.1.3</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.1.4" data-path="mlr.html"><a href="mlr.html#python-code-12"><i class="fa fa-check"></i><b>3.1.4</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#two-way-anova-with-interactions"><i class="fa fa-check"></i><b>3.2</b> Two-Way ANOVA with Interactions</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#post-hoc-testing-1"><i class="fa fa-check"></i><b>3.2.1</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.2.2" data-path="mlr.html"><a href="mlr.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.2.3" data-path="mlr.html"><a href="mlr.html#python-code-13"><i class="fa fa-check"></i><b>3.2.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#randomized-block-design"><i class="fa fa-check"></i><b>3.3</b> Randomized Block Design</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="mlr.html"><a href="mlr.html#garlic-bulb-weight-example"><i class="fa fa-check"></i><b>3.3.1</b> Garlic Bulb Weight Example</a></li>
<li class="chapter" data-level="3.3.2" data-path="mlr.html"><a href="mlr.html#assumptions-1"><i class="fa fa-check"></i><b>3.3.2</b> Assumptions</a></li>
<li class="chapter" data-level="3.3.3" data-path="mlr.html"><a href="mlr.html#python-code-14"><i class="fa fa-check"></i><b>3.3.3</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Linear Regression</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model Structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="mlr.html"><a href="mlr.html#global-local-inference"><i class="fa fa-check"></i><b>3.4.2</b> Global &amp; Local Inference</a></li>
<li class="chapter" data-level="3.4.3" data-path="mlr.html"><a href="mlr.html#assumptions-2"><i class="fa fa-check"></i><b>3.4.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.4.4" data-path="mlr.html"><a href="mlr.html#multiple-coefficients-of-determination"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Coefficients of Determination</a></li>
<li class="chapter" data-level="3.4.5" data-path="mlr.html"><a href="mlr.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>3.4.5</b> Categorical Predictor Variables</a></li>
<li class="chapter" data-level="3.4.6" data-path="mlr.html"><a href="mlr.html#python-code-15"><i class="fa fa-check"></i><b>3.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>4</b> Model Selection</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-selection.html"><a href="model-selection.html#selection-criteria"><i class="fa fa-check"></i><b>4.1</b> Selection Criteria</a></li>
<li class="chapter" data-level="4.2" data-path="model-selection.html"><a href="model-selection.html#stepwise-selection"><i class="fa fa-check"></i><b>4.2</b> Stepwise Selection</a>
<ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward"><i class="fa fa-check"></i>Backward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#stepwise"><i class="fa fa-check"></i>Stepwise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-selection.html"><a href="model-selection.html#significance-levels"><i class="fa fa-check"></i><b>4.3</b> Significance Levels</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="model-selection.html"><a href="model-selection.html#python-code-16"><i class="fa fa-check"></i><b>4.3.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="diag.html"><a href="diag.html"><i class="fa fa-check"></i><b>5</b> Diagnostics</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="diag.html"><a href="diag.html#python-code-17"><i class="fa fa-check"></i><b>5.0.1</b> Python Code</a></li>
<li class="chapter" data-level="5.1" data-path="diag.html"><a href="diag.html#examining-residuals"><i class="fa fa-check"></i><b>5.1</b> Examining Residuals</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="diag.html"><a href="diag.html#python-code-18"><i class="fa fa-check"></i><b>5.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="diag.html"><a href="diag.html#misspecified-model"><i class="fa fa-check"></i><b>5.2</b> Misspecified Model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="diag.html"><a href="diag.html#python-code-19"><i class="fa fa-check"></i><b>5.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="diag.html"><a href="diag.html#constant-variance"><i class="fa fa-check"></i><b>5.3</b> Constant Variance</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="diag.html"><a href="diag.html#python-code-20"><i class="fa fa-check"></i><b>5.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="diag.html"><a href="diag.html#normality"><i class="fa fa-check"></i><b>5.4</b> Normality</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="diag.html"><a href="diag.html#python-code-21"><i class="fa fa-check"></i><b>5.4.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="diag.html"><a href="diag.html#correlated-errors"><i class="fa fa-check"></i><b>5.5</b> Correlated Errors</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="diag.html"><a href="diag.html#python-code-22"><i class="fa fa-check"></i><b>5.5.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="diag.html"><a href="diag.html#influential-observations-and-outliers"><i class="fa fa-check"></i><b>5.6</b> Influential Observations and Outliers</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="diag.html"><a href="diag.html#python-code-23"><i class="fa fa-check"></i><b>5.6.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="diag.html"><a href="diag.html#multicollinearity"><i class="fa fa-check"></i><b>5.7</b> Multicollinearity</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="diag.html"><a href="diag.html#python-code-24"><i class="fa fa-check"></i><b>5.7.1</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html"><i class="fa fa-check"></i><b>6</b> Model Building and Scoring for Prediction</a>
<ul>
<li class="chapter" data-level="6.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#regularized-regression"><i class="fa fa-check"></i><b>6.1</b> Regularized Regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#penalties-in-models"><i class="fa fa-check"></i><b>6.1.1</b> Penalties in Models</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#lasso"><i class="fa fa-check"></i><b>6.1.3</b> LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Elastic Net</a></li>
<li class="chapter" data-level="6.1.5" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-25"><i class="fa fa-check"></i><b>6.1.5</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#optimizing-penalties"><i class="fa fa-check"></i><b>6.2</b> Optimizing Penalties</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cross-validation"><i class="fa fa-check"></i><b>6.2.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#cv-in-regularized-regression"><i class="fa fa-check"></i><b>6.2.2</b> CV in Regularized Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-comparisons"><i class="fa fa-check"></i><b>6.3</b> Model Comparisons</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#model-metrics"><i class="fa fa-check"></i><b>6.3.1</b> Model Metrics</a></li>
<li class="chapter" data-level="6.3.2" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#test-dataset-comparison"><i class="fa fa-check"></i><b>6.3.2</b> Test Dataset Comparison</a></li>
<li class="chapter" data-level="6.3.3" data-path="model-building-and-scoring-for-prediction.html"><a href="model-building-and-scoring-for-prediction.html#python-code-26"><i class="fa fa-check"></i><b>6.3.3</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Categorical Data Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#describing-categorical-data"><i class="fa fa-check"></i><b>7.1</b> Describing Categorical Data</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-27"><i class="fa fa-check"></i><b>7.1.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#tests-of-association"><i class="fa fa-check"></i><b>7.2</b> Tests of Association</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-28"><i class="fa fa-check"></i><b>7.2.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#measures-of-association"><i class="fa fa-check"></i><b>7.3</b> Measures of Association</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-29"><i class="fa fa-check"></i><b>7.3.1</b> Python Code</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Introduction to Logistic Regression</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#linear-probability-model"><i class="fa fa-check"></i><b>7.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.4.2</b> Binary Logistic Regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#adding-categorical-variables"><i class="fa fa-check"></i><b>7.4.3</b> Adding Categorical Variables</a></li>
<li class="chapter" data-level="7.4.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#model-assessment"><i class="fa fa-check"></i><b>7.4.4</b> Model Assessment</a></li>
<li class="chapter" data-level="7.4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#variable-selection-and-regularized-regression"><i class="fa fa-check"></i><b>7.4.5</b> Variable Selection and Regularized Regression</a></li>
<li class="chapter" data-level="7.4.6" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#python-code-30"><i class="fa fa-check"></i><b>7.4.6</b> Python Code</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="slr" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction to ANOVA and Linear Regression<a href="slr.html#slr" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<pre><code>## C:\Users\sjsimmo2\AppData\Local\R\win-library\4.3\reticulate\python\rpytools\loader.py:117: UserWarning: Pandas requires version &#39;2.8.4&#39; or newer of &#39;numexpr&#39; (version &#39;2.8.3&#39; currently installed).
##   return _find_and_load(name, import_)
## C:\Users\sjsimmo2\AppData\Local\R\win-library\4.3\reticulate\python\rpytools\loader.py:117: UserWarning: Pandas requires version &#39;1.3.6&#39; or newer of &#39;bottleneck&#39; (version &#39;1.3.5&#39; currently installed).
##   return _find_and_load(name, import_)</code></pre>
<p>This Chapter aims to answer the following questions:</p>
<ul>
<li><p>What is a predictive model versus an explanatory model?</p></li>
<li><p>How to perform an honest assessment of a model.</p></li>
<li><p>How to estimate associations.</p>
<ul>
<li>Continuous-Continuous</li>
<li>Continuous-Categorical</li>
<li>Pearson’s correlation</li>
<li>Test of Hypothesis</li>
<li>Effect of outliers</li>
<li>Correlation Matrix</li>
</ul></li>
<li><p>How to perform ANOVA.</p>
<ul>
<li>Testing assumptions</li>
<li>Kruskal-Wallis</li>
<li>Post-hoc tests</li>
</ul></li>
<li><p>How to perform Simple Linear Regression.</p>
<ul>
<li>Assumptions</li>
<li>Inference</li>
</ul></li>
</ul>
<p>In this chapter, we introduce one of the most commonly used tools in data science: the linear model. We’ll start with some basic terminology. A <strong>linear model</strong> is an equation that typically takes the form
<span class="math display" id="eq:linmod">\[\begin{equation}
\mathbf{y} = \beta_0 + \beta_1\mathbf{x_1} + \dots + \beta_k\mathbf{x_k} + \boldsymbol \varepsilon
\tag{2.1}
\end{equation}\]</span></p>
<p>The left-hand side of this equation, <span class="math inline">\(\mathbf{y}\)</span> is equivalently called the <strong>target</strong>, <strong>response</strong>, or <strong>dependent</strong> variable. The right-hand side is a linear combination of the columns <span class="math inline">\(\{\mathbf{x_i}\}_{i=1}^{k}\)</span> which are commonly referred to as <strong>explanatory</strong>, <strong>input</strong>, <strong>predictor</strong>, or <strong>independent</strong> variables.</p>
<div id="evp" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Predictive vs. Explanatory<a href="slr.html#evp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The purpose of a linear model like Equation <a href="slr.html#eq:linmod">(2.1)</a> is generally two-fold:</p>
<ol style="list-style-type: decimal">
<li>The model is <strong>predictive</strong> in that it can estimate the value of <span class="math inline">\(y\)</span> for a given combination of the <span class="math inline">\(x\)</span> attributes.</li>
<li>The model is <strong>explanatory</strong> in that it can estimate how <span class="math inline">\(y\)</span> changes for a unit increase in a given <span class="math inline">\(x\)</span> attribute, holding all else constant (via the slope parameters <span class="math inline">\(\beta\)</span>).</li>
</ol>
<p>However, it’s common for <em>one</em> of these purposes to be more aligned with the specific goals of your project, and it is common to approach the building of such a model differently for each purpose.</p>
<p>In predictive modeling, you are most interested in how much error your model has on <em>holdout data</em>, that is, validation or test data. This is a notion that we introduce next in Section <a href="slr.html#trainvalidtest">2.2</a>. If good predictions are all you want from your model, you are unlikely to care how many variables (including polynomial and interaction terms) are included in the final model.</p>
<p>In explanatory modeling, you foremost want a model that is simple to interpret and doesn’t have too many input variables. It’s common to avoid many polynomial and interaction terms for explanatory models. While the error rates on holdout data will still be useful reporting metrics for explanatory models, it will be more important to craft the model for ease of interpretation.</p>
</div>
<div id="trainvalidtest" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Honest Assessment<a href="slr.html#trainvalidtest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When performing predictive or explanatory modeling, we <em>always</em> divide our data into subsets for training, validation, and/or final testing. Because models are prone to discovering small, spurious patterns on the data that is used to create them (the <em>training data</em>), we set aside the <em>validation</em> and <em>testing data</em> to get a clear view of how they might perform on new data that they’ve never seen before. This is a concept that will be revisited several times throughout this text, highlighting its importance to honest assessment of models.</p>
<p>There is no single right answer for how this division should occur for every data set - the answer depends on a multitude of factors that are beyond the scope of our present discussion. Generally speaking, one expects to keep about 70% of the data for model training purposes, and the remaining 30% for validation and testing. These proportions may change depending on the amount of data available. If one has millions of observations, they can often get away with a much smaller proportion of training data to reduce computation time and increase confidence in validation. If one has substantially fewer observations, it may be necessary to increase the training proportion in order to build a sound model - trading validation confidence for proper training.</p>
<p>Below we demonstrate one technique for separating the data into just two subsets: training and test. These two subsets will suffice for our analyses in this text. We’ll use 70% of our data for the training set and the remainder for testing.</p>
<p>Since we are taking a random sample, each time you run this functions you will get a different result. This can be difficult for team members who wish to keep their analyses in sync. To avoid that variation of results, we can provide a “seed” to the internal random number generation process, which ensures that the randomly generated output is the same to all who use that seed.</p>
<p>The following code demonstrates sampling via the <code>tidyverse</code>. This method requires the use of an id variable. If your data set has a unique identifier built in, you may omit the first line of code (after <code>set.seed()</code>) and use that unique identifier in the third line.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="slr.html#cb2-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-2"><a href="slr.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="slr.html#cb2-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-4"><a href="slr.html#cb2-4" tabindex="-1"></a></span>
<span id="cb2-5"><a href="slr.html#cb2-5" tabindex="-1"></a>ames <span class="ot">&lt;-</span> ames <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">id =</span> <span class="fu">row_number</span>())</span>
<span id="cb2-6"><a href="slr.html#cb2-6" tabindex="-1"></a></span>
<span id="cb2-7"><a href="slr.html#cb2-7" tabindex="-1"></a>train <span class="ot">&lt;-</span> ames <span class="sc">%&gt;%</span> <span class="fu">sample_frac</span>(<span class="fl">0.7</span>)</span>
<span id="cb2-8"><a href="slr.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="slr.html#cb2-9" tabindex="-1"></a>test <span class="ot">&lt;-</span> <span class="fu">anti_join</span>(ames, train, <span class="at">by =</span> <span class="st">&#39;id&#39;</span>)</span>
<span id="cb2-10"><a href="slr.html#cb2-10" tabindex="-1"></a></span>
<span id="cb2-11"><a href="slr.html#cb2-11" tabindex="-1"></a><span class="fu">dim</span>(train)</span></code></pre></div>
<pre><code>## [1] 2051   82</code></pre>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="slr.html#cb4-1" tabindex="-1"></a><span class="fu">dim</span>(test)</span></code></pre></div>
<pre><code>## [1] 879  82</code></pre>
<div id="python-code-5" class="section level3 hasAnchor" number="2.2.1">
<h3><span class="header-section-number">2.2.1</span> Python Code<a href="slr.html#python-code-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To create the training data set in Python</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="slr.html#cb6-1" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb6-2"><a href="slr.html#cb6-2" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb6-3"><a href="slr.html#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a href="slr.html#cb6-4" tabindex="-1"></a>train,test <span class="op">=</span> train_test_split(ames_py,test_size<span class="op">=</span><span class="fl">0.3</span>,random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb6-5"><a href="slr.html#cb6-5" tabindex="-1"></a>train.head()</span></code></pre></div>
<pre><code>##                               MS_SubClass  ...   Latitude
## 2278  One_Story_1946_and_Newer_All_Styles  ...  41.992318
## 1379  One_Story_1946_and_Newer_All_Styles  ...  42.031571
## 2182     PUD_Multilevel_Split_Level_Foyer  ...  42.018973
## 1436             Two_Story_1946_and_Newer  ...  42.017423
## 1599         Two_Story_PUD_1946_and_Newer  ...  41.992133
## 
## [5 rows x 81 columns]</code></pre>
<p>However, note that this will create a different train/test split. Therefore, we will just pull in our train/test data set from R.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="slr.html#cb8-1" tabindex="-1"></a>train <span class="op">=</span> r.train</span>
<span id="cb8-2"><a href="slr.html#cb8-2" tabindex="-1"></a>test <span class="op">=</span> r.test</span></code></pre></div>
</div>
</div>
<div id="bivariate-eda" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Bivariate EDA<a href="slr.html#bivariate-eda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As stated in Chapter <a href="intro-stat.html#intro-stat">1</a>, exploratory data analysis is the foundation of any successful data science project. As we move on to the discussion of modeling, we begin to explore <em>bivariate</em> relationships in our data. In doing so, we will often explore the input variables’ relationships with the target. <strong>Such exploration should only be done on the training data; we should never let insights from the validation or test data inform our decisions about modeling.</strong></p>
<p>Bivariate exploratory analysis is often used to assess relationships between two variables. An <strong>association</strong> or <strong>relationship</strong> exists when the expected value of one variable changes at different levels of the other variable. A <strong>linear relationship</strong> between two continuous variables can be inferred when the general shape of a scatter plot of the two variables is a straight line.</p>
<div id="continuous-continuous-associations" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> Continuous-Continuous Associations<a href="slr.html#continuous-continuous-associations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s conduct a preliminary assessment of the relationship between the size of the house in square feet (via <code>Gr_Liv_Area</code>) and the <code>Sale_Price</code> by creating a scatter plot (only on the training data). Note that we call this a preliminary assessment because we should not declare a statistical relationship without a formal hypothesis test (see Section <a href="slr.html#cor">2.6</a>).</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="slr.html#cb9-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> train) <span class="sc">+</span> </span>
<span id="cb9-2"><a href="slr.html#cb9-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Gr_Liv_Area, <span class="at">y =</span> Sale_Price<span class="sc">/</span><span class="dv">1000</span>)) <span class="sc">+</span></span>
<span id="cb9-3"><a href="slr.html#cb9-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Greater Living Area (Sqft)&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scatterplot"></span>
<img src="bookdownproj_files/figure-html/scatterplot-1.png" alt="Scatter plot demonstrating a positive linear relationship" width="672" />
<p class="caption">
Figure 2.1: Scatter plot demonstrating a positive linear relationship
</p>
</div>
</div>
<div id="continuous-categorical-associations" class="section level3 hasAnchor" number="2.3.2">
<h3><span class="header-section-number">2.3.2</span> Continuous-Categorical Associations<a href="slr.html#continuous-categorical-associations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ll also revisit the plots that we created in Section <a href="intro-stat.html#eda">1.1</a>, this time being careful to use only our training data since our goal is eventually to use a linear model to predict <code>Sale_Price</code>.</p>
<p>We start by exploring the relationship between the external quality rating of the home (via the ordinal variable <code>Exter_Qual</code> and the <code>Sale_Price</code>).</p>
<p>The simplest graphic we may wish to create is a bar chart like Figure <a href="slr.html#fig:barsale">2.2</a> that shows the average sale price of homes with each value of exterior quality.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="slr.html#cb10-1" tabindex="-1"></a><span class="fu">ggplot</span>(train) <span class="sc">+</span> </span>
<span id="cb10-2"><a href="slr.html#cb10-2" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="fu">aes</span>(<span class="at">x=</span>Exter_Qual,<span class="at">y=</span> Sale_Price), </span>
<span id="cb10-3"><a href="slr.html#cb10-3" tabindex="-1"></a>           <span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>, <span class="at">stat =</span> <span class="st">&quot;summary&quot;</span>, <span class="at">fun =</span> <span class="st">&quot;mean&quot;</span>) <span class="sc">+</span>                                      </span>
<span id="cb10-4"><a href="slr.html#cb10-4" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">labels =</span> <span class="cf">function</span>(x) <span class="fu">format</span>(x, <span class="at">scientific =</span> <span class="cn">FALSE</span>)) <span class="co"># Modify formatting of axis</span></span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:barsale"></span>
<img src="bookdownproj_files/figure-html/barsale-1.png" alt="Bar Chart Comparing Average Sale Price of Homes with each Level of Exterior Quality" width="672" />
<p class="caption">
Figure 2.2: Bar Chart Comparing Average Sale Price of Homes with each Level of Exterior Quality
</p>
</div>
<p>This gives us the idea that there may be an association between these two attributes, but it can be tricky to rely solely on this graph without exploring the overall distribution in sale price for each group. While this chart is great for the purposes of <em>reporting</em> (once we’ve verified the relationship), it’s not the best one for exploratory analysis. The next two charts allow us to have much more information on one graphic.</p>
<p>The frequency histogram in Figure <a href="slr.html#fig:overhistogram">2.3</a> allows us to see that much fewer of the homes have a rating of <code>Excellent</code> versus the other tiers, a fact that makes it difficult to compare the distributions. To normalize that quantity and compare the raw probability densities, we can change our axes to density (which is analogous to percentage) and employ a kernel density estimator with the <code>geom_density</code> plot as shown in Figure <a href="slr.html#fig:overhistogramdensitykernel">2.4</a>. We can then clearly see that as the exterior quality of the home “goes up” (in the ordinal sense, not in the linear sense), the sale price of the home also increases.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="slr.html#cb11-1" tabindex="-1"></a><span class="fu">ggplot</span>(train,<span class="fu">aes</span>(<span class="at">x=</span>Sale_Price<span class="sc">/</span><span class="dv">1000</span>, <span class="at">fill=</span>Exter_Qual)) <span class="sc">+</span> </span>
<span id="cb11-2"><a href="slr.html#cb11-2" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">alpha=</span><span class="fl">0.2</span>, <span class="at">position=</span><span class="st">&quot;identity&quot;</span>) <span class="sc">+</span> </span>
<span id="cb11-3"><a href="slr.html#cb11-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>) </span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overhistogram"></span>
<img src="bookdownproj_files/figure-html/overhistogram-1.png" alt="Histogram: Frequency of Sale_Price for varying qualities of home exterior" width="672" />
<p class="caption">
Figure 2.3: Histogram: Frequency of Sale_Price for varying qualities of home exterior
</p>
</div>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="slr.html#cb13-1" tabindex="-1"></a><span class="fu">ggplot</span>(ames,<span class="fu">aes</span>(<span class="at">x=</span>Sale_Price<span class="sc">/</span><span class="dv">1000</span>, <span class="at">fill=</span>Exter_Qual)) <span class="sc">+</span> </span>
<span id="cb13-2"><a href="slr.html#cb13-2" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">alpha=</span><span class="fl">0.2</span>, <span class="at">position=</span><span class="st">&quot;identity&quot;</span>) <span class="sc">+</span> </span>
<span id="cb13-3"><a href="slr.html#cb13-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:overhistogramdensitykernel"></span>
<img src="bookdownproj_files/figure-html/overhistogramdensitykernel-1.png" alt="Histogram: Density of Sale_Price for varying qualities of home exterior" width="672" />
<p class="caption">
Figure 2.4: Histogram: Density of Sale_Price for varying qualities of home exterior
</p>
</div>
<p>To further explore the location and spread of the data, we can create box-plots for each group using the following code:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="slr.html#cb14-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> train, <span class="fu">aes</span>(<span class="at">y =</span> Sale_Price<span class="sc">/</span><span class="dv">1000</span>, <span class="at">x =</span> <span class="st">`</span><span class="at">Exter_Qual</span><span class="st">`</span>, <span class="at">fill =</span> <span class="st">`</span><span class="at">Exter_Qual</span><span class="st">`</span>)) <span class="sc">+</span></span>
<span id="cb14-2"><a href="slr.html#cb14-2" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb14-3"><a href="slr.html#cb14-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Sales Price (Thousands $)&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Exterior Quality Category&quot;</span>) <span class="sc">+</span></span>
<span id="cb14-4"><a href="slr.html#cb14-4" tabindex="-1"></a>  <span class="fu">stat_summary</span>(<span class="at">fun =</span> mean, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">shape =</span> <span class="dv">20</span>, <span class="at">size =</span> <span class="dv">5</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb14-5"><a href="slr.html#cb14-5" tabindex="-1"></a>  <span class="fu">scale_fill_brewer</span>(<span class="at">palette=</span><span class="st">&quot;Blues&quot;</span>) <span class="sc">+</span> <span class="fu">theme_classic</span>() <span class="sc">+</span> <span class="fu">coord_flip</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multiboxplot"></span>
<img src="bookdownproj_files/figure-html/multiboxplot-1.png" alt="Box Plots of Sale_Price for each level of Exter_Qual" width="672" />
<p class="caption">
Figure 2.5: Box Plots of Sale_Price for each level of Exter_Qual
</p>
</div>
<p>Notice that we’ve highlighted the mean on each box-plot for the purposes of comparison. We now have a hypothesis that we may want to formally test. After all, it is not good practice to look at Figures <a href="slr.html#fig:overhistogramdensitykernel">2.4</a> and <a href="slr.html#fig:multiboxplot">2.5</a> and <em>declare</em> that a statistical difference exists. While we do, over time, get a feel for which visually apparent relationships turn out to be statistically significant, it’s <em>imperative</em> that we conduct formal testing before declaring such insights to a colleague or stakeholder.</p>
<p>If we want to test whether the <code>Sale_Price</code> is different for the different values of <code>Exter_Qual</code>, we have to reach for the multi-group alternative to the 2-sample t-test. This is called <strong>Analysis of Variance</strong>, or <strong>ANOVA</strong> for short.</p>
</div>
<div id="python-code-6" class="section level3 hasAnchor" number="2.3.3">
<h3><span class="header-section-number">2.3.3</span> Python Code<a href="slr.html#python-code-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For Continuous-Continuous Associations:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="slr.html#cb15-1" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> gaussian_kde</span>
<span id="cb15-2"><a href="slr.html#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="slr.html#cb15-3" tabindex="-1"></a>plt.scatter(train[<span class="st">&#39;Gr_Liv_Area&#39;</span>], [price<span class="op">/</span><span class="dv">1000</span> <span class="cf">for</span> price <span class="kw">in</span> train[<span class="st">&#39;Sale_Price&#39;</span>]])</span>
<span id="cb15-4"><a href="slr.html#cb15-4" tabindex="-1"></a></span>
<span id="cb15-5"><a href="slr.html#cb15-5" tabindex="-1"></a><span class="co"># Setting labels</span></span>
<span id="cb15-6"><a href="slr.html#cb15-6" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;Greater Living Area (Sqft)&#39;</span>)</span>
<span id="cb15-7"><a href="slr.html#cb15-7" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;Sales Price (Thousands $)&#39;</span>)</span>
<span id="cb15-8"><a href="slr.html#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="slr.html#cb15-9" tabindex="-1"></a><span class="co"># Showing the plot</span></span>
<span id="cb15-10"><a href="slr.html#cb15-10" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>For Continuous-Categorical Associations:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="slr.html#cb16-1" tabindex="-1"></a><span class="co"># Calculate mean Sale Price for each Exterior Quality</span></span>
<span id="cb16-2"><a href="slr.html#cb16-2" tabindex="-1"></a>train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">=</span> train[<span class="st">&#39;Exter_Qual&#39;</span>].cat.remove_unused_categories()</span>
<span id="cb16-3"><a href="slr.html#cb16-3" tabindex="-1"></a></span>
<span id="cb16-4"><a href="slr.html#cb16-4" tabindex="-1"></a>mean_prices <span class="op">=</span> train.groupby(<span class="st">&#39;Exter_Qual&#39;</span>)[<span class="st">&#39;Sale_Price&#39;</span>].mean()</span>
<span id="cb16-5"><a href="slr.html#cb16-5" tabindex="-1"></a></span>
<span id="cb16-6"><a href="slr.html#cb16-6" tabindex="-1"></a><span class="co"># Plotting the data</span></span></code></pre></div>
<pre><code>## &lt;string&gt;:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.</code></pre>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="slr.html#cb18-1" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb18-2"><a href="slr.html#cb18-2" tabindex="-1"></a>mean_prices.plot(kind<span class="op">=</span><span class="st">&#39;bar&#39;</span>, ax<span class="op">=</span>ax)</span>
<span id="cb18-3"><a href="slr.html#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a href="slr.html#cb18-4" tabindex="-1"></a><span class="co"># Setting labels</span></span>
<span id="cb18-5"><a href="slr.html#cb18-5" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Sales Price (Thousands $)&#39;</span>)</span>
<span id="cb18-6"><a href="slr.html#cb18-6" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Exterior Quality&#39;</span>)</span>
<span id="cb18-7"><a href="slr.html#cb18-7" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Average Sale Price by Exterior Quality&#39;</span>)</span>
<span id="cb18-8"><a href="slr.html#cb18-8" tabindex="-1"></a></span>
<span id="cb18-9"><a href="slr.html#cb18-9" tabindex="-1"></a><span class="co"># Displaying the plot</span></span>
<span id="cb18-10"><a href="slr.html#cb18-10" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-7-3.png" width="672" /></p>
<div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="slr.html#cb19-1" tabindex="-1"></a>ames_py[<span class="st">&#39;Sale_Price2&#39;</span>] <span class="op">=</span> ames_py[<span class="st">&#39;Sale_Price&#39;</span>] <span class="op">/</span> <span class="dv">1000</span></span>
<span id="cb19-2"><a href="slr.html#cb19-2" tabindex="-1"></a></span>
<span id="cb19-3"><a href="slr.html#cb19-3" tabindex="-1"></a><span class="co"># Get unique categories for coloring</span></span>
<span id="cb19-4"><a href="slr.html#cb19-4" tabindex="-1"></a>unique_categories <span class="op">=</span> ames_py[<span class="st">&#39;Exter_Qual&#39;</span>].unique()</span>
<span id="cb19-5"><a href="slr.html#cb19-5" tabindex="-1"></a>colors <span class="op">=</span> plt.cm.viridis(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(unique_categories)))</span>
<span id="cb19-6"><a href="slr.html#cb19-6" tabindex="-1"></a></span>
<span id="cb19-7"><a href="slr.html#cb19-7" tabindex="-1"></a><span class="co"># Plotting the data</span></span>
<span id="cb19-8"><a href="slr.html#cb19-8" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb19-9"><a href="slr.html#cb19-9" tabindex="-1"></a></span>
<span id="cb19-10"><a href="slr.html#cb19-10" tabindex="-1"></a><span class="cf">for</span> category, color <span class="kw">in</span> <span class="bu">zip</span>(unique_categories, colors):</span>
<span id="cb19-11"><a href="slr.html#cb19-11" tabindex="-1"></a>    subset <span class="op">=</span> ames_py[ames_py[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> category]</span>
<span id="cb19-12"><a href="slr.html#cb19-12" tabindex="-1"></a>    ax.hist(subset[<span class="st">&#39;Sale_Price2&#39;</span>], bins<span class="op">=</span><span class="dv">10</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span>category, color<span class="op">=</span>color)</span>
<span id="cb19-13"><a href="slr.html#cb19-13" tabindex="-1"></a></span>
<span id="cb19-14"><a href="slr.html#cb19-14" tabindex="-1"></a><span class="co"># Setting labels</span></span></code></pre></div>
<pre><code>## (array([ 11., 151., 602., 699., 233.,  71.,  23.,   4.,   2.,   3.]), array([ 12.789 ,  53.0101,  93.2312, 133.4523, 173.6734, 213.8945,
##        254.1156, 294.3367, 334.5578, 374.7789, 415.    ]), &lt;BarContainer object of 10 artists&gt;)
## (array([ 18., 293., 399., 197.,  55.,  21.,   2.,   2.,   1.,   1.]), array([ 52. , 121.3, 190.6, 259.9, 329.2, 398.5, 467.8, 537.1, 606.4,
##        675.7, 745. ]), &lt;BarContainer object of 10 artists&gt;)
## (array([ 6., 10., 28., 26., 14., 12.,  5.,  5.,  0.,  1.]), array([160. , 219.5, 279. , 338.5, 398. , 457.5, 517. , 576.5, 636. ,
##        695.5, 755. ]), &lt;BarContainer object of 10 artists&gt;)
## (array([1., 3., 9., 5., 6., 6., 3., 0., 1., 1.]), array([ 13.1 ,  31.79,  50.48,  69.17,  87.86, 106.55, 125.24, 143.93,
##        162.62, 181.31, 200.  ]), &lt;BarContainer object of 10 artists&gt;)</code></pre>
<div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="slr.html#cb21-1" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Sales Price (Thousands $)&#39;</span>)</span>
<span id="cb21-2"><a href="slr.html#cb21-2" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Frequency&#39;</span>)</span>
<span id="cb21-3"><a href="slr.html#cb21-3" tabindex="-1"></a>ax.legend(title<span class="op">=</span><span class="st">&#39;Exter_Qual&#39;</span>)</span>
<span id="cb21-4"><a href="slr.html#cb21-4" tabindex="-1"></a></span>
<span id="cb21-5"><a href="slr.html#cb21-5" tabindex="-1"></a><span class="co"># Displaying the plot</span></span>
<span id="cb21-6"><a href="slr.html#cb21-6" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-8-5.png" width="672" /></p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="slr.html#cb22-1" tabindex="-1"></a><span class="co"># Get unique categories for coloring</span></span>
<span id="cb22-2"><a href="slr.html#cb22-2" tabindex="-1"></a>unique_categories <span class="op">=</span> ames_py[<span class="st">&#39;Exter_Qual&#39;</span>].unique()</span>
<span id="cb22-3"><a href="slr.html#cb22-3" tabindex="-1"></a>colors <span class="op">=</span> plt.cm.viridis(np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="bu">len</span>(unique_categories)))</span>
<span id="cb22-4"><a href="slr.html#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a href="slr.html#cb22-5" tabindex="-1"></a><span class="co"># Plotting the data</span></span>
<span id="cb22-6"><a href="slr.html#cb22-6" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb22-7"><a href="slr.html#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="slr.html#cb22-8" tabindex="-1"></a><span class="cf">for</span> category, color <span class="kw">in</span> <span class="bu">zip</span>(unique_categories, colors):</span>
<span id="cb22-9"><a href="slr.html#cb22-9" tabindex="-1"></a>    subset <span class="op">=</span> ames_py[ames_py[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> category]</span>
<span id="cb22-10"><a href="slr.html#cb22-10" tabindex="-1"></a>    kde <span class="op">=</span> gaussian_kde(subset[<span class="st">&#39;Sale_Price2&#39;</span>])</span>
<span id="cb22-11"><a href="slr.html#cb22-11" tabindex="-1"></a>    x <span class="op">=</span> np.linspace(subset[<span class="st">&#39;Sale_Price2&#39;</span>].<span class="bu">min</span>(), subset[<span class="st">&#39;Sale_Price&#39;</span>].<span class="bu">max</span>(), <span class="dv">1000</span>)</span>
<span id="cb22-12"><a href="slr.html#cb22-12" tabindex="-1"></a>    y <span class="op">=</span> kde(x)</span>
<span id="cb22-13"><a href="slr.html#cb22-13" tabindex="-1"></a>    ax.fill_between(x, y, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span>category, color<span class="op">=</span>color)</span>
<span id="cb22-14"><a href="slr.html#cb22-14" tabindex="-1"></a></span>
<span id="cb22-15"><a href="slr.html#cb22-15" tabindex="-1"></a><span class="co"># Setting labels</span></span>
<span id="cb22-16"><a href="slr.html#cb22-16" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Sales Price (Thousands $)&#39;</span>)</span>
<span id="cb22-17"><a href="slr.html#cb22-17" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Density&#39;</span>)</span>
<span id="cb22-18"><a href="slr.html#cb22-18" tabindex="-1"></a>ax.legend(title<span class="op">=</span><span class="st">&#39;Exter_Qual&#39;</span>)</span>
<span id="cb22-19"><a href="slr.html#cb22-19" tabindex="-1"></a></span>
<span id="cb22-20"><a href="slr.html#cb22-20" tabindex="-1"></a><span class="co"># Displaying the plot</span></span>
<span id="cb22-21"><a href="slr.html#cb22-21" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-9-7.png" width="672" /></p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="slr.html#cb23-1" tabindex="-1"></a><span class="co"># Plotting the data</span></span>
<span id="cb23-2"><a href="slr.html#cb23-2" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb23-3"><a href="slr.html#cb23-3" tabindex="-1"></a></span>
<span id="cb23-4"><a href="slr.html#cb23-4" tabindex="-1"></a><span class="co"># Create box plot</span></span>
<span id="cb23-5"><a href="slr.html#cb23-5" tabindex="-1"></a>ames_py.boxplot(column<span class="op">=</span><span class="st">&#39;Sale_Price&#39;</span>, by<span class="op">=</span><span class="st">&#39;Exter_Qual&#39;</span>, ax<span class="op">=</span>ax)</span>
<span id="cb23-6"><a href="slr.html#cb23-6" tabindex="-1"></a></span>
<span id="cb23-7"><a href="slr.html#cb23-7" tabindex="-1"></a><span class="co"># Set labels</span></span>
<span id="cb23-8"><a href="slr.html#cb23-8" tabindex="-1"></a>ax.set_xlabel(<span class="st">&#39;Exter_Qual&#39;</span>)</span>
<span id="cb23-9"><a href="slr.html#cb23-9" tabindex="-1"></a>ax.set_ylabel(<span class="st">&#39;Sale Price&#39;</span>)</span>
<span id="cb23-10"><a href="slr.html#cb23-10" tabindex="-1"></a>ax.set_title(<span class="st">&#39;Sale Price by Exterior Quality&#39;</span>)</span>
<span id="cb23-11"><a href="slr.html#cb23-11" tabindex="-1"></a></span>
<span id="cb23-12"><a href="slr.html#cb23-12" tabindex="-1"></a><span class="co"># Remove the automatic &#39;Boxplot grouped by Exter_Qual&#39; title</span></span>
<span id="cb23-13"><a href="slr.html#cb23-13" tabindex="-1"></a>plt.suptitle(<span class="st">&#39;&#39;</span>)</span>
<span id="cb23-14"><a href="slr.html#cb23-14" tabindex="-1"></a></span>
<span id="cb23-15"><a href="slr.html#cb23-15" tabindex="-1"></a><span class="co"># Display the plot</span></span>
<span id="cb23-16"><a href="slr.html#cb23-16" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-10-9.png" width="672" /></p>
</div>
</div>
<div id="oneway" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> One-Way ANOVA<a href="slr.html#oneway" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>One-way ANOVA</strong> aims to determine whether there is a difference in the mean of a continuous attribute across levels of a categorical attribute. Sound like a two-sample t-test? Indeed, it’s the extension of that test to more than two groups. Performing ANOVA with a binary input variable is mathematically identical to the two-sample t-test, assuming the variances are equal:</p>
<ol style="list-style-type: decimal">
<li>The observations are independent</li>
<li>The model residuals are normally distributed</li>
<li>The variances for each group are equal</li>
</ol>
<p>A one-way ANOVA refers to a single hypothesis test, which is <span class="math inline">\(H_{0}: \mu_{1}=\mu_{2}=...\mu_{k}\)</span> for a predictor variable with <span class="math inline">\(k\)</span> levels against the alternative of at least one difference. We will go back to the cars data set. There is another cars data set called “cars2” that adds cars from Germany. We can visualize this information via boxplots and density plots:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="slr.html#cb24-1" tabindex="-1"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb24-2"><a href="slr.html#cb24-2" tabindex="-1"></a></span>
<span id="cb24-3"><a href="slr.html#cb24-3" tabindex="-1"></a>cars2<span class="ot">&lt;-</span><span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/cars2.csv&quot;</span>)</span>
<span id="cb24-4"><a href="slr.html#cb24-4" tabindex="-1"></a></span>
<span id="cb24-5"><a href="slr.html#cb24-5" tabindex="-1"></a><span class="fu">ggboxplot</span>(cars2,<span class="at">x=</span><span class="st">&quot;Country&quot;</span>,<span class="at">y=</span><span class="st">&quot;MPG&quot;</span>, <span class="at">add=</span><span class="st">&quot;mean&quot;</span>,<span class="at">color=</span><span class="st">&quot;Country&quot;</span>,<span class="at">fill=</span><span class="st">&quot;Country&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.
## ℹ Please use the `fun` argument instead.
## ℹ The deprecated feature was likely used in the ggpubr package.
##   Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<pre><code>## Warning: The `fun.ymin` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.
## ℹ Please use the `fun.min` argument instead.
## ℹ The deprecated feature was likely used in the ggpubr package.
##   Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<pre><code>## Warning: The `fun.ymax` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.
## ℹ Please use the `fun.max` argument instead.
## ℹ The deprecated feature was likely used in the ggpubr package.
##   Please report the issue at &lt;https://github.com/kassambara/ggpubr/issues&gt;.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was
## generated.</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-11-11.png" width="672" /></p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="slr.html#cb28-1" tabindex="-1"></a><span class="fu">ggdensity</span>(cars2, <span class="at">x =</span> <span class="st">&quot;MPG&quot;</span>,</span>
<span id="cb28-2"><a href="slr.html#cb28-2" tabindex="-1"></a>           <span class="at">add =</span> <span class="st">&quot;mean&quot;</span>, <span class="at">rug =</span> <span class="cn">TRUE</span>,</span>
<span id="cb28-3"><a href="slr.html#cb28-3" tabindex="-1"></a>           <span class="at">color =</span> <span class="st">&quot;Country&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;Country&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-11-12.png" width="672" /></p>
<p>To test that the means are equal, we can use the function aov. However, we must first assess the assumptions (normality within each group and variances are equal). We can assess the normality assumption by using a QQ-plot on the residuals:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="slr.html#cb29-1" tabindex="-1"></a>model<span class="ot">&lt;-</span><span class="fu">aov</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country), <span class="at">data=</span>cars2)</span>
<span id="cb29-2"><a href="slr.html#cb29-2" tabindex="-1"></a><span class="fu">ggqqplot</span>(<span class="fu">residuals</span>(model))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Based on residual plot, Normality appears to be a reasonable assumption.</p>
<p>We can also perform an hypothesis test for Normality on the residuals:</p>
<p><span class="math display">\[ H_{0}: The \; residuals \;  are\;  Normally\;  distributed \]</span>
<span class="math display">\[ H_{A}: The\;residuals\;are\;not \; Normally \;distributed\]</span></p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="slr.html#cb30-1" tabindex="-1"></a><span class="fu">shapiro.test</span>(<span class="fu">residuals</span>(model))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(model)
## W = 0.99322, p-value = 0.05124</code></pre>
<p>Same conclusion as QQ-plot (although, they can differ!!).</p>
<p>Yes, we do have to run the model first to get the residuals, but we should NOT look at the output of the model until we are comforable that the assumptions hold. Since Normality is fine here, we move on to testing the variances are equal using the Levene test. The hypothesis test is:</p>
<p><span class="math display">\[ H_0: \sigma_a^2 =\sigma_b^2 =\sigma_c^2=\sigma_d^2 \quad  \text{i.e., the groups have equal variance}\]</span>
<span class="math display">\[H_a: \text{at least one group&#39;s variance is different}\]</span></p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="slr.html#cb32-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb32-2"><a href="slr.html#cb32-2" tabindex="-1"></a><span class="fu">library</span>(stats)</span>
<span id="cb32-3"><a href="slr.html#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a href="slr.html#cb32-4" tabindex="-1"></a><span class="fu">leveneTest</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country),<span class="at">data=</span>cars2) <span class="co"># Most popular, but depends on Normality</span></span></code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##        Df F value  Pr(&gt;F)   
## group   2  6.2318 0.00215 **
##       425                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>According to the Levene test, we reject the null hypothesis and would conclude that at least one variance is significantly different. We can also look at the residual plot.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="slr.html#cb34-1" tabindex="-1"></a><span class="fu">ggplot</span>(model, <span class="fu">aes</span>(.fitted, .resid)) <span class="sc">+</span></span>
<span id="cb34-2"><a href="slr.html#cb34-2" tabindex="-1"></a>     <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb34-3"><a href="slr.html#cb34-3" tabindex="-1"></a>     <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>)<span class="sc">+</span></span>
<span id="cb34-4"><a href="slr.html#cb34-4" tabindex="-1"></a>     <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&#39;Residual vs. Fitted Values Plot&#39;</span>, <span class="at">x=</span><span class="st">&#39;Fitted Values&#39;</span>, <span class="at">y=</span><span class="st">&#39;Residuals&#39;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>IF we were able to FAIL TO REJECT this hypothesis and felt comfortable, we could do the ANOVA:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb35-1"><a href="slr.html#cb35-1" tabindex="-1"></a>model<span class="ot">&lt;-</span><span class="fu">aov</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country), <span class="at">data=</span>cars2)</span>
<span id="cb35-2"><a href="slr.html#cb35-2" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>##                  Df Sum Sq Mean Sq F value Pr(&gt;F)    
## factor(Country)   2   8997    4498   172.2 &lt;2e-16 ***
## Residuals       425  11105      26                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Our conclusion would be to reject the null hypothesis and assume that there is at least one mean that is significantly different. But remember, this test is NOT accurate since our variance are not the same.</p>
<p>A more appropriate test would be to perform the Welch’s One-way ANOVA. Just as with testing two means, Welch also did an extension to the ANOVA with unequal variances (this does assume Normality or at least not a too big of a departure from Normality):</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="slr.html#cb37-1" tabindex="-1"></a><span class="fu">oneway.test</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country), <span class="at">data =</span> cars2, <span class="at">var.equal =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## 
##  One-way analysis of means (not assuming equal variances)
## 
## data:  MPG and factor(Country)
## F = 174.97, num df = 2.0, denom df = 177.3, p-value &lt; 2.2e-16</code></pre>
<p>This is still testing if there is a significant difference in the means, however, it is designed for unequal variances. We arrive at the same conclusion here…there is at least one significant difference in the mean MPG for cars made in US, Japan and Germany.</p>
<p>Although a one-way ANOVA is designed to assess whether or not there is a significant difference within the mean values of the response with respect to the different levels of the predictor variable, we can draw some parallel to the regression model. For example, if we have <span class="math inline">\(k\)</span>=4, then we can let <span class="math inline">\(x_a\)</span>, <span class="math inline">\(x_b\)</span>, and <span class="math inline">\(x_c\)</span> be 3 reference-coded dummy variables for the levels: <code>a</code>, <code>b</code>, <code>c</code>, and <code>d</code>. Note that we only have 3 dummy variables because one gets left out for the reference level, in this case it is <code>d</code>. The linear model is of the following form:</p>
<p><span class="math display" id="eq:anova1">\[\begin{equation}
y=\beta_0 + \beta_ax_a+\beta_bx_b+\beta_cx_c + \varepsilon \tag{2.2}
\end{equation}\]</span></p>
<p>If we define <span class="math inline">\(x_a\)</span> as 1 if the observation belongs to level <code>a</code> and 0 otherwise, and the same definition for <span class="math inline">\(x_b\)</span> and <span class="math inline">\(x_c\)</span>, then this is called <em>reference-level coding</em> (this will change for effects-level coding). The predicted values in <a href="slr.html#eq:anova1">(2.2)</a> is basically the predicted mean of the response within the 4 levels of the predictor variable.</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> represents the mean of reference group, group <code>d</code>.</li>
<li><span class="math inline">\(\beta_a, \beta_b, \beta_c\)</span> all represent the <em>difference</em> in the respective group means compared to the reference level. Positive values thus reflect a group mean that is higher than the reference group, and negative values reflect a group mean lower than the reference group.</li>
<li><span class="math inline">\(\varepsilon\)</span> is called the <strong>error</strong>.</li>
</ul>
<p>A <em>one-way</em> ANOVA model only contains a single input variable of interest. Equation <a href="slr.html#eq:anova1">(2.2)</a>, while it has 3 dummy variable inputs, only contains a single nominal attribute. In <a href="mlr.html#mlr">3</a>, we will add more inputs to the equation via two-way ANOVA and multivariate regression models.</p>
<p>ANOVA is used to test the following hypothesis:
<span class="math display">\[H_0: \beta_a=\beta_b=\beta_c = 0 \quad\text{(i.e. all group means are equal)}\]</span>
<span class="math display">\[H_0: \beta_a\neq0\mbox{ or }\beta_b\neq0 \mbox{ or } \beta_c \neq 0 \quad\text{(i.e. at least one is different)}\]</span>
Both the <code>lm()</code> function and the <code>aov()</code> function will provide the p-values to test the hypothesis above, the only difference between the two functions is that <code>lm()</code> will also provide the user with the coefficient of determinination, <span class="math inline">\(R^2\)</span>, which tells you how much of the variation in <span class="math inline">\(y\)</span> is accounted for by your categorical input.</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="slr.html#cb39-1" tabindex="-1"></a>cars_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(MPG <span class="sc">~</span> <span class="fu">factor</span>(Country), <span class="at">data =</span> cars2)</span>
<span id="cb39-2"><a href="slr.html#cb39-2" tabindex="-1"></a><span class="fu">summary</span>(cars_lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = MPG ~ factor(Country), data = cars2)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.1647  -3.4900  -0.1647   2.8353  16.8353 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              28.4900     0.5112  55.735   &lt;2e-16 ***
## factor(Country)Japanese   1.9910     0.7694   2.588     0.01 ** 
## factor(Country)US        -8.3253     0.6052 -13.757   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.112 on 425 degrees of freedom
## Multiple R-squared:  0.4476, Adjusted R-squared:  0.445 
## F-statistic: 172.2 on 2 and 425 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can also confirm what we know about the predictions from ANOVA, that there are only <span class="math inline">\(k\)</span> unique predictions from an ANOVA with <span class="math inline">\(k\)</span> groups (the predictions being the group means), using the <code>predict</code> function.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="slr.html#cb41-1" tabindex="-1"></a>cars2<span class="sc">$</span>predict <span class="ot">&lt;-</span> <span class="fu">predict</span>(cars_lm, <span class="at">data =</span> cars2)</span>
<span id="cb41-2"><a href="slr.html#cb41-2" tabindex="-1"></a></span>
<span id="cb41-3"><a href="slr.html#cb41-3" tabindex="-1"></a>cars2<span class="sc">$</span>resid_anova <span class="ot">&lt;-</span> <span class="fu">resid</span>(cars_lm, <span class="at">data =</span> cars2)</span>
<span id="cb41-4"><a href="slr.html#cb41-4" tabindex="-1"></a></span>
<span id="cb41-5"><a href="slr.html#cb41-5" tabindex="-1"></a>model_output<span class="ot">&lt;-</span>cars2 <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">select</span>(Country, predict, resid_anova)</span>
<span id="cb41-6"><a href="slr.html#cb41-6" tabindex="-1"></a></span>
<span id="cb41-7"><a href="slr.html#cb41-7" tabindex="-1"></a><span class="fu">rbind</span>(model_output[<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>,],model_output[<span class="dv">255</span><span class="sc">:</span><span class="dv">257</span>,],model_output[<span class="dv">375</span><span class="sc">:</span><span class="dv">377</span>,])</span></code></pre></div>
<pre><code>##      Country  predict resid_anova
## 1         US 20.16466  -13.164659
## 2         US 20.16466  -12.164659
## 3         US 20.16466  -12.164659
## 255 Japanese 30.48101    4.518987
## 256 Japanese 30.48101   -6.481013
## 257 Japanese 30.48101  -11.481013
## 375  Germany 28.49000   -9.490000
## 376  Germany 28.49000   -2.490000
## 377  Germany 28.49000    8.510000</code></pre>
<p>A non-parametric version of the ANOVA test, the Kruskal-Wallis test, is also available. This test does NOT assume normality, but does need <em>similar</em> distributions among the different levels. Non-parametric tests do not have the same <em>statistical power</em> to detect differences between groups. <strong>Statistical power</strong> is the probability of detecting an effect, if there is a true effect present to detect. We should opt for these tests in situations where our data is ordinal or otherwise violates the assumptions of normality in ways that cannot be fixed by logarithmic or other similar transformation.</p>
<div id="python-code-7" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Python Code<a href="slr.html#python-code-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using statsmodels (linear regression approach to ANOVA):</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="slr.html#cb43-1" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb43-2"><a href="slr.html#cb43-2" tabindex="-1"></a></span>
<span id="cb43-3"><a href="slr.html#cb43-3" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ C(Exter_Qual)&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb43-4"><a href="slr.html#cb43-4" tabindex="-1"></a>model.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.507</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.506</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   701.8</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 17 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>1.00e-313</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:51:01</td>     <th>  Log-Likelihood:    </th> <td> -25346.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.070e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2047</td>      <th>  BIC:               </th> <td>5.072e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>                  <td> 8.422e+04</td> <td> 1.07e+04</td> <td>    7.905</td> <td> 0.000</td> <td> 6.33e+04</td> <td> 1.05e+05</td>
</tr>
<tr>
  <th>C(Exter_Qual)[T.Typical]</th>   <td> 5.789e+04</td> <td> 1.08e+04</td> <td>    5.374</td> <td> 0.000</td> <td> 3.68e+04</td> <td>  7.9e+04</td>
</tr>
<tr>
  <th>C(Exter_Qual)[T.Good]</th>      <td> 1.447e+05</td> <td> 1.09e+04</td> <td>   13.310</td> <td> 0.000</td> <td> 1.23e+05</td> <td> 1.66e+05</td>
</tr>
<tr>
  <th>C(Exter_Qual)[T.Excellent]</th> <td> 2.917e+05</td> <td> 1.24e+04</td> <td>   23.486</td> <td> 0.000</td> <td> 2.67e+05</td> <td> 3.16e+05</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>698.567</td> <th>  Durbin-Watson:     </th> <td>   2.057</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5159.626</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 1.404</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td>10.245</td>  <th>  Cond. No.          </th> <td>    21.4</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="slr.html#cb44-1" tabindex="-1"></a>sm.api.stats.anova_lm(model, typ<span class="op">=</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>##                      sum_sq      df           F         PR(&gt;F)
## C(Exter_Qual)  6.691308e+12     3.0  701.831849  1.000132e-313
## Residual       6.505408e+12  2047.0         NaN            NaN</code></pre>
<p>Another way of doing it in statsmodels:</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="slr.html#cb46-1" tabindex="-1"></a>sm.stats.oneway.anova_oneway(data <span class="op">=</span> train[<span class="st">&#39;Sale_Price&#39;</span>], groups <span class="op">=</span> train[<span class="st">&#39;Exter_Qual&#39;</span>], use_var <span class="op">=</span> <span class="st">&#39;equal&#39;</span>, welch_correction <span class="op">=</span> <span class="va">False</span>)</span></code></pre></div>
<pre><code>## &lt;class &#39;statsmodels.stats.base.HolderTuple&#39;&gt;
## statistic = 701.8318492122419
## pvalue = 1.00013222764e-313
## df = (3.0, 2047.0)
## df_num = 3.0
## df_denom = 2047.0
## nobs_t = 2051.0
## n_groups = 4
## means = array([375904.17948718,  84219.39285714, 228909.64264706, 142107.30671937])
## nobs = array([  78.,   28.,  680., 1265.])
## vars_ = array([1.10988963e+10, 1.13439030e+09, 5.15642396e+09, 1.67638622e+09])
## use_var = &#39;equal&#39;
## welch_correction = False
## tuple = (701.8318492122419, 1.00013222764e-313)</code></pre>
<p>Using Scipy instead of statsmodels:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="slr.html#cb48-1" tabindex="-1"></a>sp.stats.f_oneway(train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Excellent&#39;</span>],</span>
<span id="cb48-2"><a href="slr.html#cb48-2" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Good&#39;</span>],</span>
<span id="cb48-3"><a href="slr.html#cb48-3" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Typical&#39;</span>],</span>
<span id="cb48-4"><a href="slr.html#cb48-4" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Fair&#39;</span>])</span></code></pre></div>
<pre><code>## F_onewayResult(statistic=701.8318492122418, pvalue=1.00013222764e-313)</code></pre>
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="slr.html#cb50-1" tabindex="-1"></a>train[<span class="st">&#39;pred_anova&#39;</span>] <span class="op">=</span> model.predict()</span>
<span id="cb50-2"><a href="slr.html#cb50-2" tabindex="-1"></a>train[<span class="st">&#39;resid_anova&#39;</span>] <span class="op">=</span> model.resid</span>
<span id="cb50-3"><a href="slr.html#cb50-3" tabindex="-1"></a></span>
<span id="cb50-4"><a href="slr.html#cb50-4" tabindex="-1"></a>train[[<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;pred_anova&#39;</span>, <span class="st">&#39;resid_anova&#39;</span>]].head(n <span class="op">=</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##    Sale_Price     pred_anova   resid_anova
## 0      232600  228909.642647   3690.357353
## 1      166000  228909.642647 -62909.642647
## 2      170000  142107.306719  27892.693281
## 3      252000  228909.642647  23090.357353
## 4      134000  142107.306719  -8107.306719
## 5      164700  228909.642647 -64209.642647
## 6      193500  142107.306719  51392.693281
## 7      118500  142107.306719 -23607.306719
## 8       94000  142107.306719 -48107.306719
## 9      111250  142107.306719 -30857.306719</code></pre>
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="slr.html#cb52-1" tabindex="-1"></a>sm.api.qqplot(train[<span class="st">&#39;resid_anova&#39;</span>])</span>
<span id="cb52-2"><a href="slr.html#cb52-2" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode" id="cb53"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb53-1"><a href="slr.html#cb53-1" tabindex="-1"></a>sp.stats.shapiro(model.resid)</span></code></pre></div>
<pre><code>## ShapiroResult(statistic=0.9187809406211249, pvalue=7.319434407349789e-32)</code></pre>
<div class="sourceCode" id="cb55"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb55-1"><a href="slr.html#cb55-1" tabindex="-1"></a>sp.stats.levene(train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Excellent&#39;</span>],</span>
<span id="cb55-2"><a href="slr.html#cb55-2" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Good&#39;</span>],</span>
<span id="cb55-3"><a href="slr.html#cb55-3" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Typical&#39;</span>],</span>
<span id="cb55-4"><a href="slr.html#cb55-4" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Fair&#39;</span>])</span></code></pre></div>
<pre><code>## LeveneResult(statistic=76.87916723411712, pvalue=4.0352332459689874e-47)</code></pre>
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="slr.html#cb57-1" tabindex="-1"></a>sp.stats.fligner(train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Excellent&#39;</span>],</span>
<span id="cb57-2"><a href="slr.html#cb57-2" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Good&#39;</span>],</span>
<span id="cb57-3"><a href="slr.html#cb57-3" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Typical&#39;</span>],</span>
<span id="cb57-4"><a href="slr.html#cb57-4" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Fair&#39;</span>])</span></code></pre></div>
<pre><code>## FlignerResult(statistic=206.2591684986186, pvalue=1.8733883114714104e-44)</code></pre>
</div>
<div id="kruskal" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Kruskal-Wallis<a href="slr.html#kruskal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Kruskal-Wallis test, proposed in 1952, is equivalent to a parametric one-way ANOVA where the data values have been replaced with their ranks (i.e. largest value = 1, second largest value = 2, etc.). When the data <em>is not</em> normally distributed but <em>is</em> identically distributed (having the same shape and variance), the Kruskal-Wallis test can be considered a test for differences in medians. If those identical distributions are also symmetric, then Kruskal-Wallis can be interpretted as testing for a difference in means. When the data is not identically distributed, or when the distributions are not symmetric, Kruskal-Wallis is a test of <strong>dominance</strong> between distributions. <strong>Distributional dominance</strong> is the notion that one group’s distribution is located at larger values than another, probabilistically speaking. Formally, a random variable A has dominance over random variable B if <span class="math inline">\(P(A\geq x) \geq P(B\geq x)\)</span> for all <span class="math inline">\(x\)</span>, and for some <span class="math inline">\(x\)</span>, <span class="math inline">\(P(A\geq x) &gt; P(B\geq x)\)</span>.</p>
<p>We summarize this information in the following table:</p>
<table style="width:auto; margin-left: auto; margin-right: auto;">
<tr>
<td>
Conditions
<td>
Interpretation of Significant <br> Kruskal-Wallis Test
<tr>
<td>
Group distributions are identical in shape,<br> variance, and symmetric
<td>
Difference in means
<tr>
<td>
Group distributions are identical in shape,<br> but not symmetric
<td>
Difference in medians
<tr>
<td>
Group distributions are not identical in shape,<br> variance, and are not symmetric
<td>
Difference in location. <br> (distributional dominance)
</tr>
</table>
<p>Implementing the Kruskal-Wallis test in R is simple:</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="slr.html#cb59-1" tabindex="-1"></a><span class="fu">ggdensity</span>(train, <span class="at">x =</span> <span class="st">&quot;Sale_Price&quot;</span>,</span>
<span id="cb59-2"><a href="slr.html#cb59-2" tabindex="-1"></a>           <span class="at">add =</span> <span class="st">&quot;mean&quot;</span>, <span class="at">rug =</span> <span class="cn">TRUE</span>,</span>
<span id="cb59-3"><a href="slr.html#cb59-3" tabindex="-1"></a>           <span class="at">color =</span> <span class="st">&quot;Exter_Qual&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;Exter_Qual&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-28-3.png" width="672" /></p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="slr.html#cb60-1" tabindex="-1"></a>model<span class="ot">&lt;-</span><span class="fu">aov</span>(Sale_Price <span class="sc">~</span> <span class="fu">factor</span>(Exter_Qual), <span class="at">data =</span> train)</span>
<span id="cb60-2"><a href="slr.html#cb60-2" tabindex="-1"></a></span>
<span id="cb60-3"><a href="slr.html#cb60-3" tabindex="-1"></a><span class="fu">ggqqplot</span>(<span class="fu">residuals</span>(model))</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-28-4.png" width="672" /></p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="slr.html#cb61-1" tabindex="-1"></a><span class="fu">fligner.test</span>(Sale_Price <span class="sc">~</span> Exter_Qual, <span class="at">data =</span> train) <span class="co"># DOES NOT depend on Normality</span></span></code></pre></div>
<pre><code>## 
##  Fligner-Killeen test of homogeneity of variances
## 
## data:  Sale_Price by Exter_Qual
## Fligner-Killeen:med chi-squared = 206.26, df = 3, p-value &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="slr.html#cb63-1" tabindex="-1"></a><span class="fu">kruskal.test</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country),<span class="at">data=</span>cars2)</span></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  MPG by factor(Country)
## Kruskal-Wallis chi-squared = 197.74, df = 2, p-value &lt; 2.2e-16</code></pre>
<p>Our conclusion would be that the distribution of sale price is different across different levels of exterior quality.</p>
</div>
<div id="python-code-8" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> Python Code<a href="slr.html#python-code-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="slr.html#cb65-1" tabindex="-1"></a>sp.stats.kruskal(train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Excellent&#39;</span>],</span>
<span id="cb65-2"><a href="slr.html#cb65-2" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Good&#39;</span>],</span>
<span id="cb65-3"><a href="slr.html#cb65-3" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Typical&#39;</span>],</span>
<span id="cb65-4"><a href="slr.html#cb65-4" tabindex="-1"></a>               train[<span class="st">&#39;Sale_Price&#39;</span>][train[<span class="st">&#39;Exter_Qual&#39;</span>] <span class="op">==</span> <span class="st">&#39;Fair&#39;</span>])</span></code></pre></div>
<pre><code>## KruskalResult(statistic=975.9781472622338, pvalue=2.9251120247488444e-211)</code></pre>
</div>
</div>
<div id="posthoc" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> ANOVA Post-hoc Testing<a href="slr.html#posthoc" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After performing an ANOVA and learning that there is a difference between the groups of data, our next natural question ought to be <em>which groups of data are different, and how?</em> In order to explore this question, we must first consider the notion of <strong>experimentwise error</strong>. When conducting multiple hypothesis tests simultaneously, the experimentwise error rate is the proportion of time you expect to make an error in at least one test.</p>
<p>Let’s suppose we are comparing grocery spending on 4 different credit card rewards programs. If we’d like to compare the rewards programs pairwise, that entails 6 different hypothesis tests (each is a two-sample t-test). If we keep a confidence level of <span class="math inline">\(\alpha = 0.05\)</span> and subsequently view “being wrong in one test” as a random event happening with probability <span class="math inline">\(p=0.05\)</span> then our probability of being wrong <em>in at least</em> one test out of 6 could be as great as 0.26!</p>
<p>To control this experiment-wise error rate, we must lower our significance thresholds to account for it. Alternatively, we can view this as an adjustment of our p-values higher while keeping our significance threshold fixed as usual. This is typically the approach taken, as we prefer to fix our significance thresholds in accordance with previous literature or industry standards. There are many methods of adjustment that have been proposed over the years for this purpose. Here, we consider a few popular methods: Tukey’s test (when a basic ANOVA is done), Games-Howell test (when Welch’s ANOVA is done), Dunn’s test (when Kruskal-Wallis test is done) for pairwise comparisons and Dunnett’s test (or Mann-Whitney test with Bonferroni correction) for control group comparisons.</p>
<div id="tukey" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Tukey-Kramer<a href="slr.html#tukey" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If our objective is to compare each group to every other group then Tukey’s test of honest significant differences, also known as the Tukey-Kramer test is probably the most widely-available and popular corrections in practice (should ONLY be used if you do the basic ANOVA). However, it should be noted that Tukey’s test <em>should not</em> be used if one does not plan to make all pairwise comparisons. If only a subset of comparisons are of interest to the user (like comparisons only to a control group) then one should opt for the Dunnett or a modified Bonferroni correction.</p>
<p>To employ Tukey’s HSD in R, we must use the <code>aov()</code> function to create our ANOVA object rather than the <code>lm()</code> function. The output of the test shows the difference in means and the p-value for testing the null hypothesis that the means are equal (i.e. that the differences are equal to 0).</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="slr.html#cb67-1" tabindex="-1"></a>cars_aov <span class="ot">&lt;-</span> <span class="fu">aov</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country), <span class="at">data =</span> cars2)</span>
<span id="cb67-2"><a href="slr.html#cb67-2" tabindex="-1"></a>tukey.cars <span class="ot">&lt;-</span> <span class="fu">TukeyHSD</span>(cars_aov)</span>
<span id="cb67-3"><a href="slr.html#cb67-3" tabindex="-1"></a><span class="fu">print</span>(tukey.cars)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = MPG ~ factor(Country), data = cars2)
## 
## $`factor(Country)`
##                        diff         lwr       upr     p adj
## Japanese-Germany   1.991013   0.1813164  3.800709 0.0269593
## US-Germany        -8.325341  -9.7486718 -6.902011 0.0000000
## US-Japanese      -10.316354 -11.8687997 -8.763908 0.0000000</code></pre>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="slr.html#cb69-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">10</span>,<span class="dv">4</span>,<span class="dv">2</span>))</span>
<span id="cb69-2"><a href="slr.html#cb69-2" tabindex="-1"></a><span class="fu">plot</span>(tukey.cars, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tukey"></span>
<img src="bookdownproj_files/figure-html/tukey-1.png" alt="Confidence intervals for mean differences adjusted via Tukey-Kramer" width="672" />
<p class="caption">
Figure 2.6: Confidence intervals for mean differences adjusted via Tukey-Kramer
</p>
</div>
<p>The p-values in this table have been adjusted higher to account for the possible experimentwise error rate. For every pairwise comparison shown, we reject the null hypothesis and conclude that the mean MPG is different among the US, Japanese and German made cars. Furthermore, Figure <a href="slr.html#fig:tukey">2.6</a> shows us experiment-wise (family-wise) adjusted confidence intervals for the differences in means for each pair. The plot option <code>las=1</code> guides the axis labels. Type <code>?par</code> for a list of plot options for base R, including an explanation of <code>las</code>.</p>
<p>If we utilized the Welch’s ANOVA (assumed variances were NOT equal), then we would use the Games-Howell test:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="slr.html#cb70-1" tabindex="-1"></a><span class="fu">library</span>(PMCMRplus)</span></code></pre></div>
<pre><code>## Warning: package &#39;PMCMRplus&#39; was built under R version 4.3.3</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="slr.html#cb72-1" tabindex="-1"></a>cars_aov <span class="ot">&lt;-</span> <span class="fu">aov</span>(MPG<span class="sc">~</span><span class="fu">factor</span>(Country), <span class="at">data=</span>cars2)</span>
<span id="cb72-2"><a href="slr.html#cb72-2" tabindex="-1"></a>gh.cars <span class="ot">&lt;-</span> <span class="fu">gamesHowellTest</span>(cars_aov)</span>
<span id="cb72-3"><a href="slr.html#cb72-3" tabindex="-1"></a><span class="fu">summary</span>(gh.cars)</span></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using Games-Howell test</code></pre>
<pre><code>## data: MPG by factor(Country)</code></pre>
<pre><code>## alternative hypothesis: two.sided</code></pre>
<pre><code>## P value adjustment method: none</code></pre>
<pre><code>## H0</code></pre>
<pre><code>##                         q value   Pr(&gt;|q|)    
## Japanese - Germany == 0   3.547   0.035479   *
## US - Germany == 0       -22.859 5.3291e-15 ***
## US - Japanese == 0      -19.164 4.3188e-14 ***</code></pre>
<pre><code>## ---</code></pre>
<pre><code>## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The conclusion from the Games-Howell test is similar to Tukey’s test.</p>
<p>However, if we use Kruskal-Wallis test, then we would use the Dunn’s test for multiple comparisons:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="slr.html#cb81-1" tabindex="-1"></a><span class="fu">library</span>(dunn.test)</span></code></pre></div>
<pre><code>## Warning: package &#39;dunn.test&#39; was built under R version 4.3.3</code></pre>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="slr.html#cb83-1" tabindex="-1"></a>dunn.cars <span class="ot">&lt;-</span> <span class="fu">dunn.test</span>(cars2<span class="sc">$</span>MPG,cars2<span class="sc">$</span>Country,<span class="at">kw=</span>T,<span class="at">method=</span><span class="st">&quot;bonferroni&quot;</span>)</span></code></pre></div>
<pre><code>##   Kruskal-Wallis rank sum test
## 
## data: x and group
## Kruskal-Wallis chi-squared = 197.7403, df = 2, p-value = 0
## 
## 
##                            Comparison of x by group                            
##                                  (Bonferroni)                                  
## Col Mean-|
## Row Mean |    Germany   Japanese
## ---------+----------------------
## Japanese |  -1.147428
##          |     0.3768
##          |
##       US |   10.95670   11.38301
##          |    0.0000*    0.0000*
## 
## alpha = 0.05
## Reject Ho if p &lt;= alpha/2</code></pre>
<p>This is a nonparametric test and does not have as much power as the previous multiple comparisons. Be sure you use the correct test based on what you see in the data.</p>
</div>
<div id="dunnett" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Dunnett’s Test<a href="slr.html#dunnett" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If the plan is to make fewer comparisons, specifically just <span class="math inline">\(k-1\)</span> comparisons where <span class="math inline">\(k\)</span> is the number of groups in your data (indicating you plan to compare all the groups to one specific group, usually the control group), then Dunnett’s test would be preferrable to the Tukey-Kramer test. If all pairwise comparisons are not made, the Tukey-Kramer test is overly conservative, creating a confidence level that is much lower than specified by the user. Dunnett’s test factors in fewer comparisons and thus should not be used for tests of all pairwise comparisons.</p>
<p>To use Dunnett’s test (should only be used with basic ANOVA), we must add the <code>DescTools</code> package to our library. The control group to which all other groups will be compared is designated by the <code>control=</code> option.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="slr.html#cb85-1" tabindex="-1"></a>cars2<span class="sc">$</span>group<span class="ot">&lt;-</span><span class="fu">ifelse</span>(cars2<span class="sc">$</span>Country<span class="sc">==</span><span class="st">&quot;US&quot;</span>,<span class="st">&quot;aUS&quot;</span>,cars2<span class="sc">$</span>Country)</span>
<span id="cb85-2"><a href="slr.html#cb85-2" tabindex="-1"></a><span class="fu">dunnettTest</span>(<span class="at">x =</span> cars2<span class="sc">$</span>MPG, <span class="at">g =</span> <span class="fu">factor</span>(cars2<span class="sc">$</span>group))</span></code></pre></div>
<pre><code>##          aUS    
## Germany  &lt; 2e-16
## Japanese 4.4e-16</code></pre>
<p>In the output from Dunnett’s test, we see that Japanese and German made cars are significantly different than US made cars (note that this does not compare Japanese to German!).</p>
<p>If we are not able to assume equal variances, we can use the Welch two-sample test to compare each group to the control and then adjust the p-value using Bonferroni.</p>
<p>We can use this same idea to to the Wilcoxon test for a nonparametric version (then adjust using Bonferroni).</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="slr.html#cb87-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">welchManyOneTTest</span>(<span class="at">x =</span> cars2<span class="sc">$</span>MPG, <span class="at">g =</span> <span class="fu">factor</span>(cars2<span class="sc">$</span>group),<span class="at">p.adjust.method=</span><span class="st">&quot;bonferroni&quot;</span>))</span></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using Welch&#39;s t-test</code></pre>
<pre><code>## data: cars2$MPG and factor(cars2$group)</code></pre>
<pre><code>## alternative hypothesis: two.sided</code></pre>
<pre><code>## P value adjustment method: bonferroni</code></pre>
<pre><code>## H0</code></pre>
<pre><code>##                     t value   Pr(&gt;|t|)    
## Germany - aUS == 0   16.164 &lt; 2.22e-16 ***
## Japanese - aUS == 0  13.551 &lt; 2.22e-16 ***</code></pre>
<pre><code>## ---</code></pre>
<pre><code>## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="slr.html#cb96-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">manyOneUTest</span>(<span class="at">x =</span> cars2<span class="sc">$</span>MPG, <span class="at">g =</span> <span class="fu">factor</span>(cars2<span class="sc">$</span>group),<span class="at">p.adjust.method=</span><span class="st">&quot;bonferroni&quot;</span>))</span></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using Wilcoxon, Mann, Whittney U-test
##  for multiple comparisons with one control</code></pre>
<pre><code>## data: cars2$MPG and factor(cars2$group)</code></pre>
<pre><code>## alternative hypothesis: two.sided</code></pre>
<pre><code>## P value adjustment method: bonferroni</code></pre>
<pre><code>## H0</code></pre>
<pre><code>##                     z value   Pr(&gt;|z|)    
## Germany - aUS == 0  -11.565 &lt; 2.22e-16 ***
## Japanese - aUS == 0 -10.677 &lt; 2.22e-16 ***</code></pre>
<pre><code>## ---</code></pre>
<pre><code>## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
</div>
<div id="python-code-9" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Python Code<a href="slr.html#python-code-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Tukey-Kramer test:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="slr.html#cb105-1" tabindex="-1"></a><span class="im">import</span> statsmodels.stats.multicomp <span class="im">as</span> mc</span>
<span id="cb105-2"><a href="slr.html#cb105-2" tabindex="-1"></a></span>
<span id="cb105-3"><a href="slr.html#cb105-3" tabindex="-1"></a>comp <span class="op">=</span> mc.MultiComparison(train[<span class="st">&#39;Sale_Price&#39;</span>], train[<span class="st">&#39;Exter_Qual&#39;</span>])</span>
<span id="cb105-4"><a href="slr.html#cb105-4" tabindex="-1"></a>ph_res <span class="op">=</span> comp.tukeyhsd(alpha <span class="op">=</span> <span class="fl">0.05</span>)</span>
<span id="cb105-5"><a href="slr.html#cb105-5" tabindex="-1"></a>ph_res.summary()</span></code></pre></div>
<table class="simpletable">
<caption>Multiple Comparison of Means - Tukey HSD, FWER=0.05</caption>
<tr>
   <th>group1</th>   <th>group2</th>    <th>meandiff</th>   <th>p-adj</th>     <th>lower</th>        <th>upper</th>    <th>reject</th>
</tr>
<tr>
  <td>Excellent</td>  <td>Fair</td>   <td>-291684.7866</td>  <td>0.0</td>  <td>-323617.1589</td> <td>-259752.4144</td>  <td>True</td> 
</tr>
<tr>
  <td>Excellent</td>  <td>Good</td>   <td>-146994.5368</td>  <td>0.0</td>  <td>-164322.0969</td> <td>-129666.9768</td>  <td>True</td> 
</tr>
<tr>
  <td>Excellent</td> <td>Typical</td> <td>-233796.8728</td>  <td>0.0</td>   <td>-250707.122</td> <td>-216886.6235</td>  <td>True</td> 
</tr>
<tr>
    <td>Fair</td>     <td>Good</td>    <td>144690.2498</td>  <td>0.0</td>   <td>116739.8693</td>  <td>172640.6303</td>  <td>True</td> 
</tr>
<tr>
    <td>Fair</td>    <td>Typical</td>  <td>57887.9139</td>   <td>0.0</td>   <td>30194.3052</td>   <td>85581.5226</td>   <td>True</td> 
</tr>
<tr>
    <td>Good</td>    <td>Typical</td>  <td>-86802.3359</td>  <td>0.0</td>   <td>-93694.6435</td>  <td>-79910.0283</td>  <td>True</td> 
</tr>
</table>
<div class="sourceCode" id="cb106"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="slr.html#cb106-1" tabindex="-1"></a>ph_res.plot_simultaneous()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-32-1.png" width="960" /></p>
<p>No Dunnett’s test in Python. The team at statsmodels is supposedly “working on it”.</p>
</div>
</div>
<div id="cor" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Pearson Correlation<a href="slr.html#cor" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>ANOVA is used to formally test the relationship between a categorical variable and a continuous variable. To formally test the (linear) relationship between two continuous attributes, we introduce <strong>Pearson correlation</strong>, commonly referred to as simply <strong>correlation</strong>. Correlation is a number between -1 and 1 which measures the strength of a linear relationship between two continuous attributes.</p>
<p>Negative values of correlation indicate a <em>negative linear relationship</em>, meaning that as one of the variables increases, the other tends to decrease. Similarly, positive values of correlation indicate a <em>positive linear relationship</em> meaning that as one of the variables increases, the other tends to increase. Absolute values of correlation equal to 1 indicate a perfect linear relationship. For example, if our data had a column for “mile time in minutes” and a column for “mile time in seconds”, these two columns would have a correlation of 1 due to the fact that there are 60 seconds in a minute. A correlation value near 0 indicates that the variables have no linear relationship.</p>
<p>It’s important to emphasize that Pearson correlation is only designed to detect <em>linear</em> associations between variables. Even when a correlation between two variables is 0, the two variables may still have a very clear association, whether it be quadratic, cyclical, or some other nonlinear pattern of association. Figure <a href="slr.html#fig:correlation">2.7</a> illustrates all of these statements. On top of each scatter plot, the correlation coefficient is shown. The middle row of this figure aims to illustrate that a perfect correlation has nothing to do with the <em>magnitude</em> or <em>slope</em> of the relationship. In the center image, middle row, we note that the correlation is undefined for any pair that includes a constant variable. In that image, the value of <span class="math inline">\(y\)</span> is constant across the sample. Equation <a href="slr.html#eq:correlation">(2.3)</a> makes this mathematically clear.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:correlation"></span>
<img src="img/correlation.png" alt="Examples of relationships and their associated correlations" width="100%" />
<p class="caption">
Figure 2.7: Examples of relationships and their associated correlations
</p>
</div>
<p>The population correlation parameter is denoted <span class="math inline">\(\rho\)</span> and estimated by the sample correlation, denoted as <span class="math inline">\(r\)</span>. The formula for the sample correlation between columns of data <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is</p>
<p><span class="math display" id="eq:correlation">\[\begin{equation}
r = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{x})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2\sum_{i=1}^n(y_i-\bar{x})^2}}.
\tag{2.3}
\end{equation}\]</span></p>
<p>Note that with <em>centered</em> variable vectors <span class="math inline">\(\mathbf{x_c}\)</span> and <span class="math inline">\(\mathbf{y_c}\)</span> this formula becomes much cleaner with linear algebra notation:</p>
<p><span class="math display" id="eq:veccorrelation">\[\begin{equation}
r = \frac{\mathbf{x_c}^T\mathbf{y_c}}{\|\mathbf{x_c}\|\|\mathbf{y_c}\|}.
\tag{2.4}
\end{equation}\]</span></p>
<p>It is interesting to note that Equation <a href="slr.html#eq:veccorrelation">(2.4)</a> is identical to the formula for the cosine of the angle between to vectors. While this geometrical relationship does not benefit our intuition<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, it is noteworthy nonetheless.</p>
<p>Pearson’s correlation can be calculated in R with the built in <code>cor()</code> function, with the two continuous variables as input:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="slr.html#cb107-1" tabindex="-1"></a><span class="fu">cor</span>(train<span class="sc">$</span>Gr_Liv_Area,train<span class="sc">$</span>Sale_Price)</span></code></pre></div>
<pre><code>## [1] 0.698509</code></pre>
<div id="testcor" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Statistical Test<a href="slr.html#testcor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To test the statistical significance of correlation, we use a t-test with the null hypothesis that the correlation is equal to 0:
<span class="math display">\[H_0: \rho = 0\]</span>
<span class="math display">\[H_a: \rho \neq 0\]</span>
If we can reject the null hypothesis, then we declare a significant linear association between the two variables. The <code>cor.test()</code> function in R will perform the test:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="slr.html#cb109-1" tabindex="-1"></a><span class="fu">cor.test</span>(train<span class="sc">$</span>Gr_Liv_Area,train<span class="sc">$</span>Sale_Price)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  x and y
## t = 44.185, df = 2049, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.6756538 0.7200229
## sample estimates:
##      cor 
## 0.698509</code></pre>
<p>We conclude that <code>Gr_Liv_Area</code> has a linear association with Sale_Price.</p>
<p>It must be noted that this t-test for Pearson’s correlation is not free from assumptions. In fact, there are 4 assumptions that must be met, and they are detailed in Section <a href="slr.html#slrassumptions">2.7.1</a>.</p>
</div>
<div id="effect-of-anomalous-observations" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Effect of Anomalous Observations<a href="slr.html#effect-of-anomalous-observations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One final nuance that is important to note is the effect of anomalous observations on correlation. In Figure <a href="slr.html#fig:nocor">2.8</a> we display 30 random 2-dimensional data points <span class="math inline">\((x,y)\)</span> with no linear relationship.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="slr.html#cb111-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">11</span>)</span>
<span id="cb111-2"><a href="slr.html#cb111-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">30</span>)</span>
<span id="cb111-3"><a href="slr.html#cb111-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">30</span>)</span>
<span id="cb111-4"><a href="slr.html#cb111-4" tabindex="-1"></a><span class="fu">plot</span>(x,y)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nocor"></span>
<img src="bookdownproj_files/figure-html/nocor-1.png" alt="The variables x and y have no correlation" width="672" />
<p class="caption">
Figure 2.8: The variables x and y have no correlation
</p>
</div>
<p>The correlation is not exactly zero (we wouldn’t expect perfection from random data) but it is very close at 0.002.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="slr.html#cb112-1" tabindex="-1"></a><span class="fu">cor.test</span>(x,y)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  x and y
## t = 0.012045, df = 28, p-value = 0.9905
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.3582868  0.3622484
## sample estimates:
##         cor 
## 0.002276214</code></pre>
<p>Next, we’ll add a single anomalous observation to our data and see how it affects both the correlation value and the correlation test.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="slr.html#cb114-1" tabindex="-1"></a>x[<span class="dv">31</span>] <span class="ot">=</span> <span class="dv">4</span></span>
<span id="cb114-2"><a href="slr.html#cb114-2" tabindex="-1"></a>y[<span class="dv">31</span>] <span class="ot">=</span> <span class="dv">50</span></span>
<span id="cb114-3"><a href="slr.html#cb114-3" tabindex="-1"></a><span class="fu">cor.test</span>(x,y)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  x and y
## t = 5.803, df = 29, p-value = 2.738e-06
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.5115236 0.8631548
## sample estimates:
##       cor 
## 0.7330043</code></pre>
<p>The correlation jumps to 0.73 from 0.002 and is declared strongly significant! Figure <a href="slr.html#fig:extremenocor">2.9</a> illustrates the new data. This simple example shows why exploratory data analysis is so important! If we don’t explore our data and detect anomalous observations, we might improperly declare relationships are significant when they are driven by a single observation or a small handful of observations.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="slr.html#cb116-1" tabindex="-1"></a><span class="fu">plot</span>(x,y)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:extremenocor"></span>
<img src="bookdownproj_files/figure-html/extremenocor-1.png" alt="A single anomalous observation creates strong correlation (0.73) where there previously was none" width="672" />
<p class="caption">
Figure 2.9: A single anomalous observation creates strong correlation (0.73) where there previously was none
</p>
</div>
</div>
<div id="the-correlation-matrix" class="section level3 hasAnchor" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> The Correlation Matrix<a href="slr.html#the-correlation-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It’s common to consider and calculate all pairwise correlations between variables in a dataset. If many attributes share a high degree of mutual correlation, this can cause problems for regression as will be discussed in Chapter 5. The pairwise correlations are generally arranged in an array called the <strong>correlation matrix</strong>, where the <span class="math inline">\((i,j)^{th}\)</span> entry is the correlation between the <span class="math inline">\(i^{th}\)</span> variable and <span class="math inline">\(j^{th}\)</span> variable in your list. To compute the correlation matrix, we again use the <code>cor()</code> function.</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="slr.html#cb117-1" tabindex="-1"></a><span class="fu">cor</span>(train[, <span class="fu">c</span>(<span class="st">&#39;Year_Built&#39;</span>,<span class="st">&#39;Total_Bsmt_SF&#39;</span>,<span class="st">&#39;First_Flr_SF&#39;</span>,<span class="st">&#39;Gr_Liv_Area&#39;</span>,<span class="st">&#39;Sale_Price&#39;</span>)])</span></code></pre></div>
<pre><code>##               Year_Built Total_Bsmt_SF First_Flr_SF Gr_Liv_Area Sale_Price
## Year_Built     1.0000000     0.4037104    0.3095407   0.2454325  0.5668889
## Total_Bsmt_SF  0.4037104     1.0000000    0.8120419   0.4643838  0.6276502
## First_Flr_SF   0.3095407     0.8120419    1.0000000   0.5707205  0.6085229
## Gr_Liv_Area    0.2454325     0.4643838    0.5707205   1.0000000  0.6985090
## Sale_Price     0.5668889     0.6276502    0.6085229   0.6985090  1.0000000</code></pre>
<p>Not surprisingly, we see strong positive correlation between the square footage of the basement and that of the first floor, and also between all of the area variables and the sale price. As demonstrated by Figures <a href="slr.html#fig:correlation">2.7</a> and <a href="slr.html#fig:extremenocor">2.9</a>, raw correlation values can be misleading and it’s unwise to calculate them without a scatter plot for context. The <code>pairs()</code> function in base R provides a simple matrix of scatterplots for this purpose.</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="slr.html#cb119-1" tabindex="-1"></a><span class="fu">pairs</span>(train[, <span class="fu">c</span>(<span class="st">&#39;Year_Built&#39;</span>,<span class="st">&#39;Total_Bsmt_SF&#39;</span>,<span class="st">&#39;First_Flr_SF&#39;</span>,<span class="st">&#39;Gr_Liv_Area&#39;</span>,<span class="st">&#39;Sale_Price&#39;</span>)])</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
</div>
<div id="python-code-10" class="section level3 hasAnchor" number="2.6.4">
<h3><span class="header-section-number">2.6.4</span> Python Code<a href="slr.html#python-code-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Pearson’s correlation</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="slr.html#cb120-1" tabindex="-1"></a>np.corrcoef(train[<span class="st">&#39;Gr_Liv_Area&#39;</span>], train[<span class="st">&#39;Sale_Price&#39;</span>])</span></code></pre></div>
<pre><code>## array([[1.        , 0.69850904],
##        [0.69850904, 1.        ]])</code></pre>
<p>Statistical test for Correlation</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="slr.html#cb122-1" tabindex="-1"></a>sp.stats.pearsonr(train[<span class="st">&#39;Gr_Liv_Area&#39;</span>], train[<span class="st">&#39;Sale_Price&#39;</span>])</span></code></pre></div>
<pre><code>## PearsonRResult(statistic=0.6985090408804115, pvalue=4.195282462397299e-300)</code></pre>
<p>Correlation Matrix</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="slr.html#cb124-1" tabindex="-1"></a>np.corrcoef(train[[<span class="st">&#39;Year_Built&#39;</span>, <span class="st">&#39;Total_Bsmt_SF&#39;</span>, <span class="st">&#39;First_Flr_SF&#39;</span>, <span class="st">&#39;Gr_Liv_Area&#39;</span>, <span class="st">&#39;Sale_Price&#39;</span>]], rowvar <span class="op">=</span> <span class="va">False</span>)</span></code></pre></div>
<pre><code>## array([[1.        , 0.40371038, 0.3095407 , 0.24543253, 0.56688895],
##        [0.40371038, 1.        , 0.81204187, 0.46438378, 0.62765021],
##        [0.3095407 , 0.81204187, 1.        , 0.57072054, 0.60852293],
##        [0.24543253, 0.46438378, 0.57072054, 1.        , 0.69850904],
##        [0.56688895, 0.62765021, 0.60852293, 0.69850904, 1.        ]])</code></pre>
<div class="sourceCode" id="cb126"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb126-1"><a href="slr.html#cb126-1" tabindex="-1"></a></span>
<span id="cb126-2"><a href="slr.html#cb126-2" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb126-3"><a href="slr.html#cb126-3" tabindex="-1"></a><span class="co"># Selecting relevant columns</span></span>
<span id="cb126-4"><a href="slr.html#cb126-4" tabindex="-1"></a>columns <span class="op">=</span> [<span class="st">&#39;Year_Built&#39;</span>, <span class="st">&#39;Total_Bsmt_SF&#39;</span>, <span class="st">&#39;First_Flr_SF&#39;</span>, <span class="st">&#39;Gr_Liv_Area&#39;</span>, <span class="st">&#39;Sale_Price&#39;</span>]</span>
<span id="cb126-5"><a href="slr.html#cb126-5" tabindex="-1"></a></span>
<span id="cb126-6"><a href="slr.html#cb126-6" tabindex="-1"></a><span class="co"># Create a pair plot</span></span>
<span id="cb126-7"><a href="slr.html#cb126-7" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="bu">len</span>(columns), ncols<span class="op">=</span><span class="bu">len</span>(columns), figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">12</span>))</span>
<span id="cb126-8"><a href="slr.html#cb126-8" tabindex="-1"></a></span>
<span id="cb126-9"><a href="slr.html#cb126-9" tabindex="-1"></a><span class="cf">for</span> i, col1 <span class="kw">in</span> <span class="bu">enumerate</span>(columns):</span>
<span id="cb126-10"><a href="slr.html#cb126-10" tabindex="-1"></a>    <span class="cf">for</span> j, col2 <span class="kw">in</span> <span class="bu">enumerate</span>(columns):</span>
<span id="cb126-11"><a href="slr.html#cb126-11" tabindex="-1"></a>        ax <span class="op">=</span> axes[i, j]</span>
<span id="cb126-12"><a href="slr.html#cb126-12" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> j:</span>
<span id="cb126-13"><a href="slr.html#cb126-13" tabindex="-1"></a>            ax.hist(train[col1], bins<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb126-14"><a href="slr.html#cb126-14" tabindex="-1"></a>            ax.set_ylabel(col1)</span>
<span id="cb126-15"><a href="slr.html#cb126-15" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb126-16"><a href="slr.html#cb126-16" tabindex="-1"></a>            ax.scatter(train[col2], train[col1])</span>
<span id="cb126-17"><a href="slr.html#cb126-17" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="bu">len</span>(columns) <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb126-18"><a href="slr.html#cb126-18" tabindex="-1"></a>            ax.set_xlabel(col2)</span>
<span id="cb126-19"><a href="slr.html#cb126-19" tabindex="-1"></a></span>
<span id="cb126-20"><a href="slr.html#cb126-20" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb126-21"><a href="slr.html#cb126-21" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-42-1.png" width="1152" /></p>
</div>
</div>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Simple Linear Regression<a href="slr.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>After learning that two variables share a linear relationship, the next question is natural: <em>what is that relationship?</em> How much,on average, should we expect one variable to change as the other changes by a single unit? Simple linear regression answers this question by creating a linear equation that best represents the relationship in the sense that it minimizes the squared error between the observed data and the model predictions (i.e. the sum of the squared residuals). The simple linear regression equation is typically written
<span class="math display" id="eq:slr">\[\begin{equation}
y=\beta_0 + \beta_1x + \varepsilon
\tag{2.5}
\end{equation}\]</span>
where <span class="math inline">\(\beta_0\)</span>, the <strong>intercept</strong>, gives the expected value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0\)</span> and <span class="math inline">\(\beta_1\)</span>, the <strong>slope</strong> gives the expected change in <span class="math inline">\(y\)</span> for a one-unit increase in <span class="math inline">\(x\)</span>. The <strong>error</strong>, <span class="math inline">\(\varepsilon\)</span> is the amount each individual <span class="math inline">\(y\)</span> differs from the population line (we would not expect all values of <span class="math inline">\(y\)</span> to fall directly on the line). When we use a sample of data to estimate the true population line, we get our prediction equation or <span class="math inline">\(\hat{y}=\hat{\beta}_0 + \hat{\beta}_1x\)</span>. Residuals from the predicted line is defined as <span class="math inline">\(e=y-\hat{y}\)</span>. <em>Ordinary Least Squares seeks to minimize the sum of squared residuals or sum of squared error</em>. That objective is known as a <strong>loss function</strong>. The sum of squared error (SSE) or equivalently the mean squared error (MSE) loss functions are by far the most popular loss functions for continuous prediction problems.</p>
<p>We should note that SSE is not the <em>only</em> loss function at our disposal. Minimizing the <em>mean absolute error</em> (MAE) is common in situations with a highly skewed response variable (squaring very large errors gives those observations in the tail too much influence on the regression as we will later discuss). Using MAE to drive our loss function gives us predictions that are conditional <em>medians</em> of the response, given the input data. Other loss functions, like Huber’s M function, are also used to handle problems with influential observations as discussed in Chapter 5.</p>
<p>As we mentioned in Section <a href="slr.html#evp">2.1</a>, a simple linear regression serves two purposes:</p>
<ol style="list-style-type: decimal">
<li>to predict the expected value of <span class="math inline">\(y\)</span> for each value of <span class="math inline">\(x\)</span> and</li>
<li>to explain <em>how</em> <span class="math inline">\(y\)</span> is expected to change for a unit change in <span class="math inline">\(x\)</span>.</li>
</ol>
<p>In order to accurately use a regression for the second purpose, however, we must first meet assumptions with our data.</p>
<div id="slrassumptions" class="section level3 hasAnchor" number="2.7.1">
<h3><span class="header-section-number">2.7.1</span> Assumptions of Linear Regression<a href="slr.html#slrassumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Linear regression, in particular the hypothesis tests that are generally performed as part of linear regression, has 4 assumptions:</p>
<ol style="list-style-type: decimal">
<li>The expected value of <span class="math inline">\(y\)</span> is linear in <span class="math inline">\(x\)</span> (proper model specification).</li>
<li>The random errors are independent.</li>
<li>The random errors are normally distributed.</li>
<li>The random errors have equal variance (homoskedasticity).</li>
</ol>
<p>It must now be noted that these assumptions are also in effect for the test of Pearson’s correlation in Section <a href="slr.html#testcor">2.6.1</a>, because the tests in simple linear regression are mathematically equivalent to that test. When these assumptions are not met, another approach to testing the significance of a linear relationship should be considered. The most common non-parametric approach to testing for an association between two continuous variables is Spearman’s correlation. Spearman’s correlation does not limit its findings to linear relationships; any monotonic relationship (one that is always increasing or always decreasing) will cause Spearman’s correlation to be significant. Similar to the approach taken by Kruskal-Wallis, Spearman’s correlation replaces the data with its ranks and computes Pearson’s correlation on the ranks. The same <code>cor</code> and <code>cor.test()</code> functions can be used; simply specify the <code>method='spearman'</code> option.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="slr.html#cb127-1" tabindex="-1"></a><span class="fu">cor.test</span>(train<span class="sc">$</span>Gr_Liv_Area,train<span class="sc">$</span>Sale_Price, <span class="at">method =</span> <span class="st">&#39;spearman&#39;</span>)</span></code></pre></div>
<pre><code>## Warning in cor.test.default(x, y, ...): Cannot compute exact p-value with ties</code></pre>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  x and y
## S = 408364087, p-value &lt; 2.2e-16
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.7160107</code></pre>
</div>
<div id="testing-for-association" class="section level3 hasAnchor" number="2.7.2">
<h3><span class="header-section-number">2.7.2</span> Testing for Association<a href="slr.html#testing-for-association" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The statistical test of correlation is mathematically equivalent to testing the hypothesis that the slope parameter in Equation <a href="slr.html#eq:slr">(2.5)</a> is zero. This t-test is part of the output from any linear regression function, like <code>lm()</code> which we saw in Section <a href="slr.html#oneway">2.4</a>. Let’s confirm this using the example from the Section <a href="slr.html#testcor">2.6.1</a> where we investigate the relationship between <code>Gr_Liv_Area</code> and <code>Sale_Price</code>. Again, the t-test in the output tests the following hypothesis:
<span class="math display">\[H_0: \beta_1=0\]</span>
<span class="math display">\[H_a: \beta_1 \neq 0\]</span>
The first thing we will do after creating the linear model is check our assumption using the default plots from <code>lm()</code> . From these four plots we will be most interested in the first two.</p>
<p>In the top left plot, we are visually checking for homoskedasticity. We’d like to see the variability of the points remain constant from left to right on this chart, indicating that the errors have constant variance for each value of y. We do not want to see any fan shapes in this chart. Unfortunately, we do see just that: the variability of the errors is much smaller for smaller values of Sale Price than it is for larger values of Sale Price.</p>
<p>In the top right plot, we are visually checking for normality of errors. We’d like to see the QQ-plot indicate normality with all the points roughly following the line. Unfortunately, we do not see that here. The errors do not appear to be normally distributed.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="slr.html#cb130-1" tabindex="-1"></a>slr <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sale_Price <span class="sc">~</span> Gr_Liv_Area, <span class="at">data=</span>train)</span>
<span id="cb130-2"><a href="slr.html#cb130-2" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb130-3"><a href="slr.html#cb130-3" tabindex="-1"></a><span class="fu">plot</span>(slr)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:slrplots"></span>
<img src="bookdownproj_files/figure-html/slrplots-1.png" alt="The variables x and y have no correlation" width="672" />
<p class="caption">
Figure 2.10: The variables x and y have no correlation
</p>
</div>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="slr.html#cb131-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</span></code></pre></div>
<p>Despite the violation of assumptions, let’s continue examining the output from this regression in order to practice our interpretation of it.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="slr.html#cb132-1" tabindex="-1"></a><span class="fu">summary</span>(slr)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -478762  -30030   -1405   22273  335855 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 14045.872   3942.503   3.563 0.000375 ***
## Gr_Liv_Area   110.726      2.506  44.185  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 57430 on 2049 degrees of freedom
## Multiple R-squared:  0.4879, Adjusted R-squared:  0.4877 
## F-statistic:  1952 on 1 and 2049 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The first thing we’re likely to examine in the coefficient table is the p-value for <code>Gr_Liv_Area</code>. It is strongly significant (in fact, it’s the same t-value and p-value as we saw for the <code>cor.test</code> as these tests are mathematically equivalent), indicating that there is an association between the size of the home and the sale price. Furthermore, the <em>parameter estimate</em> is 115.5 indicating that we’d expect the price of a home to increase by $115.5 for every additional square foot of living space. Because of the linearity of the model, we can extend this slope estimate to any unit change in <span class="math inline">\(x\)</span>. For example, it might be difficult to think in terms of single square feet when comparing houses, so we might prefer to use a 100 square-foot change and report our conclusion as follows: For each additional 100 square feet of living area, we expect the house price to increase by $11,550.</p>
</div>
<div id="python-code-11" class="section level3 hasAnchor" number="2.7.3">
<h3><span class="header-section-number">2.7.3</span> Python Code<a href="slr.html#python-code-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Assumptions of Linear Regression</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb134-1"><a href="slr.html#cb134-1" tabindex="-1"></a>sp.stats.spearmanr(train[<span class="st">&#39;Gr_Liv_Area&#39;</span>], train[<span class="st">&#39;Sale_Price&#39;</span>])</span></code></pre></div>
<pre><code>## SignificanceResult(statistic=0.7160107413159085, pvalue=3.66e-322)</code></pre>
<p>Test for Association</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb136-1"><a href="slr.html#cb136-1" tabindex="-1"></a>model_slr <span class="op">=</span> smf.ols(<span class="st">&quot;Sale_Price ~ Gr_Liv_Area&quot;</span>, data <span class="op">=</span> train).fit()</span>
<span id="cb136-2"><a href="slr.html#cb136-2" tabindex="-1"></a>model_slr.summary()</span></code></pre></div>
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>       <td>Sale_Price</td>    <th>  R-squared:         </th> <td>   0.488</td> 
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.488</td> 
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1952.</td> 
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 17 Jun 2024</td> <th>  Prob (F-statistic):</th> <td>4.20e-300</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:51:27</td>     <th>  Log-Likelihood:    </th> <td> -25385.</td> 
</tr>
<tr>
  <th>No. Observations:</th>      <td>  2051</td>      <th>  AIC:               </th> <td>5.077e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  2049</td>      <th>  BIC:               </th> <td>5.078e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>
<table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  
</tr>
<tr>
  <th>Intercept</th>   <td> 1.405e+04</td> <td> 3942.503</td> <td>    3.563</td> <td> 0.000</td> <td> 6314.141</td> <td> 2.18e+04</td>
</tr>
<tr>
  <th>Gr_Liv_Area</th> <td>  110.7259</td> <td>    2.506</td> <td>   44.185</td> <td> 0.000</td> <td>  105.811</td> <td>  115.640</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>385.081</td> <th>  Durbin-Watson:     </th> <td>   2.022</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4372.360</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.535</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td>10.072</td>  <th>  Cond. No.          </th> <td>4.89e+03</td>
</tr>
</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.89e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems.
<div class="sourceCode" id="cb137"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb137-1"><a href="slr.html#cb137-1" tabindex="-1"></a>train[<span class="st">&#39;pred_slr&#39;</span>] <span class="op">=</span> model_slr.predict()</span>
<span id="cb137-2"><a href="slr.html#cb137-2" tabindex="-1"></a>train[<span class="st">&#39;resid_slr&#39;</span>] <span class="op">=</span> model_slr.resid</span>
<span id="cb137-3"><a href="slr.html#cb137-3" tabindex="-1"></a></span>
<span id="cb137-4"><a href="slr.html#cb137-4" tabindex="-1"></a>train[[<span class="st">&#39;Sale_Price&#39;</span>, <span class="st">&#39;pred_anova&#39;</span>, <span class="st">&#39;pred_slr&#39;</span>]].head(n <span class="op">=</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##    Sale_Price     pred_anova       pred_slr
## 0      232600  228909.642647  178916.802008
## 1      166000  228909.642647  182792.210023
## 2      170000  142107.306719  200065.457178
## 3      252000  228909.642647  230293.639700
## 4      134000  142107.306719  139387.640249
## 5      164700  228909.642647  152453.301559
## 6      193500  142107.306719  199954.731235
## 7      118500  142107.306719  148688.619487
## 8       94000  142107.306719  111041.798764
## 9      111250  142107.306719  109713.087445</code></pre>
<div class="sourceCode" id="cb139"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb139-1"><a href="slr.html#cb139-1" tabindex="-1"></a>sm.api.qqplot(train[<span class="st">&#39;resid_slr&#39;</span>])</span>
<span id="cb139-2"><a href="slr.html#cb139-2" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>The n-dimensional “variable vectors” and live in the vast <em>sample space</em> where the <span class="math inline">\(i^{th}\)</span> axis represents the <span class="math inline">\(i^th\)</span> observation in your dataset. In this space, a single point/vector is one possible set of sample values of n observations; this space can be difficult to grasp mentally<a href="slr.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intro-stat.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mlr.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/edit/master/02-introduction_ANOVA_regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/blob/master/02-introduction_ANOVA_regression.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"toc": null
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
