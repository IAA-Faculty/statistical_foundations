<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Model Building &amp; Scoring for Prediction | Statistical Foundations</title>
  <meta name="description" content="Chapter 6 Model Building &amp; Scoring for Prediction | Statistical Foundations" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Model Building &amp; Scoring for Prediction | Statistical Foundations" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="IAA-Faculty/statistical_foundations" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Model Building &amp; Scoring for Prediction | Statistical Foundations" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="diagnostics.html"/>
<link rel="next" href="categorical-data-analysis.html"/>
<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a style="font-weight:bold" href="https://github.com/IAA-Faculty/statistical_foundations/">Statistical Foundations</a>
<img src="./img/iaaicon.png" alt="IAA"  class="center"</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#structure-of-the-book"><i class="fa fa-check"></i>Structure of the book</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-the-authors"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-stat.html"><a href="intro-stat.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-stat.html"><a href="intro-stat.html#eda"><i class="fa fa-check"></i><b>1.1</b> Exploratory Data Analysis (EDA)</a><ul>
<li class="chapter" data-level="1.1.1" data-path="intro-stat.html"><a href="intro-stat.html#vartypes"><i class="fa fa-check"></i><b>1.1.1</b> Types of Variables</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro-stat.html"><a href="intro-stat.html#dist"><i class="fa fa-check"></i><b>1.1.2</b> Distributions</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro-stat.html"><a href="intro-stat.html#normal"><i class="fa fa-check"></i><b>1.1.3</b> The Normal Distribution</a></li>
<li class="chapter" data-level="1.1.4" data-path="intro-stat.html"><a href="intro-stat.html#skew"><i class="fa fa-check"></i><b>1.1.4</b> Skewness</a></li>
<li class="chapter" data-level="1.1.5" data-path="intro-stat.html"><a href="intro-stat.html#kurt"><i class="fa fa-check"></i><b>1.1.5</b> Kurtosis</a></li>
<li class="chapter" data-level="1.1.6" data-path="intro-stat.html"><a href="intro-stat.html#graphdist"><i class="fa fa-check"></i><b>1.1.6</b> Graphical Displays of Distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro-stat.html"><a href="intro-stat.html#pointest"><i class="fa fa-check"></i><b>1.2</b> Point Estimates</a></li>
<li class="chapter" data-level="1.3" data-path="intro-stat.html"><a href="intro-stat.html#ci"><i class="fa fa-check"></i><b>1.3</b> Confidence Intervals</a></li>
<li class="chapter" data-level="1.4" data-path="intro-stat.html"><a href="intro-stat.html#hypotest"><i class="fa fa-check"></i><b>1.4</b> Hypothesis Testing</a><ul>
<li class="chapter" data-level="1.4.1" data-path="intro-stat.html"><a href="intro-stat.html#onesample"><i class="fa fa-check"></i><b>1.4.1</b> One-Sample T-Test</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="intro-stat.html"><a href="intro-stat.html#two-sample-t-tests"><i class="fa fa-check"></i><b>1.5</b> Two-Sample t-tests</a><ul>
<li class="chapter" data-level="1.5.1" data-path="intro-stat.html"><a href="intro-stat.html#testnorm"><i class="fa fa-check"></i><b>1.5.1</b> Testing Normality of Groups</a></li>
<li class="chapter" data-level="1.5.2" data-path="intro-stat.html"><a href="intro-stat.html#ftest"><i class="fa fa-check"></i><b>1.5.2</b> Testing Equality of Variances</a></li>
<li class="chapter" data-level="1.5.3" data-path="intro-stat.html"><a href="intro-stat.html#tsttest"><i class="fa fa-check"></i><b>1.5.3</b> Testing Equality of Means</a></li>
<li class="chapter" data-level="1.5.4" data-path="intro-stat.html"><a href="intro-stat.html#wilcoxon"><i class="fa fa-check"></i><b>1.5.4</b> Mann-Whitney-Wilcoxon Test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="slr.html"><a href="slr.html"><i class="fa fa-check"></i><b>2</b> Introduction to ANOVA and Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="slr.html"><a href="slr.html#evp"><i class="fa fa-check"></i><b>2.1</b> Prediction vs. Explanation</a></li>
<li class="chapter" data-level="2.2" data-path="slr.html"><a href="slr.html#trainvalidtest"><i class="fa fa-check"></i><b>2.2</b> Honest Assessment</a></li>
<li class="chapter" data-level="2.3" data-path="slr.html"><a href="slr.html#bivariate-eda"><i class="fa fa-check"></i><b>2.3</b> Bivariate EDA</a><ul>
<li class="chapter" data-level="2.3.1" data-path="slr.html"><a href="slr.html#continuous-continuous-associations"><i class="fa fa-check"></i><b>2.3.1</b> Continuous-Continuous Associations</a></li>
<li class="chapter" data-level="2.3.2" data-path="slr.html"><a href="slr.html#continuous-categorical-associations"><i class="fa fa-check"></i><b>2.3.2</b> Continuous-Categorical Associations</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="slr.html"><a href="slr.html#oneway"><i class="fa fa-check"></i><b>2.4</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="2.4.1" data-path="slr.html"><a href="slr.html#testing-assumptions"><i class="fa fa-check"></i><b>2.4.1</b> Testing Assumptions</a></li>
<li class="chapter" data-level="2.4.2" data-path="slr.html"><a href="slr.html#kruskal"><i class="fa fa-check"></i><b>2.4.2</b> Kruskal-Wallis</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="slr.html"><a href="slr.html#posthoc"><i class="fa fa-check"></i><b>2.5</b> ANOVA Post-hoc Testing</a><ul>
<li class="chapter" data-level="2.5.1" data-path="slr.html"><a href="slr.html#tukey"><i class="fa fa-check"></i><b>2.5.1</b> Tukey-Kramer</a></li>
<li class="chapter" data-level="2.5.2" data-path="slr.html"><a href="slr.html#dunnett"><i class="fa fa-check"></i><b>2.5.2</b> Dunnett’s Test</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="slr.html"><a href="slr.html#cor"><i class="fa fa-check"></i><b>2.6</b> Pearson Correlation</a><ul>
<li class="chapter" data-level="2.6.1" data-path="slr.html"><a href="slr.html#testcor"><i class="fa fa-check"></i><b>2.6.1</b> Statistical Test</a></li>
<li class="chapter" data-level="2.6.2" data-path="slr.html"><a href="slr.html#effect-of-anomalous-observations"><i class="fa fa-check"></i><b>2.6.2</b> Effect of Anomalous Observations</a></li>
<li class="chapter" data-level="2.6.3" data-path="slr.html"><a href="slr.html#the-correlation-matrix"><i class="fa fa-check"></i><b>2.6.3</b> The Correlation Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="slr.html"><a href="slr.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.7</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="2.7.1" data-path="slr.html"><a href="slr.html#slrassumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions of Linear Regression</a></li>
<li class="chapter" data-level="2.7.2" data-path="slr.html"><a href="slr.html#testing-for-association"><i class="fa fa-check"></i><b>2.7.2</b> Testing for Association</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="mlr.html"><a href="mlr.html"><i class="fa fa-check"></i><b>3</b> Complex ANOVA and Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.1" data-path="mlr.html"><a href="mlr.html#two-way-anova"><i class="fa fa-check"></i><b>3.1</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1.1" data-path="mlr.html"><a href="mlr.html#exploration"><i class="fa fa-check"></i><b>3.1.1</b> Exploration</a></li>
<li class="chapter" data-level="3.1.2" data-path="mlr.html"><a href="mlr.html#model"><i class="fa fa-check"></i><b>3.1.2</b> Model</a></li>
<li class="chapter" data-level="3.1.3" data-path="mlr.html"><a href="mlr.html#post-hoc-testing"><i class="fa fa-check"></i><b>3.1.3</b> Post-Hoc Testing</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mlr.html"><a href="mlr.html#two-way-anova-with-interactions"><i class="fa fa-check"></i><b>3.2</b> Two-Way ANOVA with Interactions</a><ul>
<li class="chapter" data-level="3.2.1" data-path="mlr.html"><a href="mlr.html#post-hoc-testing-1"><i class="fa fa-check"></i><b>3.2.1</b> Post-Hoc Testing</a></li>
<li class="chapter" data-level="3.2.2" data-path="mlr.html"><a href="mlr.html#assumptions"><i class="fa fa-check"></i><b>3.2.2</b> Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="mlr.html"><a href="mlr.html#randomized-block-design"><i class="fa fa-check"></i><b>3.3</b> Randomized Block Design</a><ul>
<li class="chapter" data-level="3.3.1" data-path="mlr.html"><a href="mlr.html#garlic-bulb-weight-example"><i class="fa fa-check"></i><b>3.3.1</b> Garlic Bulb Weight Example</a></li>
<li class="chapter" data-level="3.3.2" data-path="mlr.html"><a href="mlr.html#assumptions-1"><i class="fa fa-check"></i><b>3.3.2</b> Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="mlr.html"><a href="mlr.html#multiple-linear-regression"><i class="fa fa-check"></i><b>3.4</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="3.4.1" data-path="mlr.html"><a href="mlr.html#model-structure"><i class="fa fa-check"></i><b>3.4.1</b> Model Structure</a></li>
<li class="chapter" data-level="3.4.2" data-path="mlr.html"><a href="mlr.html#global-local-inference"><i class="fa fa-check"></i><b>3.4.2</b> Global &amp; Local Inference</a></li>
<li class="chapter" data-level="3.4.3" data-path="mlr.html"><a href="mlr.html#assumptions-2"><i class="fa fa-check"></i><b>3.4.3</b> Assumptions</a></li>
<li class="chapter" data-level="3.4.4" data-path="mlr.html"><a href="mlr.html#multiple-coefficients-of-determination"><i class="fa fa-check"></i><b>3.4.4</b> Multiple Coefficients of Determination</a></li>
<li class="chapter" data-level="3.4.5" data-path="mlr.html"><a href="mlr.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>3.4.5</b> Categorical Predictor Variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>4</b> Model Selection</a><ul>
<li class="chapter" data-level="4.1" data-path="model-selection.html"><a href="model-selection.html#selection-criteria"><i class="fa fa-check"></i><b>4.1</b> Selection Criteria</a></li>
<li class="chapter" data-level="4.2" data-path="model-selection.html"><a href="model-selection.html#stepwise-selection"><i class="fa fa-check"></i><b>4.2</b> Stepwise Selection</a><ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#forward"><i class="fa fa-check"></i>Forward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#backward"><i class="fa fa-check"></i>Backward</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#stepwise"><i class="fa fa-check"></i>Stepwise</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-selection.html"><a href="model-selection.html#significance-levels"><i class="fa fa-check"></i><b>4.3</b> Significance Levels</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="diagnostics.html"><a href="diagnostics.html"><i class="fa fa-check"></i><b>5</b> Diagnostics</a><ul>
<li class="chapter" data-level="5.1" data-path="diagnostics.html"><a href="diagnostics.html#examining-residuals"><i class="fa fa-check"></i><b>5.1</b> Examining Residuals</a></li>
<li class="chapter" data-level="5.2" data-path="diagnostics.html"><a href="diagnostics.html#misspecified-model"><i class="fa fa-check"></i><b>5.2</b> Misspecified Model</a></li>
<li class="chapter" data-level="5.3" data-path="diagnostics.html"><a href="diagnostics.html#constant-variance"><i class="fa fa-check"></i><b>5.3</b> Constant Variance</a></li>
<li class="chapter" data-level="5.4" data-path="diagnostics.html"><a href="diagnostics.html#normality"><i class="fa fa-check"></i><b>5.4</b> Normality</a></li>
<li class="chapter" data-level="5.5" data-path="diagnostics.html"><a href="diagnostics.html#correlated-errors"><i class="fa fa-check"></i><b>5.5</b> Correlated Errors</a></li>
<li class="chapter" data-level="5.6" data-path="diagnostics.html"><a href="diagnostics.html#influential-observations-and-outliers"><i class="fa fa-check"></i><b>5.6</b> Influential Observations and Outliers</a></li>
<li class="chapter" data-level="5.7" data-path="diagnostics.html"><a href="diagnostics.html#multicollinearity"><i class="fa fa-check"></i><b>5.7</b> Multicollinearity</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html"><i class="fa fa-check"></i><b>6</b> Model Building &amp; Scoring for Prediction</a><ul>
<li class="chapter" data-level="6.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#regularized-regression"><i class="fa fa-check"></i><b>6.1</b> Regularized Regression</a><ul>
<li class="chapter" data-level="6.1.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#penalties-in-models"><i class="fa fa-check"></i><b>6.1.1</b> Penalties in Models</a></li>
<li class="chapter" data-level="6.1.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ridge Regression</a></li>
<li class="chapter" data-level="6.1.3" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#lasso"><i class="fa fa-check"></i><b>6.1.3</b> LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Elastic Net</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#optimizing-penalties"><i class="fa fa-check"></i><b>6.2</b> Optimizing Penalties</a><ul>
<li class="chapter" data-level="6.2.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#cross-validation"><i class="fa fa-check"></i><b>6.2.1</b> Cross-Validation</a></li>
<li class="chapter" data-level="6.2.2" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#cv-in-regularized-regression"><i class="fa fa-check"></i><b>6.2.2</b> CV in Regularized Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#model-comparisons"><i class="fa fa-check"></i><b>6.3</b> Model Comparisons</a><ul>
<li class="chapter" data-level="6.3.1" data-path="model-building-scoring-for-prediction.html"><a href="model-building-scoring-for-prediction.html#test-dataset-comparison"><i class="fa fa-check"></i><b>6.3.1</b> Test Dataset Comparison</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html"><i class="fa fa-check"></i><b>7</b> Categorical Data Analysis</a><ul>
<li class="chapter" data-level="7.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#describing-categorical-data"><i class="fa fa-check"></i><b>7.1</b> Describing Categorical Data</a></li>
<li class="chapter" data-level="7.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#tests-of-association"><i class="fa fa-check"></i><b>7.2</b> Tests of Association</a></li>
<li class="chapter" data-level="7.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#measures-of-association"><i class="fa fa-check"></i><b>7.3</b> Measures of Association</a></li>
<li class="chapter" data-level="7.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#introduction-to-logistic-regression"><i class="fa fa-check"></i><b>7.4</b> Introduction to Logistic Regression</a><ul>
<li class="chapter" data-level="7.4.1" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#linear-probability-model"><i class="fa fa-check"></i><b>7.4.1</b> Linear Probability Model</a></li>
<li class="chapter" data-level="7.4.2" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#binary-logistic-regression"><i class="fa fa-check"></i><b>7.4.2</b> Binary Logistic Regression</a></li>
<li class="chapter" data-level="7.4.3" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#adding-categorical-variables"><i class="fa fa-check"></i><b>7.4.3</b> Adding Categorical Variables</a></li>
<li class="chapter" data-level="7.4.4" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#model-assessment"><i class="fa fa-check"></i><b>7.4.4</b> Model Assessment</a></li>
<li class="chapter" data-level="7.4.5" data-path="categorical-data-analysis.html"><a href="categorical-data-analysis.html#variable-selection-and-regularized-regression"><i class="fa fa-check"></i><b>7.4.5</b> Variable Selection and Regularized Regression</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Foundations</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-building-scoring-for-prediction" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Model Building &amp; Scoring for Prediction</h1>
<p>Chapters <a href="slr.html#slr">2</a> and <a href="mlr.html#mlr">3</a> only scratch the surface of model building. Linear regression is a great initial approach to take to model building. In fact, in the realm of statistical models, linear regression (calculated by ordinary least squares) is the <strong>best linear unbiased estimator</strong>. The two key pieces to that previous statement are “best” and “unbiased.”</p>
<p>What does it mean to be <strong>unbiased</strong>? Each of the sample coefficients (<span class="math inline">\(\hat{\beta}\)</span>’s) in the regression model are estimates of the true coefficients. Just like the statistics back in Chapter <a href="intro-stat.html#eda">1.1</a>, these sample coefficients have sampling distributions - specifically, normally distributed sampling distributions. The mean of the sampling distribution of <span class="math inline">\(\hat{\beta}_j\)</span> is the true (known) coefficient <span class="math inline">\(\beta_j\)</span>. This means the coefficient is unbiased.</p>
<p>What does it mean to be <strong>best</strong>? <em>IF</em> the assumptions of ordinary least squares are met fully, then the sampling distributions of the coefficients in the model have the <strong>minimum</strong> variance of all unbiased estimators.</p>
<p>These two things combined seem like what we want in a model - estimating what we want (unbiased) and doing it in a way that has the minimum amount of variation (best among the unbiased). Again, these rely on the assumptions of linear regression holding true. Another approach to regression would be to use <strong>regularized regression</strong> instead as a different approach to building the model.</p>
This Chapter aims to answer the following questions:
<ul>
<li>
What is regularized regression?
<ul>
<li>
Penalties in Modeling
<li>
Ridge Regression
<li>
LASSO
<li>
Elastic Net
</ul>
<li>
How do you optimize the penalty term?
<ul>
<li>
Overfitting
<li>
Cross-Validation (CV)
<li>
CV in Regularized Regression
</ul>
<li>
How do you compare different types of models?
<ul>
<li>
Model Metric
<li>
Model Scoring
<li>
Test Dataset Comparison
</ul>
</ul>
<div id="regularized-regression" class="section level2">
<h2><span class="header-section-number">6.1</span> Regularized Regression</h2>
<p>As the number of variables in a linear regression model increase, the chances of having a model that meets all of the assumptions starts to diminish. Multicollinearity can pose a large problem with bigger regression models. As previously seen in Chapter <a href="#diag"><strong>??</strong></a>, the coefficients of a linear regression vary widely in the presence of multicollinearity. These variations lead to overfitting of a regression model. <strong>Overfitting</strong> occurs when a dataset predicts the training data it was built off of really well, but does not generalize to the test dataset or the population in general. More formally, these models have higher variance than desired. In those scenarios, moving out of the realm of unbiased estimates may provide a lower variance in the model, even though the model is no longer unbiased as described above. We wouldn’t want to be too biased, but some small degree of bias might improve the model’s fit overall.</p>
<p>Another potential problem for linear regression is when we have more variables than observations in our dataset. This is a common problem in the space of genetic modeling. In this scenario, the ordinary least squares approach leads to multiple solutions instead of just one. Unfortunately, most of these infinite solutions overfit the problem at hand anyway.</p>
<p>Regularized (or penalized or shrinkage) regression techniques potentially alleviate these problems. Regularized regression puts constraints on the estimated coefficients in our model and <em>shrink</em> these estimates to zero. This helps reduce the variation in the coefficients (improving the variance of the model), but at the cost of biasing the coefficients. The specific constraints that are put on the regression inform the three common approaches - <strong>ridge regression</strong>, <strong>LASSO</strong>, and <strong>elastic nets</strong>.</p>
<div id="penalties-in-models" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Penalties in Models</h3>
<p>In ordinary least squares linear regression, we minimize the sum of the squared residuals (or errors) as laid out in Chapter @(slr).</p>
<p><span class="math display">\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2) = min(SSE)
\]</span></p>
<p>In regularized regression, however, we add a penalty term to the <span class="math inline">\(SSE\)</span> as follows:</p>
<p><span class="math display">\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2 + Penalty) = min(SSE + Penalty)
\]</span></p>
<p>As mentioned above, the penalties we choose constrain the estimated coefficients in our model and shrink these estimates to zero. Different penalties have different effects on the estimated coefficients. Two common approaches to adding penalties are the ridge and LASSO approaches. The elastic net approach is a combination of these two. Let’s explore each of these in further detail!</p>
</div>
<div id="ridge-regression" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Ridge Regression</h3>
<p>Ridge regression adds what is commonly referred to as an “<span class="math inline">\(L_2\)</span>” penalty:</p>
<p><span class="math display">\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p \hat{\beta}^2_j) = min(SSE + \lambda \sum_{j=1}^p \hat{\beta}^2_j)
\]</span></p>
<p>This penalty is controlled by the <strong>tuning parameter</strong> <span class="math inline">\(\lambda\)</span>. If <span class="math inline">\(\lambda = 0\)</span>, then we have typical OLS linear regression. However, as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the coefficients in the model shrink to zero. This makes intuitive sense. Since the estimated coefficients, <span class="math inline">\(\hat{\beta}_j\)</span>’s, are the only thing changing to minimize this equation, then as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the equation is best minimized by forcing the coefficients to be smaller and smaller. We will see how to optimize this penalty term in a later section.</p>
<p>Let’s build a regularized regression for our Ames dataset. To build a ridge regression we need separate data matrices for our predictors and our target variable. First, we isolate out the variables we are interested in using the <code>select</code> function. From there the <code>model.matrix</code> function will create any categorical dummy variables needed. We also isolate the target variable into its own vector.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb192-1" data-line-number="1">train_reg &lt;-<span class="st"> </span>train <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb192-2" data-line-number="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(Sale_Price, </a>
<a class="sourceLine" id="cb192-3" data-line-number="3">         Lot_Area,</a>
<a class="sourceLine" id="cb192-4" data-line-number="4">         Street,</a>
<a class="sourceLine" id="cb192-5" data-line-number="5">         Bldg_Type,</a>
<a class="sourceLine" id="cb192-6" data-line-number="6">         House_Style,</a>
<a class="sourceLine" id="cb192-7" data-line-number="7">         Overall_Qual,</a>
<a class="sourceLine" id="cb192-8" data-line-number="8">         Roof_Style,</a>
<a class="sourceLine" id="cb192-9" data-line-number="9">         Central_Air,</a>
<a class="sourceLine" id="cb192-10" data-line-number="10">         First_Flr_SF,</a>
<a class="sourceLine" id="cb192-11" data-line-number="11">         Second_Flr_SF,</a>
<a class="sourceLine" id="cb192-12" data-line-number="12">         Full_Bath,</a>
<a class="sourceLine" id="cb192-13" data-line-number="13">         Half_Bath,</a>
<a class="sourceLine" id="cb192-14" data-line-number="14">         Fireplaces,</a>
<a class="sourceLine" id="cb192-15" data-line-number="15">         Garage_Area,</a>
<a class="sourceLine" id="cb192-16" data-line-number="16">         Gr_Liv_Area, </a>
<a class="sourceLine" id="cb192-17" data-line-number="17">         TotRms_AbvGrd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb192-18" data-line-number="18"><span class="st">  </span><span class="kw">replace</span>(<span class="kw">is.na</span>(.), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb192-19" data-line-number="19"></a>
<a class="sourceLine" id="cb192-20" data-line-number="20">train_x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(Sale_Price <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_reg)[, <span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb192-21" data-line-number="21">train_y &lt;-<span class="st"> </span>train_reg<span class="op">$</span>Sale_Price</a></code></pre></div>
<p>We will want to do the same thing for the test dataset as well.</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1">test_reg &lt;-<span class="st"> </span>test <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb193-2" data-line-number="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(Sale_Price, </a>
<a class="sourceLine" id="cb193-3" data-line-number="3">         Lot_Area,</a>
<a class="sourceLine" id="cb193-4" data-line-number="4">         Street,</a>
<a class="sourceLine" id="cb193-5" data-line-number="5">         Bldg_Type,</a>
<a class="sourceLine" id="cb193-6" data-line-number="6">         House_Style,</a>
<a class="sourceLine" id="cb193-7" data-line-number="7">         Overall_Qual,</a>
<a class="sourceLine" id="cb193-8" data-line-number="8">         Roof_Style,</a>
<a class="sourceLine" id="cb193-9" data-line-number="9">         Central_Air,</a>
<a class="sourceLine" id="cb193-10" data-line-number="10">         First_Flr_SF,</a>
<a class="sourceLine" id="cb193-11" data-line-number="11">         Second_Flr_SF,</a>
<a class="sourceLine" id="cb193-12" data-line-number="12">         Full_Bath,</a>
<a class="sourceLine" id="cb193-13" data-line-number="13">         Half_Bath,</a>
<a class="sourceLine" id="cb193-14" data-line-number="14">         Fireplaces,</a>
<a class="sourceLine" id="cb193-15" data-line-number="15">         Garage_Area,</a>
<a class="sourceLine" id="cb193-16" data-line-number="16">         Gr_Liv_Area, </a>
<a class="sourceLine" id="cb193-17" data-line-number="17">         TotRms_AbvGrd) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb193-18" data-line-number="18"><span class="st">  </span><span class="kw">replace</span>(<span class="kw">is.na</span>(.), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb193-19" data-line-number="19"></a>
<a class="sourceLine" id="cb193-20" data-line-number="20">test_x &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(Sale_Price <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> test_reg)[, <span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb193-21" data-line-number="21">test_y &lt;-<span class="st"> </span>test_reg<span class="op">$</span>Sale_Price</a></code></pre></div>
<p>From there we use the <code>glmnet</code> function with the <code>x =</code> option where we specify the predictor model matrix and the <code>y =</code> option where we specify the target variable. The <code>alpha = 0</code> option specifies that a ridge regression will be used as defined in more detail below in the elastic net section. The <code>plot</code> function allows us to see the impact of the penalty on the coefficients in the model.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" data-line-number="1"><span class="kw">library</span>(glmnet)</a>
<a class="sourceLine" id="cb194-2" data-line-number="2"></a>
<a class="sourceLine" id="cb194-3" data-line-number="3">ames_ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> train_x,  <span class="dt">y =</span> train_y,  <span class="dt">alpha =</span> <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb194-4" data-line-number="4"></a>
<a class="sourceLine" id="cb194-5" data-line-number="5"><span class="kw">plot</span>(ames_ridge, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</a></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-68-1.png" width="672" /></p>
<p>The <code>glmnet</code> function automatically standardizes the variables before fitting the regression model. This is important so that all of the variables are on the same scale before adjustments are made to the estimated coefficients. Even with this standardization we can see the large coefficient values for some of the variables. The top of the plot lists how many variables are in the model at each value of penalty. This will never change for ridge regression, but does for LASSO.</p>
<p>What <span class="math inline">\(lambda\)</span> term is best? That will be discussed in the optimizing section below, but let’s discuss other possible penalties first.</p>
</div>
<div id="lasso" class="section level3">
<h3><span class="header-section-number">6.1.3</span> LASSO</h3>
<p><strong>Least absolute shrinkage and selection operator</strong> (LASSO) regression adds what is commonly referred to as an “<span class="math inline">\(L_1\)</span>” penalty:</p>
<p><span class="math display">\[
min(\sum_{i=1}^n(y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^p |\hat{\beta}_j|) = min(SSE + \lambda \sum_{j=1}^p |\hat{\beta}_j|)
\]</span></p>
<p>This penalty is controlled by the <strong>tuning parameter</strong> <span class="math inline">\(\lambda\)</span>. If <span class="math inline">\(\lambda = 0\)</span>, then we have typical OLS linear regression. However, as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the coefficients in the model shrink to zero. This makes intuitive sense. Since the estimated coefficients, <span class="math inline">\(\hat{\beta}_j\)</span>’s, are the only thing changing to minimize this equation, then as <span class="math inline">\(\lambda \rightarrow \infty\)</span>, the equation is best minimized by forcing the coefficients to be smaller and smaller. We will see how to optimize this penalty term in a later section.</p>
<p>However, unlike ridge regression that has the coefficient estimates approach zero asymptotically, in LASSO regression the coefficients can actually equal zero. This may not be as intuitive when initially looking at the penalty terms themselves. It becomes easier to see when dealing with the solutions to the coefficient estimates. Without going into too much mathematical detail, this is done by taking the derivative of the minimization function (objective function) and setting it equal to zero. From there we can determine the optimal solution for the estimated coefficients. In OLS regression the estimates for the coefficients can be shown to equal the following (in matrix form):</p>
<p><span class="math display">\[
\hat{\beta} = (X^TX)^{-1}X^TY
\]</span></p>
<p>This changes in the presence of penalty terms. For ridge regression, the solution becomes the following:</p>
<p><span class="math display">\[
\hat{\beta} = (X^TX + \lambda I)^{-1}X^TY
\]</span></p>
<p>There is no value for <span class="math inline">\(\lambda\)</span> that can force the coefficients to be zero by itself. Therefore, unless the data makes the coefficient zero, the penalty term can only force the estimated coefficient to zero asymptotically as <span class="math inline">\(\lambda \rightarrow \infty\)</span>.</p>
<p>However, for LASSO, the solution becomes the following:</p>
<p><span class="math display">\[
\hat{\beta} = (X^TX)^{-1}(X^TY - \lambda I)
\]</span></p>
<p>Notice the distinct difference here. In this scenario, there is a possible penalty value (<span class="math inline">\(\lambda = X^TY\)</span>) that will force the estimated coefficients to equal zero. There is some benefit to this. This makes LASSO also function as a variable selection criteria as well.</p>
<p>Let’s build a regularized regression for our Ames dataset using the LASSO approach. To build a LASSO regression we need separate data matrices for our predictors and our target variable just like we did for ridge. From there we use the <code>glmnet</code> function with the <code>x =</code> option where we specify the predictor model matrix and the <code>y =</code> option where we specify the target variable. The <code>alpha = 1</code> option specifies that a LASSO regression will be used as defined in more detail below in the elastic net section. The <code>plot</code> function allows us to see the impact of the penalty on the coefficients in the model.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb195-1" data-line-number="1">ames_lasso &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> train_x,  <span class="dt">y =</span> train_y,  <span class="dt">alpha =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb195-2" data-line-number="2"></a>
<a class="sourceLine" id="cb195-3" data-line-number="3"><span class="kw">plot</span>(ames_lasso, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</a></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<p>The <code>glmnet</code> function automatically standardizes the variables before fitting the regression model. This is important so that all of the variables are on the same scale before adjustments are made to the estimated coefficients. Even with this standardization we can see the large coefficient values for some of the variables. The top of the plot lists how many variables are in the model at each value of penalty. Notice as the penalty increases, the number of variables decreases as variables are forced to zero.</p>
<p>What <span class="math inline">\(lambda\)</span> term is best? That will be discussed in the optimizing section below, but let’s discuss the last possible penalty first - the combination of both ridge and LASSO.</p>
</div>
<div id="elastic-net" class="section level3">
<h3><span class="header-section-number">6.1.4</span> Elastic Net</h3>
<p>Which approach is better, ridge or LASSO? Both have advantages and disadvantages. LASSO performs variable selection while ridge keeps all variables in the model. However, reducing the number of variables might impact minimum error. Also, if you have two correlated variables, which one LASSO chooses to zero out is relatively arbitrary to the context of the problem.</p>
<p>Elastic nets were designed to take advantage of both penalty approaches. In elastic nets, we are using both penalties in the minimization:</p>
<p><span class="math display">\[
min(SSE + \lambda_1 \sum_{j=1}^p |\hat{\beta}_j| + \lambda_2 \sum_{j=1}^p \hat{\beta}^2_j) 
\]</span></p>
<p>In R, the <code>glmnet</code> function takes a slightly different approach to the elastic net implementation with the following:</p>
<p><span class="math display">\[
min(SSE + \lambda[ \alpha \sum_{j=1}^p |\hat{\beta}_j| + (1-\alpha) \sum_{j=1}^p \hat{\beta}^2_j]) 
\]</span></p>
<p>R still has one penalty <span class="math inline">\(\lambda\)</span>, however, it includes the <span class="math inline">\(\alpha\)</span> parameter to balance between the two penalty terms. This is why in <code>glmnet</code>, the <code>alpha = 1</code> option gives a LASSO regression and <code>alpha = 0</code> gives a ridge regression. Any value in between zero and one will provide an elastic net.</p>
<p>Let’s build a regularized regression for our Ames dataset using the elastic net approach with an <span class="math inline">\(\alpha = 0.5\)</span>. To build am elastic net we need separate data matrices for our predictors and our target variable just like we did for ridge and LASSO. From there we use the <code>glmnet</code> function with the <code>x =</code> option where we specify the predictor model matrix and the <code>y =</code> option where we specify the target variable. The <code>alpha = 0.5</code> option specifies that an elastic net will be used since it is between zero and one. The <code>plot</code> function allows us to see the impact of the penalty on the coefficients in the model.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" data-line-number="1">ames_en &lt;-<span class="st"> </span><span class="kw">glmnet</span>(<span class="dt">x =</span> train_x,  <span class="dt">y =</span> train_y,  <span class="dt">alpha =</span> <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb196-2" data-line-number="2"></a>
<a class="sourceLine" id="cb196-3" data-line-number="3"><span class="kw">plot</span>(ames_en, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</a></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<p>The <code>glmnet</code> function automatically standardizes the variables before fitting the regression model. This is important so that all of the variables are on the same scale before adjustments are made to the estimated coefficients. Even with this standardization we can see the large coefficient values for some of the variables. The top of the plot lists how many variables are in the model at each value of penalty. Notice as the penalty increases, the number of variables decreases as variables are forced to zero using the LASSO piece of the elastic net penalty.</p>
<p>What <span class="math inline">\(lambda\)</span> term is best? What is the proper balance between ridge and LASSO penalties when building an elastic net? That will be discussed in the following section.</p>
</div>
</div>
<div id="optimizing-penalties" class="section level2">
<h2><span class="header-section-number">6.2</span> Optimizing Penalties</h2>
<p>No matter the approach listed above, a penatly term <span class="math inline">\(\lambda\)</span> needs to be picked. However, we do not want to get caught overfitting our training data by minimizing the variance so much that it is not generalizable to the overall pattern and other similar data. Take the following plot:</p>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<p>The red line is overfitted to the dataset and picks up too much of the unimportant pattern. The orange dotted line is underfit as it does not pick up enough of the pattern. The light blue, solid line is fit well to the dataset as it picks up the general pattern while not overfitting to the dataset.</p>
<div id="cross-validation" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Cross-Validation</h3>
<p><strong>Cross-validation</strong> is a common approach in modeling to prevent overfitting of data when you need to <strong>tune</strong> a parameter. The idea of cross-validation is to split the training data into multiple pieces, build the model on a majority of the pieces while evaluating it on the remaining piece. Then we do the same process again, but switch out which pieces the model is built and evaluated on.</p>
<p>A common cross-validation (CV) approach is the <span class="math inline">\(k\)</span>-fold CV. In the <span class="math inline">\(k\)</span>-fold CV approach, the model is built <span class="math inline">\(k\)</span> times. The data is initially split into <span class="math inline">\(k\)</span> equally sized pieces. Each time the model is built, it is built off of <span class="math inline">\(k-1\)</span> pieces of the data and evaluated on the last piece. This process is repeated until each piece is left out for evaluation. This is diagrammed below in Figure <a href="model-building-scoring-for-prediction.html#fig:kCV">6.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:kCV"></span>
<img src="img/kCV.png" alt="Example of a 10-fold Cross-Validation" width="50%" />
<p class="caption">
Figure 6.1: Example of a 10-fold Cross-Validation
</p>
</div>
</div>
<div id="cv-in-regularized-regression" class="section level3">
<h3><span class="header-section-number">6.2.2</span> CV in Regularized Regression</h3>
<p>In R, the <code>cv.glmnet</code> function will automatically implement a 10-fold CV (by default, but can be adjusted through options) to help evaluate and optimize the <span class="math inline">\(\lambda\)</span> values for our regularized regression models.</p>
<p>Let’s perform an example using the LASSO regression. The <code>cv.glmnet</code> function takes the same inputs as the <code>glmnet</code> function above. Again, we will use the <code>plot</code> function, but this time we get a different plot.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb197-1" data-line-number="1">ames_lasso_cv &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(<span class="dt">x =</span> train_x,  <span class="dt">y =</span> train_y,  <span class="dt">alpha =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb197-2" data-line-number="2"></a>
<a class="sourceLine" id="cb197-3" data-line-number="3"><span class="kw">plot</span>(ames_lasso_cv)</a></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb198-1" data-line-number="1">ames_lasso_cv<span class="op">$</span>lambda.min </a></code></pre></div>
<pre><code>## [1] 49.69435</code></pre>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb200-1" data-line-number="1">ames_lasso_cv<span class="op">$</span>lambda<span class="fl">.1</span>se</a></code></pre></div>
<pre><code>## [1] 3588.334</code></pre>
<p>The above plot shows the results from our cross-validation. Here the models are evaluated based on their <strong>mean-squared error</strong> (MSE). The MSE is defined as <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2\)</span>. The <span class="math inline">\(\lambda\)</span> value that minimizes the MSE is 555.064 (with a <span class="math inline">\(\log(\lambda)\)</span> = 6.32). This is highlighted by the first, vertical dashed line. The second vertical dashed line is the largest <span class="math inline">\(\lambda\)</span> value that is one standard error above the minimum value. This value is especially useful in LASSO regressions. The largest <span class="math inline">\(\lambda\)</span> within one standard error would provide approximately the same MSE, but with a further reduction in the number of variables. Notice that to go from the first line to the second, the change in MSE is very small, but the reduction of variables is from around 25 variables to 7 variables.</p>
<p>Let’s look at the impact on the coefficients under this penalty using the <code>glmnet</code> function as before.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb202-1" data-line-number="1"><span class="kw">plot</span>(ames_lasso, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>)</a>
<a class="sourceLine" id="cb202-2" data-line-number="2"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">log</span>(ames_lasso_cv<span class="op">$</span>lambda<span class="fl">.1</span>se), <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)</a>
<a class="sourceLine" id="cb202-3" data-line-number="3"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">log</span>(ames_lasso_cv<span class="op">$</span>lambda.min), <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lty =</span> <span class="st">&quot;dashed&quot;</span>)</a></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
<p>To investigate which variables are important at a <span class="math inline">\(\lambda\)</span> value, we can view the coefficients using the <code>coef</code> function. They are ranked here:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb203-1" data-line-number="1"><span class="kw">coef</span>(ames_lasso, <span class="dt">s =</span> ames_lasso_cv<span class="op">$</span>lambda<span class="fl">.1</span>se) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb203-2" data-line-number="2"><span class="st">  </span>broom<span class="op">::</span><span class="kw">tidy</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb203-3" data-line-number="3"><span class="st">  </span><span class="kw">filter</span>(row <span class="op">!=</span><span class="st"> &quot;(Intercept)&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb203-4" data-line-number="4"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(value, <span class="kw">reorder</span>(row, value))) <span class="op">+</span></a>
<a class="sourceLine" id="cb203-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb203-6" data-line-number="6"><span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;Influential Variables&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb203-7" data-line-number="7"><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Coefficient&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb203-8" data-line-number="8"><span class="st">  </span><span class="kw">ylab</span>(<span class="ot">NULL</span>)</a></code></pre></div>
<pre><code>## Warning: &#39;tidy.dgCMatrix&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>## Warning: &#39;tidy.dgTMatrix&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<p>The variable describing the overall quality of the home is the driving factor of this model as well as the other variables listed above.</p>
<p>A similar approach can be taken for CV with ridge regression using the same structure of code. That will not be covered here. Elastic nets are more complicated in that they have multiple parameters to optimize. For that approach, an optimization grid will need to be structured to evaluate different <span class="math inline">\(\lambda\)</span> values across different <span class="math inline">\(\alpha\)</span> values. A loop can be set up to run the <code>cv.glmnet</code> function across many different values of <span class="math inline">\(\alpha\)</span>. That will not be covered in detail here.</p>
</div>
</div>
<div id="model-comparisons" class="section level2">
<h2><span class="header-section-number">6.3</span> Model Comparisons</h2>
<p>Now we have multiple models built for our dataset. To help evaluate which model is better, we will use the test dataset as described in Chapter @(eda).</p>
<p>The models we have built are nothing but formulas. All we have to do is put the test dataset in the formula to predict/score the test data. We <strong>do not</strong> rerun the algorithm as the goal is <strong>not</strong> to fit the test dataset, but to just score it. We need to make sure that we have the same structure to the test dataset that we do with the training dataset. Any variable transformations, new variable creations, and missing value imputations done on the training dataset must be done on the test dataset in the same way.
### Model Metrics
Once the predicted values are obtained from each model we need to evaluate good these predictions are. There are many different metrics to evaluate models depending on what type of target variable that you have. Some common metrics for continuous target variables are the square root of the mean squared error (RMSE), the mean absolute error (MAE), and mean absolute percentage error (MAPE).</p>
<p>The RMSE is evaluated as follows:</p>
<p><span class="math display">\[
RMSE = \sqrt {\frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
\]</span>
The RMSE is an approximation of the standard deviation of the prediction errors of the model. The downside of the RMSE is a lack of interpretability.</p>
<p>The MAE is evaluated as follows:</p>
<p><span class="math display">\[
MAE = \frac{1}{n} \sum_{i=1}^n |y_i - \hat{y}_i|
\]</span>
The MAE gives the average absolute difference between our predictions and the actual values. This is a symmetric measure with great interpretability. The main disadvantage of this metric is that it depends on the scale of the data. For comparing two models evaluated on the same data, this isn’t important. However, when comparing across different datasets, this may not be as helpful. For example, in temperature predictions, having an MAE of five degrees for a model built on Honolulu, Hawaii weather might not be comparable to a model built on weather in Raleigh, North Carolina.</p>
<p>The MAPE is evaluated as follows:</p>
<p><span class="math display">\[
MAPE = 100 \times \frac{1}{n} \sum_{i=1}^n |\frac{y_i - \hat{y}_i}{y_i}|
\]</span>
The MAPE gives the average absolute <em>percentage</em> difference between our predictions and the actual values. This metric is very interpretable and not dependent on the scale of the data. However, it is not symmetric like the MAE.</p>
<div id="test-dataset-comparison" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Test Dataset Comparison</h3>
<p>The final model we had from Chapter @(diag) had the variables . From this model we can use the <code>predict</code> function with the <code>newdata =</code> option to use score the <code>test</code> dataset.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb206-1" data-line-number="1">test<span class="op">$</span>pred_lm &lt;-<span class="st"> </span><span class="kw">predict</span>(ames_lm, <span class="dt">newdata =</span> test)</a>
<a class="sourceLine" id="cb206-2" data-line-number="2"></a>
<a class="sourceLine" id="cb206-3" data-line-number="3"><span class="kw">head</span>(test<span class="op">$</span>pred_lm)</a></code></pre></div>
<pre><code>##        1        2        3        4        5        6 
## 142107.3 142107.3 228909.6 142107.3 142107.3 142107.3</code></pre>
<p>To get predictions from the regularized regression models, a <span class="math inline">\(\lambda\)</span> value must be selected. For the previous LASSO regression we will choose the largest <span class="math inline">\(\lambda\)</span> value within one standard error of the minimum <span class="math inline">\(\lambda\)</span> value to help reduce the number of variables. Again, we will use the <code>predict</code> function. The <code>s =</code> option is where we input the <span class="math inline">\(\lambda\)</span> value. The <code>newx =</code> option is where we specify the <code>test</code> dataset.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb208-1" data-line-number="1">test_reg<span class="op">$</span>pred_lasso &lt;-<span class="st"> </span><span class="kw">predict</span>(ames_lasso, <span class="dt">s =</span> ames_lasso_cv<span class="op">$</span>lambda<span class="fl">.1</span>se, <span class="dt">newx =</span> test_x)</a>
<a class="sourceLine" id="cb208-2" data-line-number="2"></a>
<a class="sourceLine" id="cb208-3" data-line-number="3"><span class="kw">head</span>(test_reg<span class="op">$</span>pred_lasso)</a></code></pre></div>
<pre><code>##          1
## 1 156677.8
## 2 172432.5
## 3 239922.1
## 4 105713.6
## 5 200908.8
## 6 124913.5</code></pre>
<p>Now we need to calculate the MAE and MAPE for each model for comparison.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb210-1" data-line-number="1">test &lt;-<span class="st"> </span>test <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb210-2" data-line-number="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lm_APE =</span> <span class="dv">100</span><span class="op">*</span><span class="kw">abs</span>((Sale_Price <span class="op">-</span><span class="st"> </span>pred_lm)<span class="op">/</span>Sale_Price)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb210-3" data-line-number="3"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(<span class="dt">MAPE_lm =</span> <span class="kw">mean</span>(lm_APE))</a>
<a class="sourceLine" id="cb210-4" data-line-number="4"></a>
<a class="sourceLine" id="cb210-5" data-line-number="5">test_reg &lt;-<span class="st"> </span>test_reg <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb210-6" data-line-number="6"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">lasso_APE =</span> <span class="dv">100</span><span class="op">*</span><span class="kw">abs</span>((Sale_Price <span class="op">-</span><span class="st"> </span>pred_lasso)<span class="op">/</span>Sale_Price)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb210-7" data-line-number="7"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(<span class="dt">MAPE_lasso =</span> <span class="kw">mean</span>(lasso_APE))</a></code></pre></div>
<p>From the above results, the linear regression from OLS has a lower MAE and MAPE.</p>
<p>Once we have scored models with the test dataset, we should <strong>not</strong> go back to try and rebuild any models. We will use the model with the lowest MAE or MAPE. This number is also the number that we report on how well our model performs. No metrics on the training dataset should be reported for the performance of the model.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="diagnostics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="categorical-data-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/edit/master/06-model_building_scoring.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/IAA-Faculty/statistical_foundations.git/blob/master/06-model_building_scoring.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section",
"toc": null
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
