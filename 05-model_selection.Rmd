# Model Selection

When creating a multiple linear regression model, your goal is to find the best model possible.  However, within the data, there most likely exists variables that are informative and ones that are uninformative in predicting the response.  With many explanatory variables, it could be extremely time consuming to try all potential models by hand, and the use of automatic procedures can greatly assist in obtaining subsets of variables in which to focus your attention.  

CAUTION: you should NEVER just use the final model created from an automatic procedure!  Always double check the variables (look at those included and those not included).  Use content knowledge, common sense, diagnostics, etc to decide on your final model.

In this Chapter, we will focus on two techniques for automatic variable selection: stepwise procedures and LASSO/Ridge. Within the stepwise procedures, we will focus on forward, backward and stepwise searches using several different selection criteria.  We will end this section discussing important considerations in use of p-values when dealing with large data sets.


## Selection Criteria

when trying to find the best model, we can use different criteria to choose.  For exmaple, you have already seen $R^{2}$ (the larger value of $R^{2}$ the better the model).  However, when comparing models, the adjusted $$R^{2}$$ is better due to the fact that $R^{2}$ can potentially increase even when adding noise.  The adjusted $R^{2}$ can be thought of as $R^{2}$ with penalty (for every additional variable added to the model, we add a higher penalty).  This allows us to weigh the contribution of adding new variables against the added complexity of more variables in the model.  There are other selection criteria that are also used in selecting the "best model" (or variable selection).  As you will notice, these selection criteria also have a penalty to take into account the addition of variables.  We wil focus on the two most common ones....AIC and BIC (or also referred to as SBC).

The AIC, or Akaike information criterion, was developed by statistician Hirotugu Akaike back in the 1970's and is defined by 
$$ AIC = -2log(Likelihood) + 2p.  $$  In this case, the "penalty" is 2p, where p is the number of parameters in the model.

BIC, also known as the Bayesian Information Criterion (also called SBC or Schwarz Bayesian Information) was first developed by Gideon E. Schwarz and is defined by
$$BIC = nlog(SSE/n) + plog(n). $$ In this case, the "penalty" is plog(n), where p is the number of parameters in the model and n is the sample size.

Notice that both penalties are multiplied by p.  Keep these quantities  (2 for AIC and log(n) for BIC) in your mind for later...you will be seeing them again.


## Stepwise Selection

We will talk about three different algorithms in the stepwise selection search: forward, backward and stepwise.  Each of these algorithms either add or take away one variable at a time based on a given criterion until this criterion can no longer be met.  At which point the algorithm stops.  

```{r libraries, prompt=TRUE, eval=FALSE}
library(AmesHousing)
library(tidyverse)
library(car)
library(DescTools)
library(corrplot)
library(glmnet)
library(leaps)
```

### Forward {.unnumbered}

For forward selection, we start with a null model (only the intercept) and adds one variable at a time until no other variables can be added based on a given criterion.  The algorithm is as follows

(0) Start with a null model
(1) Create p simple linear regressions
  Loop starts here
  (a) See which linear regression is best (based on criterion)
  (b) is this regression better than the regression in the previous step?  If yes, keep this variable.  If no, algorithm stops.

```{r forward, prompt=TRUE, eval=FALSE}


# k = 2 for AIC selection
for.model <- step(empty.model,
                  scope = list(lower = empty.model,
                               upper = full.model),
                  direction = "forward", k = 2) 
# k = log(n) for BIC selection
for.model2 <- step(empty.model,
                   scope = list(lower = empty.model,
                               upper = full.model),
                   direction = "forward", k = log(nrow(train_sel))) # k = qchisq(alpha, 1, lower.tail = FALSE) for p-value with alpha selection
alpha.f=0.05
for.model3 <- step(empty.model,
                   scope = list(lower = empty.model,
                                upper = full.model),
                   direction = "forward", k = qchisq(alpha.f, 1, lower.tail = FALSE)) 

```


### Backward {.unnumbered}

### Stepwise {.unnumbered}

### LASSO {.unnumbered}


## Significance Levels