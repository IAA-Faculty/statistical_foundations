# Complex ANOVA and Multiple Linear Regression {#mlr}
In in Chapter \@ref(slr) we were introduced to the One-Way ANOVA and simple linear regression models. These models only contained a single variable - categorical for ANOVA and continuous for simple linear regression - to explain and predict our target variable. Rarely do we believe that only a single variable will suffice in predicting a variable of interest. Here in this Chapter we will generalize these models to the $n$-Way ANOVA and multiple linear regression models. These models contain multiple sets of variables to explain and predict our target variable.

This Chapter aims to answer the following questions:
<ul>
  <li> How do we include multiple variables in ANOVA? 
    <ul> 
      <li> Exploration
      <li> Assumptions
      <li> Predictions
    </ul>
  <li> What is an interaction between two predictor variables?
    <ul>
      <li> Interpretation
      <li> Evaluation
      <li> Within Category Effects
    </ul>
  <li> What is blocking in ANOVA?
    <ul>
      <li> Nuisance Factors
      <li> Differences Between Blocking and Two-Way ANOVA
    </ul>
  <li> How do we include multiple variables in regression?
    <ul>
      <li> Model Structure
      <li> Global & Local Inference
      <li> Assumptions
      <li> Adjusted $R^2$
      <li> Categorical Variables in Regression
    </ul>
</ul>

## Two-Way ANOVA
Section \@ref(slr) details the One-Way ANOVA model using a single categorical predictor variable with $k$ levels to predict our continuous target variable. Now we will generalize this model to include $n$ categorical variables that each have different numbers of levels ($k_1, k_2, ..., k_n$).

### Exploration
Let's use the basic example of two categorical predictor variables in a Two-Way ANOVA. Previously, we talked about using heating quality as a factor to explain and predict sale price of homes in Ames, Iowa. Now, we also consider whether the home has central air. Although similar in nature, these two factors potentially provide important, unique pieces of information about the home. Similar to previous data science problems, let us first explore our variables and their potential relationships.

Now that we have two variables that we will use to explain and predict sale price, here are some summary statistics (mean, standard deviation, minimum, and maximum) for each combination of category. We will use the ```group_by``` function on both predictor variables of interest to split the data and then the ```summarise``` function to calculate the metrics we are interested in.

```{r}
train %>% 
  group_by(Heating_QC, Central_Air) %>%
  summarise(mean = mean(Sale_Price), 
            sd = sd(Sale_Price), 
            max = max(Sale_Price), 
            min = min(Sale_Price))

```

We can already see above that there appears to be some differences in average sale price across the categories overall. Within each grouping of heating quality, homes with central air appear to have a higher sale price than homes without. Also, similar to before, homes with higher heating quality appear to have higher sale prices compared to homes with lower heating quality. 

We also see these relationships in the bar chart in Figure \@ref(fig:twomeans).

```{r fig=T, fig.align='center', fig.cap = 'Distribution of Variables Heating_QC and Central_Air', label='twomeans'}
CA_heat <- train %>% 
  group_by(Heating_QC, Central_Air) %>%
  summarise(mean = mean(Sale_Price), 
            sd = sd(Sale_Price), 
            max = max(Sale_Price), 
            min = min(Sale_Price))

ggplot(data = CA_heat, aes(x = Heating_QC, y = mean/1000, fill = Central_Air)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(y = "Sales Price (Thousands $)", x = "Heating Quality Category") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()
```
As before, visually looking at bar charts and mean calculations only goes so far. We need to statistically be sure of any differences that exist between average sale price in categories.

### Model
We are going to do this with the same approach as in the One-Way ANOVA of Chapter \@ref(slr), just with more variables as shown in the following equation:

$$
Y_{ijk} = \mu + \alpha_i + \beta_j + \varepsilon_{ijk}
$$
where $\mu$ is the average baseline sales price of a home in Ames, Iowa, $\alpha_i$ is the variable representing the impacts of the levels of heating quality, and $\beta_j$ is the variable representing the impacts of the levels of central air. As mentioned previously, the unexplained error in this model is represented as $\varepsilon_{ijk}$.

The same F test approach is also used, just for each one of the variables. Each variable's test has a null hypothesis assuming all categories have the same mean. The alternative for each test is that at least one category's mean is different.

Let's view the results of the ```aov``` function.

```{r}
ames_aov2 <- aov(Sale_Price ~ Heating_QC + Central_Air, data = train)

summary(ames_aov2)
```
From the above results, we have low p-values for each of the variables' F test. Heating quality had 4 degrees of freedom, derived from the 5 categories $(4 = 5-1)$. Similarly, central air's 2 categories produce 1 $(= 2-1)$ degree of freedom. The F values are calculated the exact same way as described before with the mean square for each variable divided by the mean square error. Based on these tests, at least one category in each variable is statistically different than the rest.

### Post-Hoc Testing
As with the One-Way ANOVA, the next logical question is which of these categories is different. We will use the same post-hoc tests as before with the ```TukeyHSD``` function.

```{r}
tukey.ames2 <- TukeyHSD(ames_aov2)
print(tukey.ames2)

plot(tukey.ames2, las = 1)
```
Starting with the variable for central air, we can see there is a statistical difference between the two categories. This is the exact same result as the overall F test for the variable since there are only two categories. For the heating quality variable, we can see some categories are different from each other, while others are not. Noticeably, the combination of poor with fair, good, and typical categories are **not** statistically different. Notice also the different widths of these confidence intervals do to the different combinations of sample sizes for the categories being tested.

## Two-Way ANOVA with Interactions
What if the relationship between a predictor and target variable changed depending on the value of another predictor variable? For our example, we would say that the average difference in sales price between having central air and not having central changed depending on what level of heating quality the home had. In the bar chart in Figure \@ref(fig:twomeans), an interaction effect is displayed when the differences between the two bars within each heating category is different across heating category. If the difference, was the same, then there is no interaction present. In other words, no matter the heating quality rating of the home, the average difference in sales price between having central air and not having central air is the same.

This interaction model is represented as follows:

$$
Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \varepsilon_{ijk}
$$

with the interaction effect, $(\alpha \beta)_{ij}$, as the multiplication of the two variables involved in the interaction. Interactions can occur between more than two variables as well. Interactions are good to evaluate as they can mask the effects of individual variables. For example, imagine a hypothetical example as shown in Figure \@ref(fig:twomeansint) where the impact of having central air is opposite depending on which category of heating quality you have.

```{r}
fake_HQ <- c("Poor", "Poor", "Excellent", "Excellent")
fake_CA <- c("N", "Y", "N", "Y")
fake_mean <- c(100, 150, 150, 100)

fake_df <- as.data.frame(cbind(fake_HQ, fake_CA, fake_mean))
```

```{r fig=T, fig.align='center', fig.cap = 'Distribution of Variables Heating_QC and Central_Air', label='twomeansint'}
ggplot(data = fake_df, aes(x = fake_HQ, y = as.numeric(fake_mean), fill = fake_CA)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(y = "Fake Sales Price (Thousands $)", x = "Fake Heating Quality Category") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()
```

If you were to only look at the average sales price across heating quality, you would notice no difference between the two groups (average for both heating categories is 125). However, when the interaction is accounted for, you can clearly see in the bar heights that sales price is different depending on the value of central air.

Let's evaluate the interaction term in our actual data. To do so, we just incorporate the multiplication of the two variables in the model statement by using the formula ```Sale_Price ~ Heating_QC + Central_Air + Heating_QC:Central_Air```. You could also use the shorthand version of this by using the formula ```Sale_Price ~ Heating_QC*Central_Air```. The ```*``` will include both main effects (the individual variables) and the interaction between them.

```{r}
ames_aov_int <- aov(Sale_Price ~ Heating_QC*Central_Air, data = train)

summary(ames_aov_int)
```
As seen by the output above, the interaction effect between heating quality and central air is significant at the 0.05 level. Again, this implies that the average difference in sale price of the home between having central air and not differs depending on which category of heating quality the home belongs to. If our interaction was **not** significant (say a 0.02 significance level instead) then we would remove the interaction term from our model and rerun the analysis.

### Post-Hoc Testing
Post-hoc tests are also available for interaction models as well to determine where the statistical differences exist in all the combinations of possible categories. We evaluate these post-hoc tests using the same ```TukeyHSD``` function and its corresponding ```plot``` element.

```{r}
tukey.ames_int <- TukeyHSD(ames_aov_int)
print(tukey.ames_int)

plot(tukey.ames_int, las = 1)
```

In the giant table above as well as the confidence interval plots, you are able to inspect which combination of categories are statistically different.

With interactions present in ANOVA models, post-hoc tests might get overwhelming in trying to find where differences exist. To help guide the exploration of post-hoc tests with interactions, we can do **slicing**. Slicing is when you perform One-Way ANOVA on subsets of data within categories of other variables. For example, to help discover differences in the interaction between central air and heating quality, we could subset the data into two groups - homes with central air and homes without. Within these two groups we perform One-Way ANOVA across heating quality to find where differences might exist within subgroups.

This can easily be done with the ```group_by``` function to subset the data. The ```nest``` and ```mutate``` functions are also used to applied the ```aov``` function to each subgroup. Here we run a One-Way ANOVA for heating quality within each subset of central air being present or not.

```{r}
CA_aov <- train %>% 
  group_by(Central_Air) %>%
  nest() %>%
  mutate(aov = map(data, ~summary(aov(Sale_Price ~ Heating_QC, data = .x))))

CA_aov
print(CA_aov$aov)
```
We can see that both of these results are significant at the 0.05 level. However, if our significance level was 0.02 instead, only the first One-Way ANOVA would be significant. This implies that there are statistical differences in average sale price across heating quality within homes that have central air, but not in homes that do not (again at the 0.02 level). 

### Assumptions
The assumptions for the $n$-Way ANOVA are the same as with the One-Way ANOVA - independence of observations, normality for each category of variable, and equal variances. With the inclusion of two or more variables ($n$-Way ANOVA with $n > 1$), these assumptions can be harder to evaluate and test. These assumptions are then applied to the residuals of the model. 

For equal variances, we can still apply the Levene Test for equal variances using the same ```leveneTest``` function as in Chapter \@ref(slr).

```{r}
leveneTest(Sale_Price ~ Heating_QC*Central_Air, data = train)
```
From this test, we can see that we **do not** meet our assumption of equal variance. 

Let's explore the normality assumption. Again, we will assume this normality on the random error component, $\varepsilon_{ijk}$, of the ANOVA model. More details are found for diagnostic testing using the error component in Chapter \@ref(diag). We can check normality using the same approaches of the QQ-plot or statistical testing as in Chapter \@ref(eda). Here we will use the ```plot``` function on the ```aov``` object. Specifically, we want the second plot which is why we have a ```2``` in the ```plot``` function option. We then use the ```shapiro.test``` function on the error component. The estimate of the error component is calculated using the ```residuals``` function.

```{r}
plot(ames_aov_int, 2)

ames_res <- residuals(ames_aov_int)

shapiro.test(x = ames_res)
```
Neither of the normality or equal variance assumptions appear to be met here. This would be a good scenario to have a non-parametric approach. Unfortunately, the Kruskal-Wallis approach is not applicable to $n$-Way ANOVA where $n > 1$. These approaches would need more non-parametric versions of multiple regression models to account for them.

## Randomized Block Design
There are two typical groups of data analysis studies that are conducted. The first are observational/retrospective studies which are the typical data problems people try to solve. The primary characteristic of these analysis are looking at what already happened (retrospective) and potentially inferring those results to further data. These studies have little control over other factors contributing to the target of interest because data is collected after the fact.

The other type of data analysis study are controlled experiments. In these situations, you often want to look at the outcome measure prospectively. The focus of the controlled experiment is to control for other factors that might contribute to the target variable. By manipulating these factors of interest, one can more reasonably claim causation. We can more reasonably claim causation when random assignment is used to eliminate potential **nuisance factors**. Nuisance factors are variables that can potentially impact the target variable, but are not of interest in the analysis directly. 

### Garlic Bulb Weight Example
For this analysis we will use a new dataset. This dataset contains the average garlic bulb weight from different plots of land. We want to compare the effects of fertilizer on average bulb weight. However, different plots of land could have different levels of sun exposure, pH for the soil, and rain amounts. Since we cannot alter the pH of the soil easily, or control the sun and rain, we can use blocking to account for these nuisance factors. Each fertilizer was randomly applied in quadrants of 8 plots of land. These 8 plots have different values for sun exposure, pH, and rain amount. Therefore, if we only put one fertilizer on each plot, we would not know if the fertilizer was the reason the garlic crop grew or if it was one of the nuisance factors. Since we **blocked** these 8 plots and applied all four fertilizers in each we have essentially accounted for (or removed the effect of) the nuisance factors. 

Let's briefly explore this new dataset by looking at all 32 values using the ```print``` function.

```{r}
block <- read_csv(file = "garlic_block.csv")

head(block, n = 32)
```

How do we account for this blocking in an ANOVA model context? This blocking ANOVA model is the exact same as the Two-Way ANOVA model. The variable that identifies which sector (block) an observation is in serves as another variable in the model. Think about this variable as the variable that accounts for all the nuisance factors in your ANOVA. That means we have two variables in this ANOVA model - fertilizer and sector (the block that accounts for sun exposure, pH level of soil, rain amount, etc.).

For this we can use the same ```aov``` function we described above.

```{r}
block_aov <- aov(BulbWt ~ factor(Fertilizer) + factor(Sector), data = block)

summary(block_aov)
```

Using the ```summary``` function we can see that both the sector (block) and fertilizer variables are significant in the model at the 0.05 level. What are the interpretations of this? 
First, let's address the blocking variable. Whether it is significant or not, it should **always** be included in the model. This is due to the fact that the data is structured in that way. It is a construct of the data that should be accounted for regardless of the significance. However, since the blocking variable (sector) was significant, that implies that different plots of land have different impacts of the average bulb weight of garlic. Again, this is most likely due to the differences between the plots of land - namely sun exposure, pH of soil, rain fall, etc.

Second, the variable of interest is the fertilizer variable. It is significant, implying that there is a difference in the average bulb weight of garlic for different fertilizers. To examine which fertilizer pairs are statistically difference we can use post-hos testing as described in the previous parts of this chapter using the ```TukeyHSD``` function.

```{r}
tukey.block <- TukeyHSD(block_aov)
print(tukey.block)

plot(tukey.block, las = 1)
```

### Assumptions
Outside of the typical assumptions for ANOVA that still hold here, there are two additional assumptions to be met:
- Treatments are randomly assigned within each block
- The effects of the treatment variable are constant across the levels of the blocking variable

The first, new assumption of randomness deals with the reliability of the analysis. Randomness is key to removing the impact of the nuisance factors. The second, new assumption implies there is **no interaction** between the treatment variable and the blocking variable. For example, we are implying that the fertilizers' impacts ob garlic bulb weight are not changed depending on what block you are on. In other words, fertilizers have the same impact regardless of sun exposure, pH levels, rain fall, etc. We are **not** saying these nuisance factors do not impact the target variable or bulb weight of garlic, just that they do not change the effect of the fertilizer on bulb weight.

## Multiple Linear Regression
Most practical applications of of regression modeling involve using more complicated models than a simple linear regression with only one predictor variable to predict your target. Additional variables in a model can lead to better explanations and predictions of the target. These linear regressions with more than one variable are called **multiple linear regression** models. However, as we will see in this section and the following chapters, with more variables comes much more complication.

### Model Structure
Multiple linear regression models have the same structure as simple linear regression models, only with more variables. The multiple linear regression model with $k$ variables is structured like the following:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
$$

This model has the predictor variables $x_1, x_2, ..., x_k$ trying to either explain or predict the target variable $y$. The intercept, $\beta_0$, still gives the expected value of $y$, when **all** of the predictor variables take a value of 0. With the addition of multiple predictors, the interpretation of the slope coefficients change slightly. The slopes, $\beta_1, \beta_2, ..., \beta_k$, give the expected change in $y$ for a one unit change in the respective predictor variable, **holding all other predictor variables constant**. The random error term, $\varepsilon$, is the error between our predicted value, $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \cdots + \hat{\beta}_k x_k$, and our actual value of $y$. 

Unlike simple linear regression that can be visualized as a line through a 2-dimensional scatterplot of data, a multiple linear regression is better thought of as a multi-dimensional plane through a multi-dimensional scatterplot of data. 

Let's visual an example with two predictor variables - the square footage of greater living area and the total number of rooms. These will predict sale price of a home. When none of the variables have any relationship with the target variable, we get a horizontal plane like the one shown below. This is similar in concept to a horizontal line in simple linear regression having a slope of 0, implying that the target variable does not change as the predictor variable changes.

```{r echo=FALSE}
lm0 <- lm(Sale_Price ~ 1, data = train)

train0 <- train %>% 
  mutate(lm0_pred = predict(lm0))

area <- unique(train$Gr_Liv_Area)
rooms <- unique(train$TotRms_AbvGrd)

area <- seq(min(train$Gr_Liv_Area), max(train$Gr_Liv_Area), length.out = 25)
rooms <- seq(min(train$TotRms_AbvGrd), max(train$TotRms_AbvGrd), length.out = 25)

pred_grid <- expand.grid(area, rooms) %>% 
  add_predictions(lm0)

pred_mx <- matrix(pred_grid$pred,
                  ncol = 25,
                  nrow = 25)

plot_ly() %>% add_surface(x = ~ 300:6000, 
                          y = ~ 1:15, 
                          z = ~ pred_mx
) %>% layout(scene = list(xaxis = list(title = "Living Area (SQFT)"), yaxis = list(title = "Total Rooms"), zaxis = list(title = "Sale Price")))

```

Much like if the slope of a simple linear regression line is **not** 0 (a relationship exists between the predictor and target variable), then a relationship between any of the predictor variables and the target variable shifts and rotates the plane around like the one shown below. 

```{r echo=FALSE}
lm1 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)

train0 <- train %>% 
  mutate(lm1_pred = predict(lm1))

area <- unique(train0$Gr_Liv_Area)
rooms <- unique(train0$TotRms_AbvGrd)

area <- seq(min(train0$Gr_Liv_Area), max(train0$Gr_Liv_Area), length.out = 25)
rooms <- seq(min(train0$TotRms_AbvGrd), max(train0$TotRms_AbvGrd), length.out = 25)

pred_grid <- expand.grid(area, rooms) %>%
  rename(Gr_Liv_Area = Var1,
         TotRms_AbvGrd = Var2) %>%
  add_predictions(lm1)

pred_mx <- matrix(pred_grid$pred,
                  ncol = 25,
                  nrow = 25)

plot_ly() %>% add_surface(x = ~ 300:6000, 
                          y = ~ 1:15, 
                          z = ~ pred_mx
) %>% layout(scene = list(xaxis = list(title = "Living Area (SQFT)"), yaxis = list(title = "Total Rooms"), zaxis = list(title = "Sale Price")))

```

To the naive viewer, the shifted plane would still make sense because of the model naming convention of multiple **linear** regression. However, the **linear** in linear regression doesn't have to deal with the visualization of the fitted plane (or line in two dimensions), but instead refers to the **linear combination of variables**. A linear combination is an expression constructed from a set of terms by multiplying each term by a constant and adding the results. For example, $ax + by$ is a linear combination of $x$ and $y$. Therefore, the linear model

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \varepsilon
$$
is a linear combination of predictor variables in their relationship with the target variable $y$. These predictor variables do not all have to contain linear effects though. For example, let's look at a linear regression model with four predictor variables:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \beta_4 x_4 + \varepsilon
$$

One would not be hard pressed to call this model a linear regression. However, what if we defined $x_3 = x_1^2$ and $x_4 = x_2^2$? 

```{r echo=FALSE}
lm2 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + I(Gr_Liv_Area^2) + I(TotRms_AbvGrd^2), data = train)

train0 <- train %>% 
  mutate(lm2_pred = predict(lm2))

area <- unique(train0$Gr_Liv_Area)
rooms <- unique(train0$TotRms_AbvGrd)

area <- seq(min(train0$Gr_Liv_Area), max(train0$Gr_Liv_Area), length.out = 25)
rooms <- seq(min(train0$TotRms_AbvGrd), max(train0$TotRms_AbvGrd), length.out = 25)

pred_grid <- expand.grid(area, rooms) %>%
  rename(Gr_Liv_Area = Var1,
         TotRms_AbvGrd = Var2) %>%
  add_predictions(lm2)

pred_mx <- matrix(pred_grid$pred,
                  ncol = 25,
                  nrow = 25)

plot_ly() %>% add_surface(x = ~ 300:6000, 
                          y = ~ 1:15, 
                          z = ~ pred_mx
) %>% layout(scene = list(xaxis = list(title = "Living Area (SQFT)"), yaxis = list(title = "Total Rooms"), zaxis = list(title = "Sale Price")))

```

This model is still a linear regression model. The structure of the model did not change. The model is still a linear combination of predictor variables related to the target variable. The predictor variables just do not all have a linear effect in terms of their relationship with $y$. However, mathematically, it is still a linear combination and a linear regression model. Logistic regression on the other hand, which will be covered in Chapter \@ref(cat) is an example of a **nonlinear** regression model.

### Global & Local Inference
In simple linear regression we could just look at the t-test for our slope parameter estimate to determine the utility of our model. With multiple parameter estimates comes multiple t-tests. Instead of looking at every individual parameter estimate initially, there is a way to determine the model adequacy for predicting the target variable overall.

The utility of a multiple regression model can be tested with a single test that encompasses all the coefficients from the model. This kind of test is called a **global test** since it tests all $\beta$'s simultaneously. The **Global F-Test** uses the F-distribution to do just that for multiple linear regression models. The hypotheses for this test are the following:

$$
H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0 \\
H_A: \text{at least one } \beta \text{ is nonzero}
$$

In simpler terms, the null hypothesis is that none of the variables are useful in predicting the target variable. The alternative hypothesis is that **at least one** of these variables is useful in predicting the target variable.

The F-distribution is a distribution that has the following characteristics:
- Bounded below by 0
- Right-skewed
- Both **numerator** and **denominator** degrees of freedom

A plot of a variety of F distributions is shown here:

```{r echo=FALSE}
data.frame(f = 0:1000 / 100) %>% 
           mutate(df_1_1 = df(x = f, df1 = 1, df2 = 1),
                  df_10_20 = df(x = f, df1 = 10, df2 = 30),
                  df_5_4 = df(x = f, df1 = 5, df2 = 4)) %>%
  gather(key = "df", value = "density", -f) %>%
ggplot() +
  geom_line(aes(x = f, y = density, color = df, linetype = df)) +
  labs(title = "F at Various Degrees of Freedom",
       x = "F",
       y = "Density") +
  xlim(0,5) + ylim(0,1) +theme_minimal()
```


If the global test is significant, the next step would be to examine the individual t-tests to see which variables are significant and which ones are not. This is similar to post-hoc testing in ANOVA where we explored which of the categories was statistically different when we knew at least one was.

These tests are all available using the ```summary``` function on an ```lm``` function for linear regression. To build a multiple linear regression in R using the ```lm``` function, you just add another variable to the formula element. Here we will predict the sales price (```Sale_Price```) based on the square footage of the greater living area of the home (```Gr_Liv_Area```) as well as total number of rooms above ground (```TotRms_AbvGrd```). 

```{r}
ames_lm2 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)

summary(ames_lm2)
```

At the bottom of the above output is the result of the global F-test. Since the p-value on this test is lower than the significance level of 0.05, we have statistical evidence that at least of the two variables - ```Gr_Liv_Area``` and ```TotRms_AbvGrd``` - is significant at predicting the sale price of the home. By looking at the individual t-tests in the output above, we can see that both variables are actually significant.

### Assumptions
The **main** assumptions for the multiple linear regression model are the same as with the simple linear regression model:
1. The expected value of $y$ is linear in the $x$'s (proper model specification).
2. The random errors are independent.
3. The random errors are normally distributed.
4. The random errors have equal variance (homoskedasticity).

However, with multiple variables there is an additional assumption that people tend to add to multiple linear regression modeling:
5. No **perfect** collinearity (also called multicollinearity)

The new assumption means that no combination of predictor variables is a perfect linear combination with any other predictor variables. Collinearity, also called multicollinearity, occurs when predictor variables are correlated with each other. People often misstate this additional assumption as having no collinearity at all. This is too restrictive and basically impossible to meet in a realistic setting. Only when collinearity has a drastic impact on the linear regression do we need to concern ourselves. In fact, linear regression only completely breaks when that collinearity is perfect. Dealing with multicollinearity is the subject of Chapter \@ref(diag).

Similar to simple linear regression, we can evaluate the assumptions by looking at residual plots. The ```plot``` function on the ```lm``` object provides these.

```{r}
par(mfrow=c(2,2))
plot(ames_lm2)
par(mfrow=c(1,1))
```

These will again be covered in much more detail in Chapter \@ref(diag).

### Multiple Coefficients of Determination
One of the main advantages of multiple linear regression is that the complexity of the model enables us to investigate the relationship among $y$ and several predictor variables simultaneously. However, this increased complexity makes it more difficult to not only interpret the models, but also ascertain which model is "best."

One example of this would be the coefficient of determination, $R^2$, that we discussed in Chapter \@ref(slr). The calculation for $R^2$ is the exact same:

$$
R^2 = 1 - \frac{SSE}{TSS}
$$

However, the problem with the calculation of $R^2$ in a multiple linear regression is that the addition of any variable (useful or not) will never make the $R^2$ decrease. In fact, it typically increases even with the addition of a useless variable. The reason is rather intuitive. When adding information to a regression model, your predictions can only get better, not worse. If a new predictor variable has no impact on the target variable, then the predictions can not get any worse than what they already were before the addition of the useless variable. Therefore, the $SSE$ would never increase, making the $R^2$ never decrease.

To account for this problem, there is the **adjusted coefficient of determination**, $R^2_a$. The calculation is the following:

$$
R^2_a = 1 - [(\frac{n-1}{n-(k+1)})\times (\frac{SSE}{TSS})]
$$

Notice what the calculation is doing. It takes the original ratio on the right hand side of the $R^2$ equation, $SSE/TSS$, and penalizes it. It multiplies this number by a ratio that is always greater than 1 if $k > 0$. Remember, $k$ is the number of variables in the model. Therefore, as the number of variables increases, the calculation penalizes the model more and more. However, if the reduction of SSE from adding a useful variable is low enough, then even with the additional penalization, the $R^2_a$ will increase **if the variable is a useful addition to the model**. If the variable is not a useful addition to the model, the $R^2_a$ will decrease. The $R^2_a$ is only one of many ways to select the "best" model for multiple linear regression. Many more metrics will be discussed in model selection in Chapter \@ref(sel).

One downside of this new metric is that the $R^2_a$ loses its interpretation. Since $R^2_a \le R^2$, it is no longer bounded below by zero. Therefore, it can no longer be the proportion of variation explained in the target variable by the model. However, we can easily use $R^2_a$ to select a model correctly and interpret that model with $R^2$. Both of these numbers can be found using the ```summary``` function on the ```lm``` object from the previous model.

```{r}
summary(ames_lm2)
```

From this output we can say that the combination of ```Gr_Liv_Area``` and ```TotRmsAbvGrd``` account for 50.95% of the variation in ```Sale_Price```. Now let's add a random variable to the model. This random variable will take random values from a normal distribution with mean of 0 and standard deviation of 1 and has no impact on the target variable.

```{r}
set.seed(1234)
ames_lm3 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 0, 1), data = train)

summary(ames_lm3)
```

Notice that the $R^2$ of this model actually increased to 0.5097 from 0.5095. However, the $R^2_a$ value dropped from 0.5091 to 0.5089 since the addition of this new variable did not provide enough predictive power to outweigh the penalty of adding it.

### Categorical Predictor Variables
As mentioned in Chapter \@ref(eda), there are two types of variables typically used in modeling:
- Quantitative (or numeric)
- Qualitative )or categorical)

Categorical variables need to be coded differently because they are not numerical in nature. As mentioned in Chapter \@ref(eda), two common coding techniques for linear regression are **reference** and **effects** coding. The interpretation of the coefficients ($\beta$'s) of these variables in a regression model depend on the specific coding used. The predictions from the model, however, will remain the same regardless of the specific coding that is used.

Let's use the example of the ```Central_Air``` variable with 2 categories - Y and N. Using reference coding, the **reference** coded variable to describe these 2 categories (with N as the reference level) would be the following:

<table>
<tr>
<td> Central Air<td> X1
<tr><td> Y<td> 1
<tr><td> N<td> 0
</table>
<caption> (\#tab:centralair)Reference variable coding for the categorical attribute _Central Air_ </caption>

The linear regression equation would be:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
$$

Let's see the mathematical interpretation of the coefficient $\hat{\beta}_1$. To do this, let's get the average sale price of a home prediction for a home with central air ($\hat{y}_Y$) and without central air ($\hat{y}_N$):

$$
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot 0 = \hat{\beta}_0
$$

By subtracting these two equations ($\hat{y}_Y - \hat{y}_N = \hat{\beta}_1$), we can get the prediction for the average difference in price between a home with central air and without central air. This shows that in reference coding, the coefficient on each dummy variable is the average difference between that category and the reference category (the category not represented with its own variable). The math can be extended to as many categories as needed.

Using effects coding, the **effects** coded variable to describe these 2 categories (with N as the reference level) would be the following:

<table>
<tr>
<td> Central Air<td> X1
<tr><td> Y<td> 1
<tr><td> N<td> -1
</table>
<caption> (\#tab:centralair)Effects variable coding for the categorical attribute _Central Air_ </caption>

The linear regression equation would be:

$$
\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 X_1
$$

Let's see the mathematical interpretation of the coefficient $\hat{\beta}_1$. To do this, let's get the average sale price of a home prediction for a home with central air ($\hat{y}_Y$) and without central air ($\hat{y}_N$):

$$
\hat{y}_Y = \hat{\beta}_0 + \hat{\beta}_1 \cdot 1 = \hat{\beta}_0 + \hat{\beta}_1 \\
\hat{y}_N = \hat{\beta}_0 + \hat{\beta}_1 \cdot -1 = \hat{\beta}_0 - \hat{\beta}_1
$$

Similar to reference coding, the coefficient $\hat{\beta}_1$ is the average difference between homes with central air and $\hat{\beta}_0$. However, what is $\hat{\beta}_0$? By taking the average of our two predictions: 

$$
\frac{1}{2} \times (\hat{y}_Y + \hat{y}_N) = \frac{1}{2} \times (\hat{\beta}_0 + \hat{\beta}_1 + \hat{\beta}_0 - \hat{\beta}_1) = \frac{1}{2} \times (2\hat{\beta}_0) = \hat{\beta}_0
$$

From this average we can get the prediction for the average difference in price between a home with central air and the average price across all homes. This shows that in effects coding, the coefficient on each dummy variable is the average difference between that category and the average price across **all** homes (including both with and without central air). The math can be extended to as many categories as needed.

Let's see an example with ```Central_Air``` as a variable added to our multiple linear regression model as a reference coded variable.

```{r}
ames_lm4 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Central_Air, data = train)

summary(ames_lm4)
```

With these results we estimate the average difference in sales price between homes with central air and without central air to be \$56,672.41.